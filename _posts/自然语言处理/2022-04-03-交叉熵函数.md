layout: post
title: "交叉熵损失函数"
description: "交叉熵损失函数以及优劣关系分析"
tag: "自然语言处理"

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

## 交叉熵损失函数 - Cross entropy loss function

​	标准形式
$$
\color{blue}{C=-\frac{1}{n}\sum_x{[ylna+(1-y)ln(1-a)]} ---------------- (1)}
$$
其中$\color{blue}{x}$表示样本，$\color{blue}{y}$表示实际的标签，$\color{blue}{a}$表示预测的输出，$\color{blue}{n}$表示样本总数量。

### 特点

+ 本质上是一种对数似然函数，可用于二分类和多分类中。二分类问题中的loss函数（输入数据是softmax或者sigmoid）的输出：
  $$
  \color{blue}{loss=-\frac{1}{n}\sum_x[ylna+(1-y)ln(1-a)] ------------- (2)}
  $$
  多分类问题中loss函数（输入数据是softmax或者sigmoid）的输出：
  $$
  \color{blue}{loss=-\frac{1}{n}\sum_iy_ilna_i---------------------(3)}
  $$

+ 当使用sigmoid作为激活函数时，常用交叉熵损失函数而不用均方差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有$\color{yellow}{”误差大的时候，权重更新快；误差小的时候，权重更新慢}$的良好兴致。

### 代码

```python
import numpy as np
def cross_entropy(Y,P):
    Y = np.float_(Y)
    P = np.float_(P)
    return -np.sum( Y*np.log(P) + (1-Y)*np.log(1-P) ) / len(Y)
```

### 提问

#### 1、在用sigmoid作为激活函数时，你为什么要用交叉熵损失函数，而不用均方差损失函数啊？

​	均方误差的损失函数，定义为：$\color{blue}{C=\frac{1}{2n}\sum_x(a-y)^2}$，其中$\color{blue}{y}$是label，$\color{blue}{a}$是实际输出($\color{blue}{a=\sigma(z),z=wx+b}$)，在训练神经网络的时候我们使用梯度下降方式来更新$\color{blue}{w}$和$\color{blue}{b}$，因此对$\color{blue}{w}$和$\color{blue}{b}$求导：
$$
\color{blue}{\frac{\partial C}{\partial w}=(a-y)\sigma'(z)x ------------ (4)}
$$

$$
\color{blue}{\frac{\partial C}{\partial b} = (a-y)\sigma'(z) -------------(5)}
$$

​	更新参数$\color{blue}{w}$和$\color{blue}{b}$：
$$
\color{blue}{w = w-\eta\frac{\partial C}{\partial w} = w-\eta(a-y)\sigma'(z)x --------(6)}
$$

$$
\color{blue}{b = b-\eta\frac{\partial C}{\partial b} = b-\eta(a-y)\sigma'(z) ---------(7)}
$$

由于sigmoid的性质，导致$\color{blue}{\sigma'(x)}$在$\color{blue}{z}$取大部分值时会很小，这样会使得$\color{blue}{\eta(a-y)\sigma'(z)}$很小，导致$\color{blue}{w}$和$\color{blue}{b}$更新非常慢。

​	交叉熵损失函数，定义为：$\color{blue}{C=-\frac{1}{n}\sum_x{[ylna+(1-y)ln(1-a)]}}$，字母定义如上，同样看其对$\color{blue}{w}$和$\color{blue}{b}$求导：
$$
\color{blue}{\frac{\partial C}{\partial a}=-\frac{1}{n}\sum_x{[y\frac{1}{a}+(y-1)\frac{1}{1-a}]}=-\frac{1}{n}\sum_x{[\frac{1}{a(1-a)}y-\frac{1}{1-a}]}}\\ \color{blue}{=-\frac{1}{n}\sum_x{[\frac{1}{\sigma(x)(1-\sigma(x))}y-\frac{1}{1-\sigma(x)})]}-------(8)}
$$

$$
\color{blue}{\frac{\partial C}{\partial z}=\frac{\partial C}{\partial a}\frac{\partial a}{\partial z}=-\frac{1}{n}\sum_x{[\frac{1}{\sigma(x)(1-\sigma(x))}y-\frac{1}{1-\sigma(x)}]\bullet \sigma'(x)}}
$$

$$
\color{blue}{=-\frac{1}{n}\sum_x{[\frac{1}{\sigma(x)(1-\sigma(x))}y-\frac{1}{1-\sigma(x)}]\bullet \sigma(x)(1-\sigma(x))} = -\frac{1}{n}\sum_x{y-a}-----(9)}
$$

所以有：
$$
\color{blue}{\frac{\partial C}{\partial w}=\frac{\partial C}{\partial z}\frac{\partial z}{\partial w}=(a-y)x--------------------(10)} 
$$

$$
\color{blue}{\frac{\partial C}{\partial b}=\frac{\partial C}{\partial z}\frac{\partial z}{\partial b}=(a-y)---------------------(11)} 
$$

所以更新公式为：
$$
\color{blue}{w=-\eta\frac{\partial C}{\partial w}=w-\eta(a-y)x------------(12)} 
$$

$$
\color{blue}{b=-\eta\frac{\partial C}{\partial b}=b-\eta(a-y) -------------(13)}
$$

可以看到参数更新公式中没有$\color{blue}{\sigma'(x)}$这一项，权重更新受$\color{blue}{a-y}$影响，收到误差的影响，当$\color{yellow}{当误差大的时候，权重更新快，当误差小的时候，权重更新慢}$，这是一个很好的性质。

所以当使用sigmoid作为激活函数的时候，常用**交叉熵损失函数**而不用**均方误差损失函数**。 

+ 回归问题通常用均方差损失函数，可以保证损失函数是个凸函数，即可以得到最优解。而分类问题如果用均方差的话，损失函数的表现不是凸函数，就很难得到最优解。而交叉熵函数可以保证区间内单调。
+ 分类问题的最后一层网络，需要分类函数，Sigmoid或者Softmax，如果再接均方差函数的话，其求导结果复杂，运算量比较大。用交叉熵函数的话，可以得到比较简单的计算结果，一个简单的减法就可以得到反向误差。

#### 2、交叉熵函数与似然函数的滚滚红尘

+ 联系

​	似然函数的通用形式$\color{blue}{L=P(X;\theta)}$，$\color{yellow}{似然函数的定义没有限定样本集X的分布函数}$。比如在机器学习中，将似然函数用于模型的损失函数，那么对应的样本集$\color{blue}{X}$就是样本集中的label，参数也不是机器学习的model的参数，而是prediction，作为一个损失函数，我们不关心选哪个的参数是怎么样的，而只接受两部分【prediction、label】，还有一个隐含的输入【分布模型】。

​	似然函数最终的目的就是衡量prediction背后的模型对于当前观测值的解释程度，而每个样本的prediction值恰恰就是它所服从的分布模型的参数。

​	机器学习分类任务中，我们假设prediction这个随机变量背后的模型是**单次观测下的多项式分布**？？？

​	这里的单词观测就意味着独立同分布，统计学中☞一组随机变量中每个变量的概率分布都相同，且这些随机变量相互独立。

+ 伯努利分布

  ​	也叫两点分布，类似抛硬币，因此概率密度函数为：
  $$
  \color{blue}{f_X(x)=p^x(1-p)^{1-x}={\begin{cases}p&{x=1,} \\q&{x= 0.}\end{cases}}----(14)}
  $$
  伯努利分布模型参数就是其中一个类别发生的概率。

+ 二项分布

  ​	二项分布就是将伯努利实验重复n次

 + 多项式分布

   ​	多项式分布就是将二项分布推广到多个面（类别），即$\color{blue}{f_{multi}(x;p)=\prod_{i=1}^Cp_{i}^{x_i}}$，其中C代表类别数，p代表向量形式的模型参数，即各个类别发生的概率，如$\color{blue}{p=[0.1,0.1,0.7,0.1]}$，则$\color{blue}{p1=0.1,p3=0.7}$。即多项分布的模型参数就是各个类别发生概率，x代表ont-hot形式的观测值。

 根据似然函数定义，单个样本的似然函数即：
$$
\color{blue}{L=f_{multi}(label;prediction)------------ (15)} 
$$
所以整个样本集或batch的似然函数：
$$
\color{blue}{L=\prod_{X}f_{multi}(lable;prediction)=\prod_X\prod_{i=1}^Cprediction(i)^{label(i)}---(16)} 
$$
由于有累积运算，所以家隔log函数转为累加以提高运算速度。如下：
$$
\color{blue}{L=\sum_X\sum_{i=1}^Clabel(i)log(prediction(i))--------(17)}
$$
而最大化似然函数就是等效于最小化负对数似然函数，所以前面加个负号后就和交叉熵的形式一摸一样。对于某种分布的随机变量$\color{blue}{X\sim p(x)}$，有一个模型$\color{blue}{q(x)}$用于近似$\color{blue}{p(x)}$的概率分布，则分布$\color{blue}{X}$与模型$\color{blue}{q}$之间的交叉熵即：
$$
\color{blue}{H(X,q)=-\sum_xp(x)logq(x)-------------(18)}
$$
 这里X的分布模型即样本集label的真实分布模型，这里模型$\color{blue}{q(x)}$即想要模拟真实分布模型的机器学习模型。可以说交叉熵是直接衡量两个分布，或者说两个model之间的差异。而似然函数则是解释以model的输出为参数的某分布模型对样本集的解释程度。因此，可以说这两者是“同貌不同源”，但是“殊途同归”啦。 

而苏神也在[这里](https://spaces.ac.cn/archives/4277#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6)在nlp角度做过类似的推到分析，可以参考。

$\color{red}{上面就是讨论了交叉熵和似然函数的联系，下面简要说一下区别}$

+ 区别

  交叉熵函数使用来描述模型预测值和真实值的差距大小，越大代表越不相近，似然函数的本质就是衡量在某个参数下，整个估计和真实情况一样的概率，越大代表越相近。

#### 3、交叉熵和KL散度的种种因缘

+ 相对熵（KL散度）

  ​	对同一个随机变量$\color{blue}{x}$有两个单独的概率分布$\color{blue}{P(x)}和\color{blue}{Q(x)}$，我们可以使用KL散度来衡量这两个分布的差异，这个相当于信息论范畴的均方差。即：
  $$
  \color{blue}{D_{KL}(P\|\|q)=\sum_{j=1}^n(x_j)ln\frac{p(x_j)}{q(x_j)}-------- (19)}
  $$
  $\color{blue}{n}$为事件的所有可能性。$\color{blue}{D}$的值越小，表示$\color{blue}{q}$分布和$\color{blue}{p}$分布越接近。

  ☞变形记
  $$
  \color{blue}{D_{KL}(p\|\|q)=\sum_{j=1}^n(x_j)lnp(x_j)-\sum_{j=1}^{n}p(x_j)lnq(x_j)\\=-H(p(x))+H(p,q)------------(20)}
  $$
  等式前一部分恰巧就是$\color{blue}{p}$的熵，等式的后一部分就是交叉熵。在机器学习中，我们将$\color{blue}{p,q}$替换为标签$\color{blue}{y}$和预测值$\color{blue}{a}$，即$\color{blue}{D_{KL}(y\|\|a)}$，就可以衡量评估了。KL散度前一项$\color{blue}{H(y)}$不变，所以在优化的过程中需要要关注交叉熵就行，即:
  $$
  \color{blue}{loss=-\sum_{j=1}^ny_jlna_j -------------- (21)}
  $$
  公式(21)是单个样本的情况，$\color{blue}{n}$并不是样本个数，而是分类个数，所以批量交叉熵计算公式是：
  $$
  \color{blue}{J=-\sum_{i=1}^m\sum_{j=1}^ny_{ij}lna_{ij} ------------- (22)}
  $$
  其中$\color{blue}{m}$是样本数，$\color{blue}{n}$是分类数。























## 巨人的肩膀

[1、常见的损失函数(loss function)总结](https://zhuanlan.zhihu.com/p/58883095)

[2、哈？你还认为似然函数跟交叉熵是一个意思呀？](https://zhuanlan.zhihu.com/p/27719875)

[3、最大似然](https://spaces.ac.cn/archives/4277#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6)

[4、交叉熵损失函数](https://microsoft.github.io/ai-edu/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/A2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/%E7%AC%AC1%E6%AD%A5%20-%20%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/03.2-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html)