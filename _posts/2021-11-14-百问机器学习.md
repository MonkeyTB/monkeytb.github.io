---
layout: post
title: "百问机器学习"
description: "百问机器学习书"
tag: "机器学习"
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

## 一、特征工程

巧妇难为无米之炊

### 1、为什么归一化

- 归一化能够更快的找到迭代优化的方向，因此在实际中通过梯度下降求解的模型通常是需要归一化，eg：线性回归、逻辑回归、支持向量机、神经网络等，决策模型则不适用，eg：C4.5，决策树节点分裂主要依靠数据集D关于特征x的信息增益比，信息增益比与特征是否经过归一化是无关的。
- 常用的归一化方法有线性函数归一化（最大最小值归一化），零均值归一化$z=\frac{x-\mu}{标准差}$

### 2、类别特征数据预处理

- 类别特征主要指性别（男女）、血腥（A、B、AB、O）等
- 主要方法：序号编码、独热编码、二进制编码
- 序号编码：分桶，成绩高中低
- 独热编码：ont-hot编码
  - 稀疏，可以用稀疏向量来节省空间
  - 配合特征选择来降低维度。高维度特征会带来几方面的问题。一是在K 近邻算法中，高维空间下两点之间的距离很难得到有效的衡量；二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度
- 二进制编码：类别特征的个数最少用几位的二级制可以表示，转换成对应的二级制编码

### 3、组合特征？如何处理高维组合特征？

- 组合特征就是对当前的特征两两（nn）组合成为一个新的特征
- 高维组合特征不一定有意义，并且有可能使得参数变多造成过拟合，因此可以通过矩阵分解的方式进行降维

### 4、怎么有效的找到组合特征

- 利用决策树，决策树从根节点到叶子节点，路径上的组合特征即可以看成一种路径结合方式

### 5、有那些文本表示模型？他们各有什么优缺点？

- 词袋模型和N-gram模型

  - 词袋模型就是将一段文本看成一个袋子，忽略了每个词出现的顺序，一般采用TF-IDF计算权重 TF−IDF(t,d)=TF(t,d)∗IDF(t)TF-IDF(t,d) = TF(t,d)*IDF(t)TF−IDF(t,d)=TF(t,d)∗IDF(t) 

    其中TF(t,d)为单词t再文档d中出现的频率，IDF(t)是逆文档频率，衡量单词t对表达语义所起的重要性，表示如下 IDF(t)=log文章总数包含单词t的文章总数+1IDF(t) = log^\frac{文章总数}{包含单词t的文章总数+1}IDF(t)=log包含单词t的文章总数+1文章总数 

  - 缺点：natural language processing（自然语言处理），分词后三个词表达的含义与之大相径庭。

- 主题模型

- 词嵌入和深度学习模型

  - 词嵌入是一种词向量化的统称，核心思想就是将每个映射到低维空间K上的一个稠密向量，K上每一维可以看作是一个隐含的主题，只不过不像主题模型那么直观。

### 6、Word2Vec是如何工作的？它和LDA有什么区别和联系？

- Word2Vec 见[鄙人博客](https://www.cnblogs.com/monkeyT/p/12217490.html)
- 不同
  - LDA是利用文档中单词的共现关系来对单词按主题聚类，可以理解为对“文档－单词”进行矩阵分解，得到“文档－主题”和“主题－单词”两个概率分布；而Word2Vec 是利用“文档-单词”矩阵学习上下文，因此Word2Vec融入了很多的上下文信息，因此两个词对应的词向量就比较相似了。
  - 最大的不同在于两者之间的模型本身，主题模型是一种基于概率图模型的生成式模型，而Word2Vec则利用神经网络最后映射到稠密向量。

### 7、图像任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？

- 模型所能提供的信息一般来于两个方面，一个是训练数据中蕴含的信息；二十模型在形成过程中（包括构造、学习、推理等）人们提供的先验信息。
- 训练数据不足容易导致过拟合，从两方面进行改进，
  - 一方面简化模型，比如从非线性模型改为线性模型，或者添加约束条件（L1、L2）dropout等，
  - 另一方面就是增加数据，图像数据增加数据技术相对成熟，旋转、偏移、高斯噪声、颜色对比亮度等变换；自然语言处理中可以通过随机删除、替换以及利用深度模型等进行数据扩充；迁移学习等

## 二、模型评估

没有测量就没有科学

### 8、准确率的局限性

- 首先从数据出发，数据是否存在样本严重不平衡，在此数据下模型的准确率就不能衡量模型整体的指标，可以考虑采用平均准确率
- 其次考虑模型是否存在过拟合或者欠拟合，这时候模型的准确率就不能代替实际效果

### 9、精确率和召回率的权衡

- 精确度是识别的个数中正确的占比，召回率是识别正确的占应该识别正确的总数的占比，如果只看精确度不看召回，存在线上搜索结果搜索显示不全，同样的只看召回存在召回准确性的问题

- 通常用 f1-score 来对两者进行权衡，即两者的调和平均值，计算如下$f1 = \frac{2*precision*recall}{precision+recall}$

### 10、平方根误差的“意外”

- 平方根误差一般用来衡量回归问题，计算公式如下$RMSE=\sqrt{\frac{\sum_{i=1}^n(y_i-\hat y_i)^2}{n}}$。

从公式可以看出，RMSE可以衡量y的真实值和预测值之间的差距，但如果样本中存在离群点时，即使实际准确性较高，但是RMSE可能会存在较大的情况，这时候平方根误差就出现了意外，在实际中考虑两方面，一方面这些离群点是否是噪声，那么在数据处理阶段就应该踢除，另一方面如果不是噪声，那么模型应该在拟合的时候就考虑这些离群点。第一种情况时可以考虑换其他误差函数衡量，例如MPAE（平均绝对百分比误差），计算公式如下 $MPAE = \sum_{i=1}^n|\frac{y_i-\hat y_i}{y_i}|*\frac{100}{n}$

相比RMSE，MPAE相当于把每个点的误差都进行了归一化，这样就降低了离群点带来的绝对误差的影响；第二种情况就是一个很大的场景了，不展开叙述

### 11、什么是ROC曲线

- ROC曲线全称受试者工作曲线，横坐标为假阳性率（FPR），纵坐标为真阳性率（TPR），计算公式如下 $FRP=\frac{FP}{N},TPR=\frac{TP}{P}$
  
  简单点说FPR就是所有负类中识别为正类的占比，TPR就是所有正类中识别为正类的占比

### 12、如何绘制ROC曲线

| 样本序号 | 真实标签 | 模型输出概率 | TPR  | FPR  |
| -------- | -------- | ------------ | ---- | ---- |
| 1        | p        | 0.9          | 1/10 | 0    |
| 2        | p        | 0.8          | 2/10 | 0    |
| 3        | n        | 0.7          | 2/10 | 1/10 |
| 4        | p        | 0.6          | 3/10 | 1/10 |
| 5        | p        | 0.55         | 4/10 | 1/10 |
| 6        | p        | 0.54         | 5/10 | 1/10 |
| 7        | n        | 0.53         | 5/10 | 2/10 |
| 8        | n        | 0.52         | 5/10 | 3/10 |
| 9        | p        | 0.51         | 6/10 | 3/10 |
| 10       | n        | 0.505        | 6/10 | 4/10 |
| 11       | p        | 0.4          | 7/10 | 4/10 |
| 12       | n        | 0.39         | 7/10 | 5/10 |
| 13       | p        | 0.38         | 8/10 | 5/10 |
| 14       | n        | 0.37         | 8/10 | 6/10 |
| 15       | n        | 0.36         | 8/10 | 7/10 |
| 16       | n        | 0.35         | 8/10 | 8/10 |
| 17       | p        | 0.34         | 9/10 | 8/10 |
| 18       | n        | 0.33         | 9/10 | 9/10 |
| 19       | p        | 0.30         | 1    | 9/10 |
| 20       | n        | 0.1          | 1    | 1    |



### 13、如何计算AUC

- AUC是ROC曲线下方面积的大小，一般对横轴做积分即可，AUC的取值范围一般在[0.5-1]之间，原因是ROC曲线总在y=x之上，如果在下方，则把p换成1-p即可以得到一个更好的分类器

### 14、ROC曲线相比P-R曲线有什么特点

- ROC曲线有一个特点是当正负样本的分布发生变化（正负样本比例1/1000--> 1/10000）时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈的变化，这就是的ROC曲线能尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能

### 15、结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不 是欧氏距离？

- 总体来说，欧氏距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。
  - 1）例如，统计两部剧的用户观看行为，用户A的观看向量为(0,1)，用户B为(1,0)；此时二者的余弦距很大，而欧氏距离很小；我们分析两个用户对于不同视频的偏好，更关注相对差异，显然应当使用余弦距离。
  - 2）而当我们分析用户活跃度，以登陆次数(单位：次)和平均观看时长(单：分钟)作为特征时，余弦距离会认为(1,10)、(10,100)两个用户距离很近；但显然这两个用户活跃度是有着极大差异的，此时我们更关注数值绝对差异，应当使用欧氏距离。
- 余弦相似度的取值范围是[−1,1]， 相同的两个向量之间的相似度为1。如果希望得到类似于距离的表示，将1减去余 弦相似度即为余弦距离。因此，余弦距离的取值范围为[0,2]，相同的两个向量余 弦距离为0，空间余弦角度在0-90°，余弦相似度在1-0，余弦距离在0-1，因此我们在使用词向量计算余弦距离时也在0-1，越小越相似

### 16、在对模型进行过充分的离线评估之后，为什么还要进行在线A/B测试？

- 离线评估无法完全消除模型过拟合的影响，所以离线评估无法代替线上评估
- 离线评估无法完全还原线上工程环境
- 线上系统的某些商业指标在离线评估中无法计算，eg：点击率等

### 17、如何进行线上 A/B 测试？

- A/B测试主要手段是通过对用户分桶，即将用户分成实验组和对照组，对实验组用新模型，对照组用旧模型，在分桶中注意样本独立性和采样方式的无偏性。

### 18、如何划分实验组和对照组

- 假设我们系统中对A国的用户替换了推荐模型，再上线时希望对比实验组和对照组的结果，那么如何划分实验组和对照组；由于我们是对A国的用户推荐替换模型，因此我们实验组和对照组都应该是A国用户，并且分的时候要注意独立和无偏，而不能混进其他类型的用户进行对照，一方面会稀释我们的结果，另一方面也有可能会导致我们对比不准确

### 19、在模型评估过程中，有哪些主要的验证方法，它们的优缺点是什么?

- holdout方法：模型数据按照70%-30%分为训练集和测试集，70%的数据用来训练，30%的数据用来测试，这就存在一个问题，当数据切分不均匀时，会导致测试集评估结果和训练集存在和大的gap，因此提出下面的方法
- k-fold交叉验证：数据集分为k份，每次留一份进行测试，其他用来训练，最后k次测试的平均结果作为模型结果，缺点也很明显，时间增加很多，当数据很大时会很耗费时间，当数据较少时有不适用，因此提出下面的方法
- 自助法：当数据较少时我们对数据（n）进行又放回的抽样(n)次，然后用没抽样到的样本进行测试

### 20、在自助法的采样过程中，对n个样本进行n次自助抽样，当n趋于无穷大时， 最终有多少数据从未被选择过？

- 1/e，因此样本很大时，大约有36.8%的样本从未被选择过，可作为验证集

### 21、超参数有那些调优方法？

- 网格搜索
  - 通过搜索查找范围所有点来确定最优点，这里所有点其实是以较小的步长进行搜索，问题是非常消耗计算资源和时间，因此再使用的时候开始会以较大的步长进行搜索，找到最优点再以小步幅进行搜索，但一般目标函数是非凸优化问题，所以可能会错过全局最优值
- 随机搜索
  - 在搜索范围内随机搜索点，理论上只要样本点足够多，那么通过随机采样就能找到最优点，随机搜索一般比网格搜索快一些，但是也没法保证得到最优点
- 贝叶斯优化算法
  - 贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全 不同的方法。网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息； 而贝叶斯优化算法则充分利用了之前的信息。贝叶斯优化算法通过对目标函数形 状进行学习，找到使目标函数向全局最优值提升的参数。具体来说，它学习目标 函数形状的方法是，首先根据先验分布，假设一个搜集函数；然后，每一次使用 新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布；最 后，算法测试由后验分布给出的全局最值最可能出现的位置的点。对于贝叶斯优 化算法，有一个需要注意的地方，一旦找到了一个局部最优值，它会在该区域不 断采样，所以很容易陷入局部最优值。为了弥补这个缺陷，贝叶斯优化算法会在 探索和利用之间找到一个平衡点，“探索”就是在还未取样的区域获取采样点； 而“利用”则是根据后验分布在最可能出现全局最值的区域进行采样。

### 22、在模型评估过程中，过拟合和欠拟合具体是指什么现象？

- 过拟合
  - 模型在训练集上表现良好，测试集上表现很差
- 欠拟合
  - 模型在训练集和测试集上表现都不好

### 23、能否说出几种降低过拟合和欠拟合风险的方法？

- 过拟合直观解释，再对训练数据上进行拟合时，需要照顾每个点，从而导致拟合函数波动大，即方差大。
- 误差图判断过拟合还是欠拟合：

> 模型再训练集与测试集上误差均很大，则说明模型bias很大，欠拟合
>
> 训练集和测试集误差之间有很大的Gap，则说明Variance很大，过拟合

- 欠拟合解决方法

> - 增加新的特征，考虑加入组合特征、高次特征，来增大假设空间
> - 尝试非线性模型，比如SVM、决策树、DNN等模型
> - 有正则项则尝试降低正则项参数\lambda

- 过拟合解决方法

> - 交叉验证，通过交叉验证得到最优的模型
>
> - 特征选择，减少特征数或者使用较少的特征组合，对于区间化离散特征，增大划分的空间
>
> - 正则化，常用的有L_1,L_2正则，而且L_1正则还可以自动进行特征选择
>
> - 如果有正则项考虑增大正则项参数\lambda
>
> - 增加训练数据可以有效的避免过拟合
>
> - Bagging，将多个弱学习器Bagging一下效果会好很多，比如随机森林
>
> - DNN中常用的方法
>
>   - 早停。本质还是交叉验证策略，选择合适的训练次数，避免训练的网络过度拟合训练数据
>
>   - 集成学习策略。利用Bagging思路来正则化，首先对原始的m个训练样本进行又放回的随机采样，构建N组m个样本的数据集，然后分别用N组数据训练DNN，即得到N个DNN模型的ｗ，ｂ参数组合，最后对N个DNN模型的输出用加权平均法或者投票法决定最终输出。缺点就是参数增加了N倍，导致训练花费更多的时间和空间，因此N一般不能太多，例如5-10个。
>
>   - Dropout策略。
>
>     > 训练的时候一一定的概率p让神经元失活，类似于Bagging策略，测试时所有神经元都参与，但是在其输出上乘以1-p.
>
>   - simpler model structure（简化模型）
>
>     > 简单模型拟合复杂数据时，导致模型很难拟合数据的真实分布，这边是模型欠拟合了，同样的，复杂模型拟合简单数据，会导致数据的噪声也被拟合了，导致模型再训练集上效果非常好，但泛化性能很差。
>
>   - regularization（正则化）
>
>     - ![](images/posts/机器学习/微信截图_20211114150303.png)
>
> - data augmentation(数据集扩增)
>
>   > 有效前提是训练集与将来的数据是**独立同分布**的,或者近似独立同分布
>   >
>   > - 从数据源头采集更多的数据
>   > - 复制原有数据并加上随机噪声
>   > - 重采样
>   > - 根据当前数据估计数据分布参数,使用该分布产生更多数据
>
> - Bootstrap/Bagging（封装）
>
> - ensemble（集成）
>
> - early stopping（提前终止训练）
>
>   > 迭代次数截断的方法来防止过拟合,再模型对训练数据集迭代收敛之前停止迭代防止过拟合.
>
> - utilize invariance（利用不变性）
>
> - Bayesian（贝叶斯方法）

## 三、经典算法

### 24、在空间上线性可分的两类点，分别向SVM分类的超平面上做投影，这些点在 超平面上的投影仍然是线性可分的吗？

- 对于任意线性可分的两组点，它们在SVM分类的超平面上的投影 都是线性不可分的
- 该问题也可以通过凸优化理论中的超平面分离定理（Separating Hyperplane Theorem，SHT）更加轻巧地解决。该定理描述的是，对于不相交的两 个凸集，存在一个超平面，将两个凸集分离。对于二维的情况，两个凸集间距离 最短两点连线的中垂线就是一个将它们分离的超平面。

### 25、是否存在一组参数使SVM训练误差为0？

- 当所有样本都预测正确，训练误差即为0
- 补充：
  - svm有三种，线性可分SVM（硬间隔），线性SVM（软间隔），非线性SVM（核技巧）