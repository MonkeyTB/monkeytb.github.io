I"1<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

<h2 id="交叉熵损失函数---cross-entropy-loss-function">交叉熵损失函数 - Cross entropy loss function</h2>

<p>​	标准形式
\(\color{blue}{C=-\frac{1}{n}\sum_x{[ylna+(1-y)ln(1-a)]}}\)
其中$\color{blue}{x}$表示样本，$\color{blue}{y}$表示实际的标签，$\color{blue}{a}$表示预测的输出，$\color{blue}{n}$表示样本总数量。</p>

<h3 id="特点">特点</h3>

<ul>
  <li>
    <p>本质上是一种对数似然函数，可用于二分类和多分类中。二分类问题中的loss函数（输入数据是softmax或者sigmoid）的输出：
\(\color{blue}{loss=-\frac{1}{n}\sum_x[ylna+(1-y)ln(1-a)]}\)
多分类问题中loss函数（输入数据是softmax或者sigmoid）的输出：
\(\color{blue}{loss=-\frac{1}{n}\sum_iy_ilna_i}\)</p>
  </li>
  <li>
    <p>当使用sigmoid作为激活函数时，常用交叉熵损失函数而不用均方差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有$\color{yellow}{”误差大的时候，权重更新快；误差小的时候，权重更新慢}$的良好兴致。</p>
  </li>
</ul>

<h3 id="代码">代码</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">P</span><span class="p">):</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">float_</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">float_</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span> <span class="n">Y</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">P</span><span class="p">)</span> <span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="提问">提问</h3>

<h4 id="1在用sigmoid作为激活函数时你为什么要用交叉熵损失函数而不用均方差损失函数啊">1、在用sigmoid作为激活函数时，你为什么要用交叉熵损失函数，而不用均方差损失函数啊？</h4>

<p>​	均方误差的损失函数，定义为：$\color{blue}{C=\frac{1}{2n}\sum_x(a-y)^2}$，其中$\color{blue}{y}$是label，$\color{blue}{a}$是实际输出($\color{blue}{a=\sigma(z),z=wx+b}$)，在训练神经网络的时候我们使用梯度下降方式来更新$\color{blue}{w}$和$\color{blue}{b}$，因此对$\color{blue}{w}$和$\color{blue}{b}$求导：
\(\color{blue}{\frac{\partial C}{\partial w}=(a-y)\sigma'(z)x}\)</p>

\[\color{blue}{\frac{\partial C}{\partial b} = (a-y)\sigma'(z)}\]

<p>​	更新参数$\color{blue}{w}$和$\color{blue}{b}$：
\(\color{blue}{w = w-\eta\frac{\partial C}{\partial w} = w-\eta(a-y)\sigma'(z)x }\)</p>

\[\color{blue}{b = b-\eta\frac{\partial C}{\partial b} = b-\eta(a-y)\sigma'(z)}\]

<p>由于sigmoid的性质，导致$\color{blue}{\sigma’(x)}$在$\color{blue}{z}$取大部分值时会很小，这样会使得$\color{blue}{\eta(a-y)\sigma’(z)}$很小，导致$\color{blue}{w}$和$\color{blue}{b}$更新非常慢。</p>

<p>​	交叉熵损失函数，定义为：$\color{blue}{C=-\frac{1}{n}\sum_x{[ylna+(1-y)ln(1-a)]}}$，字母定义如上，同样看其对$\color{blue}{w}$和$\color{blue}{b}$求导：
\(\color{blue}{\frac{\partial C}{\partial a}=-\frac{1}{n}\sum_x{[y\frac{1}{a}+(y-1)\frac{1}{1-a}]}=-\frac{1}{n}\sum_x{[\frac{1}{a(1-a)}y-\frac{1}{1-a}]}}\\ \color{blue}{=-\frac{1}{n}\sum_x{[\frac{1}{\sigma(x)(1-\sigma(x))}y-\frac{1}{1-\sigma(x)})]}}\)</p>

\[\color{blue}{\frac{\partial C}{\partial z}=\frac{\partial C}{\partial a}\frac{\partial a}{\partial z}=-\frac{1}{n}\sum_x{[\frac{1}{\sigma(x)(1-\sigma(x))}y-\frac{1}{1-\sigma(x)}]\bullet \sigma'(x)}}\]

\[\color{blue}{=-\frac{1}{n}\sum_x{[\frac{1}{\sigma(x)(1-\sigma(x))}y-\frac{1}{1-\sigma(x)}]\bullet \sigma(x)(1-\sigma(x))} = -\frac{1}{n}\sum_x{y-a}}\]

<p>所以有：
\(\color{blue}{\frac{\partial C}{\partial w}=\frac{\partial C}{\partial z}\frac{\partial z}{\partial w}=(a-y)x}\)</p>

\[\color{blue}{\frac{\partial C}{\partial b}=\frac{\partial C}{\partial z}\frac{\partial z}{\partial b}=(a-y)}\]

<p>所以更新公式为：
\(\color{blue}{w=-\eta\frac{\partial C}{\partial w}=w-\eta(a-y)x}\)</p>

\[\color{blue}{b=-\eta\frac{\partial C}{\partial b}=b-\eta(a-y) }\]

<p>可以看到参数更新公式中没有$\color{blue}{\sigma’(x)}$这一项，权重更新受$\color{blue}{a-y}$影响，收到误差的影响，当$\color{yellow}{当误差大的时候，权重更新快，当误差小的时候，权重更新慢}$，这是一个很好的性质。</p>

<p>所以当使用sigmoid作为激活函数的时候，常用<strong>交叉熵损失函数</strong>而不用<strong>均方误差损失函数</strong>。</p>

<ul>
  <li>回归问题通常用均方差损失函数，可以保证损失函数是个凸函数，即可以得到最优解。而分类问题如果用均方差的话，损失函数的表现不是凸函数，就很难得到最优解。而交叉熵函数可以保证区间内单调。</li>
  <li>分类问题的最后一层网络，需要分类函数，Sigmoid或者Softmax，如果再接均方差函数的话，其求导结果复杂，运算量比较大。用交叉熵函数的话，可以得到比较简单的计算结果，一个简单的减法就可以得到反向误差。</li>
</ul>

<h4 id="2交叉熵函数与似然函数的滚滚红尘">2、交叉熵函数与似然函数的滚滚红尘</h4>

<ul>
  <li>联系</li>
</ul>

<p>​	似然函数的通用形式$\color{blue}{L=P(X;\theta)}$，$\color{yellow}{似然函数的定义没有限定样本集X的分布函数}$。比如在机器学习中，将似然函数用于模型的损失函数，那么对应的样本集$\color{blue}{X}$就是样本集中的label，参数也不是机器学习的model的参数，而是prediction，作为一个损失函数，我们不关心选哪个的参数是怎么样的，而只接受两部分【prediction、label】，还有一个隐含的输入【分布模型】。</p>

<p>​	似然函数最终的目的就是衡量prediction背后的模型对于当前观测值的解释程度，而每个样本的prediction值恰恰就是它所服从的分布模型的参数。</p>

<p>​	机器学习分类任务中，我们假设prediction这个随机变量背后的模型是<strong>单次观测下的多项式分布</strong>？？？</p>

<p>​	这里的单词观测就意味着独立同分布，统计学中☞一组随机变量中每个变量的概率分布都相同，且这些随机变量相互独立。</p>

<ul>
  <li>
    <p>伯努利分布</p>

    <p>​	也叫两点分布，类似抛硬币，因此概率密度函数为：
\(\color{blue}{f_X(x)=p^x(1-p)^{1-x}={\begin{cases}p&amp;{x=1,} \\q&amp;{x= 0.}\end{cases}}}\)
伯努利分布模型参数就是其中一个类别发生的概率。</p>
  </li>
  <li>
    <p>二项分布</p>

    <p>​	二项分布就是将伯努利实验重复n次</p>
  </li>
  <li>
    <p>多项式分布</p>

    <p>​	多项式分布就是将二项分布推广到多个面（类别），即$\color{blue}{f_{multi}(x;p)=\prod_{i=1}^Cp_{i}^{x_i}}$，其中C代表类别数，p代表向量形式的模型参数，即各个类别发生的概率，如$\color{blue}{p=[0.1,0.1,0.7,0.1]}$，则$\color{blue}{p1=0.1,p3=0.7}$。即多项分布的模型参数就是各个类别发生概率，x代表ont-hot形式的观测值。</p>
  </li>
</ul>

<p>根据似然函数定义，单个样本的似然函数即：
\(\color{blue}{L=f_{multi}(label;prediction)}\)
所以整个样本集或batch的似然函数：
\(\color{blue}{L=\prod_{X}f_{multi}(lable;prediction)=\prod_X\prod_{i=1}^Cprediction(i)^{label(i)}}\)
由于有累积运算，所以家隔log函数转为累加以提高运算速度。如下：
\(\color{blue}{L=\sum_X\sum_{i=1}^Clabel(i)log(prediction(i))}\)
而最大化似然函数就是等效于最小化负对数似然函数，所以前面加个负号后就和交叉熵的形式一摸一样。对于某种分布的随机变量$\color{blue}{X\sim p(x)}$，有一个模型$\color{blue}{q(x)}$用于近似$\color{blue}{p(x)}$的概率分布，则分布$\color{blue}{X}$与模型$\color{blue}{q}$之间的交叉熵即：
\(\color{blue}{H(X,q)=-\sum_xp(x)logq(x)}\)
 这里X的分布模型即样本集label的真实分布模型，这里模型$\color{blue}{q(x)}$即想要模拟真实分布模型的机器学习模型。可以说交叉熵是直接衡量两个分布，或者说两个model之间的差异。而似然函数则是解释以model的输出为参数的某分布模型对样本集的解释程度。因此，可以说这两者是“同貌不同源”，但是“殊途同归”啦。</p>

<p>而苏神也在<a href="https://spaces.ac.cn/archives/4277#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6">这里</a>在nlp角度做过类似的推到分析，可以参考。</p>

<p>$\color{red}{上面就是讨论了交叉熵和似然函数的联系，下面简要说一下区别}$</p>

<ul>
  <li>
    <p>区别</p>

    <p>交叉熵函数使用来描述模型预测值和真实值的差距大小，越大代表越不相近，似然函数的本质就是衡量在某个参数下，整个估计和真实情况一样的概率，越大代表越相近。</p>
  </li>
</ul>

<h4 id="3交叉熵和kl散度的种种因缘">3、交叉熵和KL散度的种种因缘</h4>

<ul>
  <li>
    <p>相对熵（KL散度）</p>

    <p>​	对同一个随机变量$\color{blue}{x}$有两个单独的概率分布$\color{blue}{P(x)}和\color{blue}{Q(x)}$，我们可以使用KL散度来衡量这两个分布的差异，这个相当于信息论范畴的均方差。即：
\(\color{blue}{D_{KL}(P\|\|q)=\sum_{j=1}^n(x_j)ln\frac{p(x_j)}{q(x_j)}}\)
$\color{blue}{n}$为事件的所有可能性。$\color{blue}{D}$的值越小，表示$\color{blue}{q}$分布和$\color{blue}{p}$分布越接近。</p>

    <p>☞变形记
\(\color{blue}{D_{KL}(p\|\|q)=\sum_{j=1}^n(x_j)lnp(x_j)-\sum_{j=1}^{n}p(x_j)lnq(x_j)=-H(p(x))+H(p,q)}\)
等式前一部分恰巧就是$\color{blue}{p}$的熵，等式的后一部分就是交叉熵。在机器学习中，我们将$\color{blue}{p,q}$替换为标签$\color{blue}{y}$和预测值$\color{blue}{a}$，即$\color{blue}{D_{KL}(y||a)}$，就可以衡量评估了。KL散度前一项$\color{blue}{H(y)}$不变，所以在优化的过程中需要要关注交叉熵就行，即:
\(\color{blue}{loss=-\sum_{j=1}^ny_jlna_j }\)
公式(21)是单个样本的情况，$\color{blue}{n}$并不是样本个数，而是分类个数，所以批量交叉熵计算公式是：
\(\color{blue}{J=-\sum_{i=1}^m\sum_{j=1}^ny_{ij}lna_{ij}}\)
其中$\color{blue}{m}$是样本数，$\color{blue}{n}$是分类数。</p>
  </li>
</ul>

<h2 id="巨人的肩膀">巨人的肩膀</h2>

<p><a href="https://zhuanlan.zhihu.com/p/58883095">1、常见的损失函数(loss function)总结</a></p>

<p><a href="https://zhuanlan.zhihu.com/p/27719875">2、哈？你还认为似然函数跟交叉熵是一个意思呀？</a></p>

<p><a href="https://spaces.ac.cn/archives/4277#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6">3、最大似然</a></p>

<p><a href="https://microsoft.github.io/ai-edu/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/A2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/%E7%AC%AC1%E6%AD%A5%20-%20%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/03.2-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html">4、交叉熵损失函数</a></p>
:ET