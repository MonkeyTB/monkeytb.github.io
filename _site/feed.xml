<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YiYao</title>
    <description>欢迎来到我的个人站~</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 03 Apr 2022 20:54:33 +0800</pubDate>
    <lastBuildDate>Sun, 03 Apr 2022 20:54:33 +0800</lastBuildDate>
    <generator>Jekyll v4.2.1</generator>
    
      <item>
        <title>交叉熵损失函数</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;交叉熵损失函数---cross-entropy-loss-function&quot;&gt;交叉熵损失函数 - Cross entropy loss function&lt;/h2&gt;

&lt;p&gt;​	标准形式
\(\color{blue}{C=-\frac{1}{n}\sum_x{[ylna+(1-y)ln(1-a)]}}\)
其中$\color{blue}{x}$表示样本，$\color{blue}{y}$表示实际的标签，$\color{blue}{a}$表示预测的输出，$\color{blue}{n}$表示样本总数量。&lt;/p&gt;

&lt;h3 id=&quot;特点&quot;&gt;特点&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;本质上是一种对数似然函数，可用于二分类和多分类中。二分类问题中的loss函数（输入数据是softmax或者sigmoid）的输出：
\(\color{blue}{loss=-\frac{1}{n}\sum_x[ylna+(1-y)ln(1-a)]}\)
多分类问题中loss函数（输入数据是softmax或者sigmoid）的输出：
\(\color{blue}{loss=-\frac{1}{n}\sum_iy_ilna_i}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当使用sigmoid作为激活函数时，常用交叉熵损失函数而不用均方差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有$\color{yellow}{”误差大的时候，权重更新快；误差小的时候，权重更新慢}$的良好兴致。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;代码&quot;&gt;代码&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;提问&quot;&gt;提问&lt;/h3&gt;

&lt;h4 id=&quot;1在用sigmoid作为激活函数时你为什么要用交叉熵损失函数而不用均方差损失函数啊&quot;&gt;1、在用sigmoid作为激活函数时，你为什么要用交叉熵损失函数，而不用均方差损失函数啊？&lt;/h4&gt;

&lt;p&gt;​	均方误差的损失函数，定义为：$\color{blue}{C=\frac{1}{2n}\sum_x(a-y)^2}$，其中$\color{blue}{y}$是label，$\color{blue}{a}$是实际输出($\color{blue}{a=\sigma(z),z=wx+b}$)，在训练神经网络的时候我们使用梯度下降方式来更新$\color{blue}{w}$和$\color{blue}{b}$，因此对$\color{blue}{w}$和$\color{blue}{b}$求导：
\(\color{blue}{\frac{\partial C}{\partial w}=(a-y)\sigma'(z)x}\)&lt;/p&gt;

\[\color{blue}{\frac{\partial C}{\partial b} = (a-y)\sigma'(z)}\]

&lt;p&gt;​	更新参数$\color{blue}{w}$和$\color{blue}{b}$：
\(\color{blue}{w = w-\eta\frac{\partial C}{\partial w} = w-\eta(a-y)\sigma'(z)x }\)&lt;/p&gt;

\[\color{blue}{b = b-\eta\frac{\partial C}{\partial b} = b-\eta(a-y)\sigma'(z)}\]

&lt;p&gt;由于sigmoid的性质，导致$\color{blue}{\sigma’(x)}$在$\color{blue}{z}$取大部分值时会很小，这样会使得$\color{blue}{\eta(a-y)\sigma’(z)}$很小，导致$\color{blue}{w}$和$\color{blue}{b}$更新非常慢。&lt;/p&gt;

&lt;p&gt;​	交叉熵损失函数，定义为：$\color{blue}{C=-\frac{1}{n}\sum_x{[ylna+(1-y)ln(1-a)]}}$，字母定义如上，同样看其对$\color{blue}{w}$和$\color{blue}{b}$求导：
\(\color{blue}{\frac{\partial C}{\partial a}=-\frac{1}{n}\sum_x{[y\frac{1}{a}+(y-1)\frac{1}{1-a}]}=-\frac{1}{n}\sum_x{[\frac{1}{a(1-a)}y-\frac{1}{1-a}]}}\\ \color{blue}{=-\frac{1}{n}\sum_x{[\frac{1}{\sigma(x)(1-\sigma(x))}y-\frac{1}{1-\sigma(x)})]}}\)&lt;/p&gt;

\[\color{blue}{\frac{\partial C}{\partial z}=\frac{\partial C}{\partial a}\frac{\partial a}{\partial z}=-\frac{1}{n}\sum_x{[\frac{1}{\sigma(x)(1-\sigma(x))}y-\frac{1}{1-\sigma(x)}]\bullet \sigma'(x)}}\]

\[\color{blue}{=-\frac{1}{n}\sum_x{[\frac{1}{\sigma(x)(1-\sigma(x))}y-\frac{1}{1-\sigma(x)}]\bullet \sigma(x)(1-\sigma(x))} = -\frac{1}{n}\sum_x{y-a}}\]

&lt;p&gt;所以有：
\(\color{blue}{\frac{\partial C}{\partial w}=\frac{\partial C}{\partial z}\frac{\partial z}{\partial w}=(a-y)x}\)&lt;/p&gt;

\[\color{blue}{\frac{\partial C}{\partial b}=\frac{\partial C}{\partial z}\frac{\partial z}{\partial b}=(a-y)}\]

&lt;p&gt;所以更新公式为：
\(\color{blue}{w=-\eta\frac{\partial C}{\partial w}=w-\eta(a-y)x}\)&lt;/p&gt;

\[\color{blue}{b=-\eta\frac{\partial C}{\partial b}=b-\eta(a-y) }\]

&lt;p&gt;可以看到参数更新公式中没有$\color{blue}{\sigma’(x)}$这一项，权重更新受$\color{blue}{a-y}$影响，收到误差的影响，当$\color{yellow}{当误差大的时候，权重更新快，当误差小的时候，权重更新慢}$，这是一个很好的性质。&lt;/p&gt;

&lt;p&gt;所以当使用sigmoid作为激活函数的时候，常用&lt;strong&gt;交叉熵损失函数&lt;/strong&gt;而不用&lt;strong&gt;均方误差损失函数&lt;/strong&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;回归问题通常用均方差损失函数，可以保证损失函数是个凸函数，即可以得到最优解。而分类问题如果用均方差的话，损失函数的表现不是凸函数，就很难得到最优解。而交叉熵函数可以保证区间内单调。&lt;/li&gt;
  &lt;li&gt;分类问题的最后一层网络，需要分类函数，Sigmoid或者Softmax，如果再接均方差函数的话，其求导结果复杂，运算量比较大。用交叉熵函数的话，可以得到比较简单的计算结果，一个简单的减法就可以得到反向误差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2交叉熵函数与似然函数的滚滚红尘&quot;&gt;2、交叉熵函数与似然函数的滚滚红尘&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;联系&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​	似然函数的通用形式$\color{blue}{L=P(X;\theta)}$，$\color{yellow}{似然函数的定义没有限定样本集X的分布函数}$。比如在机器学习中，将似然函数用于模型的损失函数，那么对应的样本集$\color{blue}{X}$就是样本集中的label，参数也不是机器学习的model的参数，而是prediction，作为一个损失函数，我们不关心选哪个的参数是怎么样的，而只接受两部分【prediction、label】，还有一个隐含的输入【分布模型】。&lt;/p&gt;

&lt;p&gt;​	似然函数最终的目的就是衡量prediction背后的模型对于当前观测值的解释程度，而每个样本的prediction值恰恰就是它所服从的分布模型的参数。&lt;/p&gt;

&lt;p&gt;​	机器学习分类任务中，我们假设prediction这个随机变量背后的模型是&lt;strong&gt;单次观测下的多项式分布&lt;/strong&gt;？？？&lt;/p&gt;

&lt;p&gt;​	这里的单词观测就意味着独立同分布，统计学中☞一组随机变量中每个变量的概率分布都相同，且这些随机变量相互独立。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;伯努利分布&lt;/p&gt;

    &lt;p&gt;​	也叫两点分布，类似抛硬币，因此概率密度函数为：
\(\color{blue}{f_X(x)=p^x(1-p)^{1-x}={\begin{cases}p&amp;amp;{x=1,} \\q&amp;amp;{x= 0.}\end{cases}}}\)
伯努利分布模型参数就是其中一个类别发生的概率。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;二项分布&lt;/p&gt;

    &lt;p&gt;​	二项分布就是将伯努利实验重复n次&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;多项式分布&lt;/p&gt;

    &lt;p&gt;​	多项式分布就是将二项分布推广到多个面（类别），即$\color{blue}{f_{multi}(x;p)=\prod_{i=1}^Cp_{i}^{x_i}}$，其中C代表类别数，p代表向量形式的模型参数，即各个类别发生的概率，如$\color{blue}{p=[0.1,0.1,0.7,0.1]}$，则$\color{blue}{p1=0.1,p3=0.7}$。即多项分布的模型参数就是各个类别发生概率，x代表ont-hot形式的观测值。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据似然函数定义，单个样本的似然函数即：
\(\color{blue}{L=f_{multi}(label;prediction)}\)
所以整个样本集或batch的似然函数：
\(\color{blue}{L=\prod_{X}f_{multi}(lable;prediction)=\prod_X\prod_{i=1}^Cprediction(i)^{label(i)}}\)
由于有累积运算，所以家隔log函数转为累加以提高运算速度。如下：
\(\color{blue}{L=\sum_X\sum_{i=1}^Clabel(i)log(prediction(i))}\)
而最大化似然函数就是等效于最小化负对数似然函数，所以前面加个负号后就和交叉熵的形式一摸一样。对于某种分布的随机变量$\color{blue}{X\sim p(x)}$，有一个模型$\color{blue}{q(x)}$用于近似$\color{blue}{p(x)}$的概率分布，则分布$\color{blue}{X}$与模型$\color{blue}{q}$之间的交叉熵即：
\(\color{blue}{H(X,q)=-\sum_xp(x)logq(x)}\)
 这里X的分布模型即样本集label的真实分布模型，这里模型$\color{blue}{q(x)}$即想要模拟真实分布模型的机器学习模型。可以说交叉熵是直接衡量两个分布，或者说两个model之间的差异。而似然函数则是解释以model的输出为参数的某分布模型对样本集的解释程度。因此，可以说这两者是“同貌不同源”，但是“殊途同归”啦。&lt;/p&gt;

&lt;p&gt;而苏神也在&lt;a href=&quot;https://spaces.ac.cn/archives/4277#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6&quot;&gt;这里&lt;/a&gt;在nlp角度做过类似的推到分析，可以参考。&lt;/p&gt;

&lt;p&gt;$\color{red}{上面就是讨论了交叉熵和似然函数的联系，下面简要说一下区别}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;区别&lt;/p&gt;

    &lt;p&gt;交叉熵函数使用来描述模型预测值和真实值的差距大小，越大代表越不相近，似然函数的本质就是衡量在某个参数下，整个估计和真实情况一样的概率，越大代表越相近。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3交叉熵和kl散度的种种因缘&quot;&gt;3、交叉熵和KL散度的种种因缘&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;相对熵（KL散度）&lt;/p&gt;

    &lt;p&gt;​	对同一个随机变量$\color{blue}{x}$有两个单独的概率分布$\color{blue}{P(x)}和\color{blue}{Q(x)}$，我们可以使用KL散度来衡量这两个分布的差异，这个相当于信息论范畴的均方差。即：
\(\color{blue}{D_{KL}(P\|\|q)=\sum_{j=1}^n(x_j)ln\frac{p(x_j)}{q(x_j)}}\)
$\color{blue}{n}$为事件的所有可能性。$\color{blue}{D}$的值越小，表示$\color{blue}{q}$分布和$\color{blue}{p}$分布越接近。&lt;/p&gt;

    &lt;p&gt;☞变形记
\(\color{blue}{D_{KL}(p\|\|q)=\sum_{j=1}^n(x_j)lnp(x_j)-\sum_{j=1}^{n}p(x_j)lnq(x_j)=-H(p(x))+H(p,q)}\)
等式前一部分恰巧就是$\color{blue}{p}$的熵，等式的后一部分就是交叉熵。在机器学习中，我们将$\color{blue}{p,q}$替换为标签$\color{blue}{y}$和预测值$\color{blue}{a}$，即$\color{blue}{D_{KL}(y||a)}$，就可以衡量评估了。KL散度前一项$\color{blue}{H(y)}$不变，所以在优化的过程中需要要关注交叉熵就行，即:
\(\color{blue}{loss=-\sum_{j=1}^ny_jlna_j }\)
公式(21)是单个样本的情况，$\color{blue}{n}$并不是样本个数，而是分类个数，所以批量交叉熵计算公式是：
\(\color{blue}{J=-\sum_{i=1}^m\sum_{j=1}^ny_{ij}lna_{ij}}\)
其中$\color{blue}{m}$是样本数，$\color{blue}{n}$是分类数。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;巨人的肩膀&quot;&gt;巨人的肩膀&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/58883095&quot;&gt;1、常见的损失函数(loss function)总结&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/27719875&quot;&gt;2、哈？你还认为似然函数跟交叉熵是一个意思呀？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://spaces.ac.cn/archives/4277#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6&quot;&gt;3、最大似然&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://microsoft.github.io/ai-edu/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/A2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/%E7%AC%AC1%E6%AD%A5%20-%20%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/03.2-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html&quot;&gt;4、交叉熵损失函数&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Apr 2022 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2022/04/%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/04/%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>GPT1-GPT3</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;

&lt;p&gt;​	GPT（Generative Pre-trained Transformer）系列是由OpenAI提出的非常强大的预训练语言模型，这一系列的模型可以在生成式任务中取得非常好的效果，对于一个新的任务，GTP只需要很少的数据便可以理解任务的需求并达到或接近state-of-the-art的方法。&lt;/p&gt;

&lt;p&gt;​	GPT训练需要超大的训练语料以及堆叠的transformer，123系列具体情况如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;模型&lt;/th&gt;
      &lt;th&gt;发布时间&lt;/th&gt;
      &lt;th&gt;参数量&lt;/th&gt;
      &lt;th&gt;预训练数据量&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-1&lt;/td&gt;
      &lt;td&gt;2018年6月&lt;/td&gt;
      &lt;td&gt;1.17亿&lt;/td&gt;
      &lt;td&gt;约5GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-2&lt;/td&gt;
      &lt;td&gt;2019年2月&lt;/td&gt;
      &lt;td&gt;15亿&lt;/td&gt;
      &lt;td&gt;40G&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3&lt;/td&gt;
      &lt;td&gt;2020年5月&lt;/td&gt;
      &lt;td&gt;1750亿&lt;/td&gt;
      &lt;td&gt;45TB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;​	GPT的三个模型几乎都是&lt;strong&gt;相同架构&lt;/strong&gt;, 只是有非常非常少量的改动. 但一代比一代更大, 也更烧钱. 所以我对GPT系列的特点就是: &lt;strong&gt;钞能力, 大就完事了.&lt;/strong&gt; 其影响力和花费的成本是成正比的.&lt;/p&gt;

&lt;h2 id=&quot;gpt-1通过生成式预训练提高语言理解&quot;&gt;GPT-1（&lt;a href=&quot;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&quot;&gt;通过生成式预训练提高语言理解&lt;/a&gt;）&lt;/h2&gt;

&lt;p&gt;​	传统的NLP模型往往使用大量的数据对有监督模型进行训练，这种任务存在两个明显的缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;需要大量的标注数据，高质量的数据往往很难获得；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;一个任务训练的模型很难泛化到其他任务中，这个模型只能叫“领域专家”而不是真正的理解NLP。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-1&lt;/code&gt;先通过无标签数据学习一个生成式的语言模型，然后再根据特定任务进行微调。任务包括&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;自然语言推理（NLI）：判断两个句子是否包含关系，矛盾关系，中立关系；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;问答和常识推理：类似多选题，输入文章，问题以及若干候选答案，输出为每个答案的概率；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;语义相似度：判断两个句子语义上是否相关；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;分类：判断输入文本是指定的那个类别。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;数据集&quot;&gt;数据集&lt;/h3&gt;

&lt;p&gt;​	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-1&lt;/code&gt;使用的数据集BooksCorpus，包含7000本没有发布的书籍。使用原因由两点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;有助于未见过的数据集上训练语言模型，该数据不太可能在下游任务的测试集中找到；&lt;/li&gt;
  &lt;li&gt;数据集拥有长的合理的上下文依赖关系，使得模型学习更长的依赖关系。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;网络结构&quot;&gt;网络结构&lt;/h3&gt;

&lt;p&gt;​	使用12层仅解码器带有掩码自注意力的transormer，掩码的使用使模型看不见未来的信息，得到模型泛化能力更强。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;实现细节&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用字节对编码（BPE：byte pair encoding），共有4000个字节对；&lt;/li&gt;
  &lt;li&gt;词编码长度为768；&lt;/li&gt;
  &lt;li&gt;位置编码也需要学习；&lt;/li&gt;
  &lt;li&gt;12层transformer，每个transformer块有12个头；&lt;/li&gt;
  &lt;li&gt;位置编码的长度是3072；&lt;/li&gt;
  &lt;li&gt;Attention，残差，Dropout等机制用来进行正则化，drop比例为0.1；&lt;/li&gt;
  &lt;li&gt;激活函数GLUE；&lt;/li&gt;
  &lt;li&gt;训练的batchsize为64，学习率为$\color{blue}{2.5e^{-4}}$，序列长度为512，序列epoch为100。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;训练&quot;&gt;训练&lt;/h3&gt;

&lt;p&gt;​	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-1&lt;/code&gt;的训练分为无监督训练和有监督微调，详细过程如下：&lt;/p&gt;

&lt;h4 id=&quot;无监督预训练&quot;&gt;&lt;strong&gt;无监督预训练&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;​	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-1&lt;/code&gt;的无监督预训练是基于语言模型进行训练的，给定一个无标签的序列$\color{blue}{U={u_1,…,u_n}}$，语言模型的优化目标是最大化似然函数：
\(\color{blue}{L_1(U)=\sum_xlogP(u_i|u_{i-k},...,u_{i-1};\theta)-------(1)}\)
其中$\color{blue}{k}$是滑动窗口的大小，$\color{blue}{P}$是条件概率，$\color{blue}{\theta}$是模型参数，这些参数使用SGD进行优化。在&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-1&lt;/code&gt;中，使用12个transformer块的结构作为解码器，每个transformer块是一个多头注意力机制，然后通过全连接得到输出的概率分布。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/GPT-1.jpg&quot; alt=&quot;&quot; /&gt;
\(\color{blue}{h_0=UW_e+W_p---------------(2)}\\ \color{blue}{h_l=transformer_block(h_{l-1})\forall i\in[1,n])-----(3)}\\ \color{blue}{P(u)=softmax(h_nW_e^T)------------(4)}\)
​	其中 $\color{blue}{U=(u_{-k},\dots,u_{-1})}$是当前时间片的上下文token，$\color{blue}{n}$是层数，$\color{blue}{w_e}$是词嵌入矩阵，$\color{blue}{W_p}$是位置嵌入矩阵。&lt;/p&gt;

&lt;h4 id=&quot;有监督微调&quot;&gt;&lt;strong&gt;有监督微调&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;​	当得到无监督预训练模型之后，我们将它的值应用到有监督任务中，对于一个有标签的数据集$\color{blue}{C}$，每个实例有$\color{blue}{m}$个输入token：$\color{blue}{x^1,…,x^m}$，它对应的标签$\color{blue}{y}$组成。首先将这些token输入到训练好的预训练模型中，得到最终的特征向量$\color{blue}{h_l^m}$。然后再通过一个全连接层得到预测结果$\color{blue}{y}$：
\(\color{blue}{P(y|x^1, ..., x^m)=softmax(h_l^mW_y)--------(5)}\)
其中$\color{blue}{W_y}$为全连接层的参数。有监督的目标则是最大化公式（5）的值：
\(\color{blue}{L_2(X)=\sum_{x,y}logP(y|x^1,...,x^m)----------(6)}\)
作者并没有直接使用$\color{blue}{L_2}$，而是向其中加入了$\color{blue}{L_1}$，并通过$\color{blue}{\lambda}$调节权重，一般采用0.5，公式(7)如下：
\(\color{blue}{L_3(C)=L_2(C)+\lambda L_1(C) -----------(7)}\)&lt;/p&gt;

&lt;h3 id=&quot;相关任务输入变化&quot;&gt;相关任务输入变化&lt;/h3&gt;

&lt;p&gt;​	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-1&lt;/code&gt;对不同的任务形式有不同的输入形式，具体如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;分类任务：将起始和终止的token加入到原始序列两端，输入transformer中得到特征向量，最后经过一个全连接层得到预测的概率分布；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;NLI任务：将前提和假设同通过分割符隔开，两端加上起始和终止token，再一次通过transformer和全连接得到预测结果；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;语义相似度任务：输入两个句子，正向和反向各拼接一次，然后分别输入给transformer，得到特征向量拼接后再送给全连接得到预测结果；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;问答和常识推断：将$\color{blue}{n}$个选项的问题抽象化为$\color{blue}{n}$二分类问题，即每个选项分别和内容进行拼接，然后各送入transformer和全连接中，最后选择置信度最高的作为预测结果。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/GPT-1下游任务.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;对比bert&quot;&gt;对比bert&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT&lt;/code&gt;在预训练时并没有引入&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[CLS]&lt;/code&gt;和&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[SEP]&lt;/code&gt;, Bert全程引入；&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Bert&lt;/code&gt;是真正能够捕捉所有层上下文信息的，受益于自注意力机制，将所有token距离直接缩短到1，GPT-1是单向捕捉信息；&lt;/li&gt;
  &lt;li&gt;input： &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT&lt;/code&gt;采用&lt;strong&gt;BPE&lt;/strong&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Bert&lt;/code&gt;用了&lt;strong&gt;Word Piece&lt;/strong&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT&lt;/code&gt;有位置编码, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Bert&lt;/code&gt;有位置编码和段编码.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://monkeytb.github.io/2022/01/BERT/&quot;&gt;$\color{red}{Bert 详见我另一篇blog}$&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;gpt-2语言模型式无监督的多任务学习&quot;&gt;GPT-2（&lt;a href=&quot;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;语言模型式无监督的多任务学习&lt;/a&gt;）&lt;/h2&gt;

&lt;p&gt;​	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-2&lt;/code&gt;的目标是为了训练一个泛化能力更强的词向量模型，它并没有对&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-1&lt;/code&gt;的网络机构进行过多的结构创新和设计，只是使用了更大的数据集和更大的网络参数。&lt;/p&gt;

&lt;h3 id=&quot;数据集-1&quot;&gt;数据集&lt;/h3&gt;

&lt;p&gt;​	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-2&lt;/code&gt;的文章取自于Reddit上高赞的文章，命名为WebText。数据集共有约800万篇文章，累计体积约40G。为了避免和测试集的冲突，WebText移除了涉及Wikipedia的文章。&lt;/p&gt;

&lt;h3 id=&quot;网络结构-1&quot;&gt;网络结构&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;同样使用了字节对编码构建字典，字典大小为50257；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;有48层，词嵌入大小1600维;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;滑动窗口大小为1024；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;batchsize 512；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Layer Normalization移动到了每一块的输入部分，在每个self-attention知后额外添加了一个Layer Normalization；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将残差层的初始化值用$\color{blue}{1/\sqrt{N}}$，其中&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt;是残差层的个数。&lt;/p&gt;

    &lt;p&gt;​	GPT-2训练了4组不同的层数和词向量的长度的模型，具体值见表2。通过这4个模型的实验结果我们可以看出随着模型的增大，模型的效果是不断提升的。&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt;参数量&lt;/th&gt;
          &lt;th&gt;层数&lt;/th&gt;
          &lt;th&gt;词向量长度&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;117M（GPT-1）&lt;/td&gt;
          &lt;td&gt;12&lt;/td&gt;
          &lt;td&gt;769&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;345M&lt;/td&gt;
          &lt;td&gt;24&lt;/td&gt;
          &lt;td&gt;1024&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;762M&lt;/td&gt;
          &lt;td&gt;36&lt;/td&gt;
          &lt;td&gt;1280&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;1542M&lt;/td&gt;
          &lt;td&gt;48&lt;/td&gt;
          &lt;td&gt;1600&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gpt-2小结&quot;&gt;GPT-2小结&lt;/h3&gt;

&lt;p&gt;​	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-2&lt;/code&gt;的最大贡献是验证了通过海量数据和大量参数训练出来的词向量模型有迁移到其它类别任务中而不需要额外的训练。但是很多实验也表明，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-2&lt;/code&gt;的无监督学习的能力还有很大的提升空间，甚至在有些任务上的表现不比随机的好。尽管在有些zero-shot的任务上的表现不错，但是我们仍不清楚&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-2&lt;/code&gt;的这种策略究竟能做成什么样子。&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-2&lt;/code&gt;表明随着模型容量和数据量的增大，其潜能还有进一步开发的空间，基于这个思想，诞生了我们下面要介绍的&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-3&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;gpt-3语言模型少量学习&quot;&gt;GPT-3（&lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;&gt;语言模型少量学习&lt;/a&gt;）&lt;/h2&gt;

&lt;h3 id=&quot;数据集-2&quot;&gt;数据集&lt;/h3&gt;

&lt;p&gt;​	&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-3&lt;/code&gt;共训练了5个不同的语料，分别是低质量的Common Crawl，高质量的WebText2，Books1，Books2和Wikipedia，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-3&lt;/code&gt;根据数据集的不同的质量赋予了不同的权值，权值越高的在训练的时候越容易抽样到。&lt;/p&gt;

&lt;h3 id=&quot;网络结构-2&quot;&gt;网络结构&lt;/h3&gt;

&lt;p&gt;GPT-3沿用了GPT-2的结构，但是在网络容量上做了很大的提升，具体如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GPT-3采用了96层的多头transformer，头的个数为96；&lt;/li&gt;
  &lt;li&gt;词向量的长度是12888 ；&lt;/li&gt;
  &lt;li&gt;上下文划窗的窗口大小提升至2048个token；&lt;/li&gt;
  &lt;li&gt;使用了alternating dense和locally banded sparse attention。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gpt-3-总结&quot;&gt;GPT-3 总结&lt;/h3&gt;

&lt;p&gt;本文讨论了 GPT-3 模型的几个弱点以及有待改进的领域。让我们在这里总结一下。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;尽管 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-3&lt;/code&gt; 能够生成高质量的文本，但有时它会在编写长句子并一遍又一遍地重复文本序列时开始失去连贯性。此外，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-3&lt;/code&gt;在自然语言推理（确定一个句子是否暗示另一个句子）、填空、一些阅读理解任务等任务上表现不佳。该论文引用&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT&lt;/code&gt; 模型的单向性作为可能的原因这些限制并建议以这种规模训练双向模型来克服这些问题；&lt;/li&gt;
  &lt;li&gt;该论文指出的另一个限制是&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-3&lt;/code&gt;的通用语言建模目标，它平等地权衡每个token，并且缺乏任务或面向目标的token预测的概念。为了解决这个问题，本文提出了诸如增强学习目标、使用强化学习来微调模型、添加其他模式等方法；&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-3&lt;/code&gt;的其他限制包括复杂且昂贵的模型推理，因为其结构繁重，语言的可解释性和模型生成的结果较低，以及围绕什么有助于模型实现其少量学习行为的不确定性；&lt;/li&gt;
  &lt;li&gt;除了这些限制之外，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-3&lt;/code&gt;还存在滥用其类人文本生成能力进行网络钓鱼、垃圾邮件、传播错误信息或执行其他欺诈活动的潜在风险。此外，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-3&lt;/code&gt;生成的文本具有其训练所用语言的偏见。&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT-3&lt;/code&gt;生成的文章可能存在性别、种族、种族或宗教偏见。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;巨人的肩膀&quot;&gt;巨人的肩膀&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/350017443&quot;&gt;1、词向量之GPT-1，GPT-2和GPT-3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2&quot;&gt;2、&lt;strong&gt;The Journey of Open AI GPT models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;[https://ADAning.github.io/posts/3996.html](https://adaning.github.io/posts/3996.html)&quot;&gt;3、ELMo, GPT, BERT&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 03 Apr 2022 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2022/04/GPT1%E5%88%B0GPT3/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/04/GPT1%E5%88%B0GPT3/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>BERT</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;&lt;strong&gt;Bert&lt;/strong&gt;全称Bidirectional Encoder Representations from Transformers，再多项NLP任务中引起了轰动，其关键技术创新在于用双向Transformer模型，区别于之前从左向右和从右向左拼接，并添加了MLM的新技术。下面将详细介绍。&lt;/p&gt;

&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;​	计算机视觉领域，通过对大规模数据进行网络训练，然后迁移学习进行微调适用于新的任务，去的很好的效果。近年来，这样的任务也用于许多自然语言任务。在此之前比如ELMo、GPT等。&lt;/p&gt;

&lt;h2 id=&quot;前人工作&quot;&gt;前人工作&lt;/h2&gt;

&lt;h3 id=&quot;elmo&quot;&gt;elmo&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;后续使用，根据任务构造结构&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gpt&quot;&gt;gpt&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;后续使用，单向模型&lt;/li&gt;
  &lt;li&gt;cloze task， 借鉴了mask model&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;创新点&quot;&gt;创新点&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;GPT的局限性，单向的，限制了任务的类型，比如情感分类等&lt;/li&gt;
  &lt;li&gt;MLM（token） 、 NSP（句子）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;工作&quot;&gt;工作&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;非监督的特征方法&lt;/li&gt;
  &lt;li&gt;非监督的微调方法&lt;/li&gt;
  &lt;li&gt;迁移学习&lt;/li&gt;
  &lt;li&gt;细节
    &lt;ul&gt;
      &lt;li&gt;预训练&lt;/li&gt;
      &lt;li&gt;微调&lt;/li&gt;
      &lt;li&gt;模型架构：transformer&lt;/li&gt;
      &lt;li&gt;模型参数&lt;/li&gt;
      &lt;li&gt;切词：wordpiece
        &lt;ul&gt;
          &lt;li&gt;本意是为了减少词汇表，对一些低频词通过拆分表达，这么做的好处是通过拆分的方式对低频词做了相对较好的表达，相比直接用oov或者低频词表示来说，能学到更多的信息。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;mask 概率
        &lt;ul&gt;
          &lt;li&gt;具体来说，文章作者在一句话中随机选择 15% 的词汇用于预测。对于在原句中被抹去的词汇， 80% 情况下采用一个特殊符号 [MASK] 替换， 10% 情况下采用一个任意词替换，剩余 10% 情况下保持原词汇不变。这么做的主要原因是：在后续微调任务中语句中并不会出现 [MASK] 标记，而且这么做的另一个好处是：预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（ 10% 概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。上述提到了这样做的一个缺点，其实这样做还有另外一个缺点，就是每批次数据中只有 15% 的标记被预测，这意味着模型可能需要更多的预训练步骤来收敛。&lt;/li&gt;
          &lt;li&gt;在 BERT 中训练语言模型是通过预测输入中随机选择的 15% 的标记来完成的。这些标记的预处理如下——80% 被替换为“[MASK]”标记，10% 被替换为随机单词，10% 使用原始单词。导致作者选择这种方法的直觉如下（感谢来自 Google 的 Jacob Devlin 的洞察力）：
            &lt;ul&gt;
              &lt;li&gt;如果我们在 100% 的情况下使用 [MASK]，该模型不一定会为非屏蔽词生成良好的标记表示。非屏蔽词仍然用于上下文，但该模型针对预测屏蔽词进行了优化。&lt;/li&gt;
              &lt;li&gt;如果我们 90% 的时间使用 [MASK]，10% 的时间使用随机单词，这将告诉模型观察到的单词&lt;em&gt;永远不会&lt;/em&gt;正确。&lt;/li&gt;
              &lt;li&gt;如果我们 90% 的时间使用 [MASK]，10% 的时间保持相同的单词，那么模型可以简单地复制非上下文嵌入。&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;没有对这种方法的比率进行消融，它可能在不同的比率下效果更好。此外，模型性能没有通过简单地屏蔽 100% 的选定标记进行测试。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;双向性&lt;/li&gt;
  &lt;li&gt;生成类工作不方便
    &lt;ul&gt;
      &lt;li&gt;生成类工作不方便不代表不能用于生成任务，我们可以通过对生成结果进行mask，而模拟出生成效果，具体来说就是生成任务只能看到上文，而不能看到下文，与Bert的任务冲突，但是我们对生成部分用上三角mask，通过这种方式可以保证在生成的过程中只能看到上文，而看不到下文，这样就可以用Bert来做生成任务啦！&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;工作原理&quot;&gt;工作原理&lt;/h2&gt;

&lt;p&gt;​	使用了Transformer，这是一种注意力机制，可以学习文本单词之间的上下文信息。普通的Transformer包含两部分，读取文本输入的编码器和生成的解码器，Bert任务的目标是生成语言模型，因此只需要编码器机制。Transformer介绍&lt;a href=&quot;https://monkeytb.github.io/2021/12/Transformer/&quot;&gt;$\color{red}{Transformer探索}$&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;mlm&quot;&gt;MLM&lt;/h3&gt;

&lt;p&gt;​	将单词序列输入Bert之前，每个序列中15%的单词被替换为[MASK]标记，然后Bert尝试根据非屏蔽的上下文来预测屏蔽的原始词。技术角度来讲需要以下几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在编码器输出之上添加一个分类层；&lt;/li&gt;
  &lt;li&gt;将输出向量乘以嵌入矩阵，将他们转换为词汇维度；&lt;/li&gt;
  &lt;li&gt;用softmax计算词汇表中每个单词的概率。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/论文/bert mlm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	Bert损失函数只考虑掩码值的预测，而忽略非掩码词的预测，因此模型的收敛速度比定向模型慢，这一特征被其增强的上下文感知所抵消。&lt;/p&gt;

&lt;h3 id=&quot;nsp&quot;&gt;NSP&lt;/h3&gt;

&lt;p&gt;​	在Bert训练过程中，模型接受成对的句子作为输入，并学习预测成对中的第二个句子是否是原始文档中后续句子。在训练过程中，50%的输入是一对，其中第二个句子是原始文档中的后续句子，而另外50%的输入是从语料库中随机选择的一个句子作为第二个句子。&lt;/p&gt;

&lt;p&gt;​	为了帮助模型区分训练中的两个句子，输入在进入模型之前按以下方式处理：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在第一个句子的开头插入一个[CLS]标记，在每个句子的末尾插入一个[SEP]标记；&lt;/li&gt;
  &lt;li&gt;表示句子A或句子B的句子嵌入被添加到每个标记中；&lt;/li&gt;
  &lt;li&gt;将位置嵌入添加到每个标记以指示其在序列中的位置。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/论文/bert 输入.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	为了预测第二个句子是否确实与第一个句子相关，执行以下步骤：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;整个输入序列经过Transformer模型&lt;/li&gt;
  &lt;li&gt;[CLS]标记的输出使用简单的分类层转换为$\color{blue}{2\times 1}$形状的向量&lt;/li&gt;
  &lt;li&gt;用softmax计算Is Next Sequence的概率&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在训练Bert模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标是最小化两种策略的组合损失函数。&lt;/p&gt;

&lt;h2 id=&quot;微调&quot;&gt;微调&lt;/h2&gt;

&lt;p&gt;​	将Bert用于特定的任务相对简单，可以用于各种各样的语言任务，而只在核心模型中增加一个小层&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;通过在[CLS]标记的Transformer输出顶部添加分类层，完成诸如情感分析之类的分类任务与Next Sentence分类类似；&lt;/li&gt;
  &lt;li&gt;在问答任务（例如SQuAD v1.1）中，模型收到一个关于文本序列的问题，并需要在序列中标记答案，使用Bert，可以通过学习标记答案开始和结束的两个额外向量来训练问答模型；&lt;/li&gt;
  &lt;li&gt;在命名实体识别中，接受一个文本序列，并需要标记文本中出现的各种类型的实体（人、组织、地点等）。使用Bert，可以通过将每个标记的输出向量输入到预测NER标签分类层来训练NER模型。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;训练相关&quot;&gt;训练相关&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;模型的大小很重要，即使规模很大。Bert large拥有3.45亿个参数，是同类模型中最大的。他在小规模任务上明显优于Bert base，后者“仅”1.1亿个参数；&lt;/li&gt;
  &lt;li&gt;有了足够的训练数据，更多的训练步骤==更高的准确度，eg：在MNLI任务上，与具有相同批量大小的500K步相比，Bert base在1M（批量大小为128000字）上训练时的准确度提高了1%；&lt;/li&gt;
  &lt;li&gt;Bert的双向方法（MLM）的收敛速度比从左到右的方法慢（因为每批中只有15%的单词被预测），但在少量预训练步骤后，双向训练仍然优于从左到右的训练&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/论文/bert 训练 step.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/&quot;&gt;1、BERT Explained&lt;/a&gt;: State of the art language model for NLP&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/95594311&quot;&gt;2、关于BERT的若干问题整理记录&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 01 Jan 2022 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2022/01/BERT/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/01/BERT/</guid>
        
        <category>论文</category>
        
        
      </item>
    
      <item>
        <title>NER谈古论今</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h1 id=&quot;定义&quot;&gt;定义&lt;/h1&gt;

&lt;p&gt;命名实体识别（NER）是自然语言处理领域的核心问题之一，他的目标是为了再一段文本中识别出特定类型的字段，eg：人名、地名等。从而应用到下游任务中。&lt;/p&gt;

&lt;h1 id=&quot;标注类型&quot;&gt;标注类型&lt;/h1&gt;

&lt;p&gt;常见的标注类型BIOES或者BIOS，因为NER识别的是特定的字段，这个字段的start标记为B-xx，结尾标记为E-xx或者I-xx（对应两种标注方式），中间的部分均用I-xx标记，如果出现单独的字为一个实体，那么用S-xx进行标记，其他非提取字段用O进行标注。实例如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;我&lt;/th&gt;
      &lt;th&gt;爱&lt;/th&gt;
      &lt;th&gt;北&lt;/th&gt;
      &lt;th&gt;京&lt;/th&gt;
      &lt;th&gt;天&lt;/th&gt;
      &lt;th&gt;安&lt;/th&gt;
      &lt;th&gt;门&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;O&lt;/td&gt;
      &lt;td&gt;O&lt;/td&gt;
      &lt;td&gt;B-location&lt;/td&gt;
      &lt;td&gt;I-location&lt;/td&gt;
      &lt;td&gt;I-location&lt;/td&gt;
      &lt;td&gt;I-location&lt;/td&gt;
      &lt;td&gt;I-location&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;O&lt;/td&gt;
      &lt;td&gt;O&lt;/td&gt;
      &lt;td&gt;B-location&lt;/td&gt;
      &lt;td&gt;I-location&lt;/td&gt;
      &lt;td&gt;I-location&lt;/td&gt;
      &lt;td&gt;I-location&lt;/td&gt;
      &lt;td&gt;E-location&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;评测指标&quot;&gt;评测指标&lt;/h1&gt;

&lt;p&gt;对于NER识别的评测一般需要保证完整的实体识别准确才行，也就是说不是根据每个字的标签去评测，而是根据完整的字段来进行统计评测。具体的指标和其他分类任务一样，也是准确召回等。&lt;/p&gt;

&lt;h1 id=&quot;理论&quot;&gt;理论&lt;/h1&gt;

&lt;h1 id=&quot;方法&quot;&gt;方法&lt;/h1&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;标准&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;LSTM+CRF&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;问题&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;标准成本昂贵&lt;/p&gt;

  &lt;p&gt;泛化迁移能力不足&lt;/p&gt;

  &lt;p&gt;可解释性不强&lt;/p&gt;

  &lt;p&gt;计算资源&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;JD和CV描述形式不一样&lt;/li&gt;
    &lt;li&gt;严谨性，简历内容要识别出能力词以及深层挖掘能力词（看起来并不是能力词，但是代表实际的某项能力），所以的深度挖掘词意&lt;/li&gt;
    &lt;li&gt;不依赖NER，根据词典或者特定语句形式（规则）提出实体词，最后进行标准化（💡现在做的实体标准化是不是应该也是这个作用？）&lt;/li&gt;
    &lt;li&gt;模型应该又轻又快，抛弃万能的bert系列&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;建议&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不要对Bert模型进行蒸馏压缩&lt;/li&gt;
  &lt;li&gt;NER是一个重底层任务（待补充）&lt;/li&gt;
  &lt;li&gt;NER任务尽量不要引入嵌套实体，例如：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python开发&lt;/code&gt;，模型识别出&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python开发&lt;/code&gt;标记为一个技能词，这时候就倾向于所有出现的&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python开发&lt;/code&gt;都标记为技能词。但是如果模型能够在识别&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python开发&lt;/code&gt;的同时能将&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python&lt;/code&gt;标记为一种语言或者编程语言，那么所有出现的&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[语言]开发&lt;/code&gt;都能够识别为技能词，这样模型就学到一种模式，并非单独一个词。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;**快速提升NER性能 **&lt;/p&gt;

&lt;p&gt;——————————————————————————————————————————————————&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;垂直领域的词典+规则，词典不断积累，迭代，快速应急处理badcase， 通⽤领域，可以多种分词工具和多种句法短语⼯具进行融合来提取候选实体，并结合词典进行NER&lt;/p&gt;

  &lt;p&gt;embedding层，引入丰富的char、bigram、词典、词性以及业务相关特征，垂直领域可以预训练一个领域相关字向量模型、词向量模型以及bigram模型和语言模型💡？，底层特征越丰富，差异化越大越好，不同视角的特征&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;&lt;em&gt;如何构建引入词汇信息（词向量）的NER？&lt;/em&gt;&lt;/strong&gt; 我们知道中文NER通常是基于字符进行标注的，这是由于基于词汇标注存在分词误差问题。但词汇边界对于实体边界是很有用的，我们该怎么把蕴藏词汇信息的词向量“恰当”地引入到模型中呢？一种行之有效的方法就是&lt;strong&gt;信息无损的、引入词汇信息的NER方法&lt;/strong&gt;，我称之为&lt;strong&gt;词汇增强&lt;/strong&gt;，可参考《中文NER的正确打开方式：词汇增强方法总结》。&lt;strong&gt;&lt;em&gt;ACL2020的Simple-Lexicon&lt;/em&gt;&lt;/strong&gt;和&lt;strong&gt;&lt;em&gt;FLAT&lt;/em&gt;&lt;/strong&gt;两篇论文，不仅词汇增强模型十分轻量、而且可以比肩BERT的效果&lt;/p&gt;

  &lt;p&gt;粗暴的方法将词向量引入到模型中，就是将词向量对齐到相应的字符，然后将字词向量进行混合，但这需要对原始文本进行分词（存在误差），&lt;strong&gt;性能提升通常是有限的&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;——————————————————————————————————————————————————&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;词汇增强方式&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;主要分为两个方向&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dynamic Architecture：设计一个动态框架，能够兼容词汇输入–&amp;gt; 结构融入词汇信息&lt;/li&gt;
  &lt;li&gt;Adaptive Embedding：基于词汇信息，构建自适应Embedding–&amp;gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/词汇增强.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[1] &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.02023&quot;&gt;Lattice LSTM：Chinese NER Using Lattice LSTM（ACL2018）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;中文词汇信息开山之作，通过词典匹配句子，得到一个Lattice的结构&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/Lattice.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lattice是一个有向无环图，词汇的开始和结束字符决定了其位置，Lattice LSTM融合了词汇信息到原生的LSTM中&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/Lattice_2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lattice LSTM引入了一个word cell结构，对当前字符融合以该字符为结束的所有word信息，例如&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;店&lt;/code&gt;融合了&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;人和药店&lt;/code&gt;，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;药店&lt;/code&gt;的信息。对于每个字符，Lattice LSTM采用注意力机制去融合个数可变的word cell单元，表达式为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/lstm公式.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;主要区别在于&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;当前字符有词汇信息可以融入，则利用公式15，采用公式15，不利用上一时刻记忆状态$\color{blue}{c_{j-1}^c}$,如果没有则用原生的LSTM，这种方式提升了NER性能，但是缺点也比较明显，其中&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/1596638537931.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;集合D类似对&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;桥&lt;/code&gt;来说，有&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;大桥&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;长江大桥&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;缺点&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;计算性能低下，不能batch并行，因为每个词增加的word cell数目不一样&lt;/p&gt;

  &lt;p&gt;信息损失，字符只能获取以他结尾的词汇信息，无之前状态，另一个就是BiLSTM时前后向词汇信息不能共享&lt;/p&gt;

  &lt;p&gt;可迁移性差&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;[2] &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/1698/d96c6fffee9ec969e07a58bab62cb4836614.pdf&quot;&gt;LR-CNN:CNN-Based Chinese NER with Lexicon Rethinking(IJCAI2019)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/LRCNN.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lattice LSTM没有办法并行化，并且无法处理&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;词汇信息冲突问题&lt;/code&gt;，上图中，长可以匹配市长，长隆，得到不同的实体标签，对于LSTM结构，仅依靠前一步的信息输入、而不是利用全局信息，没法解决冲突问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/LRCNN_2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;论文提出两种方法&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lexicon-Based CNNs：采用CNN对字符特征进行编码，感受野为2提取Bi-gram特征，多层堆叠得到Multi-gram信息，同时采用注意力机制融入词汇信息&lt;/li&gt;
  &lt;li&gt;Refining Networks with Lexicon Rethinking（ 用Lexicon重新思考完善网络 ）：由于上述提到的词汇信息冲突问题，LR-CNN采取rethinking机制增加feedback layer（反馈层）来调整词汇信息的权值： 具体地，将高层特征作为输入通过注意力模块调节每一层词汇特征分布。如上图，高层特征得到的 [广州市] 和 [长隆]会降低 [市长] 在输出特征中的权重分布。最终对每一个字符位置提取对应的调整词汇信息分布后的multi-gram特征，喂入CRF中解码。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;最终提速3.21倍，但是仍然计算复杂不具备迁移性&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[3] &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/D19-1396.pdf&quot;&gt;CGN: Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network（ EMNLP2019）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CGN：利用词汇知识通过协作图网络进行中文命名实体识别（EMNLP2019）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/CGN.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lattice LSTM存在信息损失，特别是无法获得‘inside’的词汇信息，论文提出了基于协作的图网络，由编码层、图网络层、融合层、解码层组成，图网络层由三种不同的构建方式&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Word-Character Containing graph（C-graph）：字与字之间无连接，词与其inside的字之间有连接&lt;/li&gt;
  &lt;li&gt;Word-Character Transition graph（T-graph）：相邻字符相连接，词与其前后字符连接&lt;/li&gt;
  &lt;li&gt;Word-Character Lattice graph（L-graph）：相邻字符相连接，词与其开始字符相连&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;图网络层通过Graph Attention Network（GAN）进行特征提取，提取三种图网络的钱n个字符节点的特征：&lt;/p&gt;

&lt;p&gt;$\color{blue}{Q_k=G_k[:,0:n],k\in{1,2,3}}$&lt;/p&gt;

&lt;p&gt;特征融合则基于字符的上下文表征H与图网络表征加权融合：&lt;/p&gt;

&lt;p&gt;$\color{blue}{R = W_1H+W_2Q_1+W_3Q_2+W_4Q_3}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[4] &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/D19-1096.pdf&quot;&gt;LGN: A Lexicon-Based Graph Neural Network for Chinese NER(EMNLP2019)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;LGN：面向中文NER的基于词典的图神经网络（EMNLP2019）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/LGN.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;论文与LR-CNN出发点类似，Lattice LSTM这种RNN结构仅仅依靠前一步的信息输入，而不是利用全局信息，如上图所示：字符 [流]可以匹配到词汇 [河流] 和 [流经]两个词汇信息，但Lattice LSTM却只能利用 [河流] ；字符 [度]只能看到前序信息，不能充分利用 [印度河] 信息，从而造成标注冲突问题 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/LGN_2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;论文通过采取 lexicon-based graph neural network (LGN)来解决上述问题。如上图所示，将每一个字符作为节点，匹配到的词汇信息构成边。通过图结构实现局部信息的聚合，并增加全局节点进行全局信息融入。聚合方式采取Multi-Head Attention&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[5] &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2004.11795.pdf&quot;&gt;FLAT: Chinese NER Using Flat-Lattice Transformer（ACL2020）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lattice-LSTM和LR-CNN采取的RNN和CNN结构无法捕捉长距离依赖，而动态的Lattice结构不能充分进行GPU并行&lt;/p&gt;

  &lt;p&gt;CGN和LGN采取的图网络虽然可以捕捉对于NER任务至关重要的顺序结构，但这两者之间的gap是不可忽略的。其次，这类图网络通常需要RNN作为底层编码器来捕捉顺序性，通常需要复杂的模型结构。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/FLAT.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;众所周知，Transformer采取全连接的自注意力机制可以很好捕捉长距离依赖，由于自注意力机制对位置是无偏的，因此Transformer引入位置向量来保持位置信息。受到位置向量表征的启发，这篇论文提出的FLAT设计了一种巧妙position encoding来融合Lattice 结构，具体地，如上图所示，对于每一个字符和词汇都构建两个head position encoding 和 tail position encoding，可以证明，这种方式可以重构原有的Lattice结构。也正是由于此，FLAT可以直接建模字符与所有匹配的词汇信息间的交互，例如，字符[药]可以匹配词汇[人和药店]和[药店]。&lt;/p&gt;

&lt;p&gt;因此，我们可以将Lattice结构展平，将其从一个有向无环图展平为一个平面的Flat-Lattice Transformer结构，由多个span构成：每个字符的head和tail是相同的，每个词汇的head和tail是skipped的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/FLAT_2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在知乎专栏文章《&lt;a href=&quot;https://zhuanlan.zhihu.com/p/137315695&quot;&gt;如何解决Transformer在NER任务中效果不佳的问题？&lt;/a&gt;》，我们介绍了对于Tranformer结构，绝对位置编码并不适用于NER任务。因此，FLAT这篇论文采取XLNet论文中提出相对位置编码计算attention score：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/FLAT-Attention-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;论文提出四种相对距离表示$\color{blue}{x_i}$和$\color{blue}{x_j}$之间的关系，同时也考虑字符和词汇之间的关系：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/FLAT-Attention-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\color{blue}{d_{ij}^{(hh)}}$表示$\color{blue}{x_i}$的head到$\color{blue}{x_j}$的tain距离，其余类似。相对位置encoding为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/FLAT-Attention-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;（圆圈加号）表示级联，四种距离编码通过简单的非线性变换得到。其中&lt;img src=&quot;https://www.zhihu.com/equation?tex=p_d&quot; alt=&quot;[公式]&quot; /&gt;的计算方式与vanilla Transformer相同，&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/1596643537145.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;综上，FLAT采取这种全连接自注意力结构，可以直接字符与其所匹配词汇间的交互，同时捕捉长距离依赖。如果将字符与词汇间的attention进行masked，性能下降明显，可见引入词汇信息对于中文NER 的重要性。此外，相关实验表明，&lt;strong&gt;FLAT有效的原因是&lt;/strong&gt;：新的相对位置encoding有利于定位实体span，而引入词汇的word embedding有利于实体type的分类。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解决NER实体span过长&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;——————————————————————————————————————————————————&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果NER任务中某一类实体span比较长，比如咱们提出的能力词，有很多很长&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;软件开发技术方案编写&lt;/code&gt;，直接采取CRF解码可能会导致很多连续的实体span断裂。除了加入规则进行修正外，这时候也可尝试引入&lt;strong&gt;指针网络+CRF&lt;/strong&gt;构建&lt;strong&gt;多任务学习&lt;/strong&gt;（指针网络会更容易捕捉较长的span，不过指针网络的收敛是较慢的，可以试着调节学习率）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;——————————————————————————————————————————————————&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;如何缓解NER标注数据的噪声问题&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;——————————————————————————————————————————————————&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;实际中，我们常常会遇到NER数据可能存在标注质量问题，也许是标注规范就不合理，正常的情况下只是存在一些小规模的噪声。一种简单地有效的方式就是对训练集进行交叉验证，然后人工去清洗这些“脏数据”。当然也可以将noisy label learning应用于NER任务，惩罚那些噪音大的样本loss权重&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;——————————————————————————————————————————————————&lt;/p&gt;

&lt;h2 id=&quot;嵌套命名实体识别&quot;&gt;嵌套命名实体识别&lt;/h2&gt;

&lt;p&gt;序列标准NER问题简单两步&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;选择有效的标注Schema
    &lt;ul&gt;
      &lt;li&gt;BIO&lt;/li&gt;
      &lt;li&gt;BIOS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;模型选择
    &lt;ul&gt;
      &lt;li&gt;CNN/Bi-LSTM/BERT等&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;解决方法&lt;/code&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python开发&lt;/code&gt; 中的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python&lt;/code&gt; 同时属于B-语言，也属于B-技能&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;将NER的分类任务从单标签编程多标签&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Schema不变，模型也不变，将输出从单标签转变为多标签&lt;/p&gt;

&lt;p&gt;1、Schema不变，训练集label从one-hot编码形式变成指定类型的均匀分布💡？，训练时将损失函数改为BCE或KL-divergence；推理时给定一个hard threshold，超过这个阈值的都被预测出来，当作这个token的类。&lt;em&gt;阈值难以设定，难以泛化&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;2、修改Schema，将共同出现的类别两两组合，构造新的标签，B-语言&lt;/td&gt;
      &lt;td&gt;技能，最后仍然是一个单分类。&lt;em&gt;标签增加，分布稀疏难以学习，预测结果不具有唯一性&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;总结&quot;&gt;总结&lt;/h1&gt;

&lt;h1 id=&quot;参考文献&quot;&gt;参考文献&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://www.its404.com/article/GJ_0418/120258136&quot;&gt;1、命名实体识别（NER）综述_SunnyGJing’s blog-程序员ITS404&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;amp;mid=2247505310&amp;amp;idx=2&amp;amp;sn=616b29f1d3996ae5b335e4eb1069073d&amp;amp;chksm=ebb7ed4adcc0645ce7c5970d274f7b2f2c9e85f748a42ca2c992f0357246cb65e04797b169ff&amp;amp;mpshare=1&amp;amp;scene=24&amp;amp;srcid=07227mK0wRN6V8m2kSKZvrUF&amp;amp;sharer_sharetime=1595381490241&amp;amp;sharer_shareid=5351471bb805db262b0658602c8a4f96#rd&quot;&gt;2、工业界求解NER问题的12条黄金法则&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/142615620&quot;&gt;3、中文NER的正确打开方式: 词汇增强方法总结 (从Lattice LSTM到FLAT)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/126347862&quot;&gt;4、浅谈嵌套命名实体识别（Nested NER）&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 12 Dec 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/12/NER%E8%B0%88%E5%8F%A4%E8%AE%BA%E4%BB%8A/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/12/NER%E8%B0%88%E5%8F%A4%E8%AE%BA%E4%BB%8A/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>Transformer探索</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;1什么是transformer&quot;&gt;1、什么是Transformer&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;《Attention is All You Need》是一篇谷歌提出的将attention思想发挥到极致的论文，这篇论文提出一个全新的模型，叫Transformer，抛弃了以往深度学习中的CNN和RNN。包括后来的Bert、GPT等都是基于此构建的，广泛的应用于NLP领域。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2transformer结构&quot;&gt;2、Transformer结构&lt;/h2&gt;

&lt;h3 id=&quot;21-总体结构&quot;&gt;2.1 总体结构&lt;/h3&gt;

&lt;p&gt;​	Transformer的结构和Attention模型一样，Transformer模型中也采用了encoder-decoder结构。论文中encoder层由6个encoder堆叠在一起，decoder层也一样。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点仅仅关注当前的词，从而能获得上下文语意&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;decoder，包含encoder提到的两层，但这两层（encoder-decoder）中间还有一层attention层，帮助当前节点获得需要关注的重点内容。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/transformer.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-encoder&quot;&gt;2.2 Encoder&lt;/h3&gt;

&lt;p&gt;​	如图所示，encoder由6个相同的transformer block构成，针对一个block来讲，我们可以看出，首先对其输入进行embedding编码输入，这一步呢类似于word2vec编码，然后通过self-attention送到前馈神经网络，然后到下一个block。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/transformer-encoder.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面对encoder每个模块进行说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;输入embedding&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;对输入文本进行向量编码，值得注意的是，编码维度为$\color{blue}{d_{model}}$，encoder的输入和decoder的输入我们进行了权值共享，并且在embedding层每个权重都乘了一个$\color{blue}{\sqrt{d_{model}}}$。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;位置embedding&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;Transformer结构由于天然无法感知空间，因此对于文本位置信息并不明确，而对于我们的翻译类任务，顺序信息尤为重要，因此引入位置信息的表达也就迫在眉睫，论文中引入了绝对位置编码，公式如下：&lt;/p&gt;

      &lt;p&gt;$\color{blue}{PE_{(pos,2i)}=sin(pos/10000^{\frac{2i}{d_{model}}})}$&lt;/p&gt;

      &lt;p&gt;$\color{blue}{PE_{(pos,2i)}=cos(pos/10000^{\frac{2i}{d_{model}}})}$&lt;/p&gt;

      &lt;p&gt;其中$\color{blue}{pos}$代表位置，$\color{blue}i$代表向量维度中的index，这样对每个维度都有一个位置编码，偶数位置用正弦编码，奇数位置用余弦编码，最后将位置embedding和输入embedding相加送入block模块中。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;layer normalization&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;这里主要说明一下Layer normalization 和Batch normalization的区别，他们都是数据归一化的一种方式，本质都是为了解决梯度消失或者梯度爆炸，主要是将模型中每层偏离的数据拉回归一化数据，这样有利于模型的学习（梯度更新）。区别如图所示：&lt;/p&gt;

      &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/Layer Normalization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

      &lt;p&gt;图中可以看出，假设输入为三个字符，每个字符的embedding为6维，那么Batch Normalization是在特征维度进行归一化，而Layer Normalization不同的是在seq也就是图中的batch方向归一化，既对每个字符进行归一化。&lt;/p&gt;

      &lt;p&gt;（小声bb：图中的batch当作sequence理解，纵向当作每个word的embedding理解，理解二维后再提升到三维理解（带上batch））&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;23-decoder&quot;&gt;2.3 Decoder&lt;/h3&gt;

&lt;p&gt;​	decoder和encoder结构大同小异，基本一致，也是对输入做了embedding和位置编码，可以参照encoder，不同之处在于decoder中用到了mask attention，这是较为关键的一点，下面我们对论文提到的三种attention进行说明。&lt;/p&gt;

&lt;h3 id=&quot;24-attention&quot;&gt;2.4 Attention&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;self attention&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\color{yellow}{self-attention}$再论文中用于encoder阶段，既每个block模块中对输入进行了$\color{yellow}{self-attention}$，也叫自注意力机制，自注意力机制输入的$\color{blue}{query、key、value}$是相同的，简单来讲，论文中的输入为（None，512），copy为三份，对每一份都用不同的权重矩阵（$\color{blue}{W^q、W^k、W^v\in R^{64\times 512}}$)，如图所示：&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/self attention 1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p&gt;对于$\color{yellow}{self-attention}$来讲，就是要对当前query的词计算对每个词的注意力权重，这个计算由query和key计算得来，计算方式如下，对$\color{blue}{q_1}$来讲，分别计算每个k的Score，论文中提到，对于$\color{blue}{q}$和$\color{blue}{k}$的点乘结果除以一个8（$\color{blue}{\sqrt{64}}$），最后做一个softmax，就得到当前q对每个q的注意力分值，对应分值再乘以value向量，然后相加，便得到当前节点的self attention的值。&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/self attention 2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p&gt;实际计算中，为了提升效率，q、k、v都为矩阵，计算公式如下：$\color{blue}{Attention(K,Q,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V}$。论文中也把这种方式叫做$\color{yellow}{Scaled Dot-Product Attention}$。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;attention&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;解码器和编码器之间也用到attention，我们暂叫做普通attention，就是相当于把$\color{yellow}{self-attention}$的query向量变为了decoder中的输入编码，encoder的输出向量作为key、value向量。计算方式同上。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;mask attention&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\color{yellow}{mask-attention}$用于解码器中，由于再解码的过程中，实际上我们并不知道下文会出现什么，而天然的attention会自动学习到后面的信息，这对于实际预测来讲并不公平，因此采用了$\color{yellow}{mask-attention}$，故名思意，就是对当前节点我们通过mask的方式不让attention看到后面的信息，transformer中具体用到了两种mask，一种是$\color{yellow}{padding-mask}$，这部分再$\color{yellow}{self-attention}$中用到，一个是$\color{yellow}{sequence-mask}$，这部分再解码器中用到，具体做法如下：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;padding mask&lt;/li&gt;
  &lt;/ul&gt;

  &lt;blockquote&gt;
    &lt;p&gt;$\color{yellow}{padding-mask}$是因为模型接受到的输入长度不同，为了便于计算我们通常要padding到相同的长度，这就会有一些短的会补0，而这些是没有意义的，因此再计算attention的时候我们需要把这部分无用的剔除。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！而我们的$\color{yellow}{padding-mask}$实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。&lt;/p&gt;
  &lt;/blockquote&gt;

  &lt;ul&gt;
    &lt;li&gt;sequence mask&lt;/li&gt;
  &lt;/ul&gt;

  &lt;blockquote&gt;
    &lt;p&gt;$\color{yellow}{sequence-mask}$是为了解码的时候不看到未来的信息，具体做法就是我们构造一个上三角全为0的的矩阵作用到序列上，这样我们再解码计算的时候就不会看到但前节点以后的信息了。&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;25-multi-head-attention&quot;&gt;2.5 Multi-head Attention&lt;/h3&gt;

&lt;p&gt;​	原论文中说到进行Multi-head Attention的原因是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，最后再将各个方面的信息综合起来。其实直观上也可以想到，如果自己设计这样的一个模型，必然也不会只做一次attention，多次attention综合的结果至少能够起到增强模型的作用，也可以类比CNN中同时使用&lt;strong&gt;多个卷积核&lt;/strong&gt;的作用，直观上讲，多头的注意力&lt;strong&gt;有助于网络捕捉到更丰富的特征/信息&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&quot;3transformer相比于rnnlstm有什么优势为什么&quot;&gt;3、Transformer相比于RNN/LSTM，有什么优势？为什么？&lt;/h2&gt;

&lt;p&gt;​	1、Transformer 相比RNN/LSTM并行计算的能力更好，序列信息由于要等信息的传递，所以并行度不高。&lt;/p&gt;

&lt;p&gt;​	2、Transformer 相比RNN/LSTM特征提取能力更强，但不代表可以完全取代RNN/LSTM，任何模型都有其特定的使用场景。&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/mantch/p/11591937.html&quot;&gt;1、Transformer各层网络结构详解！面试必备！(附代码实现)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://space.bilibili.com/1567748478/video&quot;&gt;2、视频&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://readpaper.com/pdf-annotate/note?noteId=627057414680178688&amp;amp;pdfId=4557871218548547585&quot;&gt;3、论文&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 04 Dec 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/12/Transformer/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/12/Transformer/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>生成式模型探索</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h1 id=&quot;简述&quot;&gt;简述&lt;/h1&gt;

&lt;p&gt;​	工作中需要对jd职位描述生成一些具有总结性的话语，可以理解为自动文本摘要，因此尝试了如下模型，记录以下碰到的问题以及改进，话不多说，直接上干货吧~&lt;/p&gt;

&lt;h1 id=&quot;先知&quot;&gt;先知&lt;/h1&gt;

&lt;p&gt;​	由于是工作探索阶段，因此没有很好的数据，具体方式如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;通过训练NER提取关键词（这一步怎么来的就不再介绍），关键词组成的语句作为label；&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;seq2seqattention&quot;&gt;Seq2seq+Attention&lt;/h1&gt;

&lt;h2 id=&quot;训练方式&quot;&gt;训练方式&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jieba切词，词为单元训练&lt;/li&gt;
  &lt;li&gt;训练使用 teach forcing，预测使用 Beam search&lt;/li&gt;
  &lt;li&gt;encoder 和 decoder都使用 lstm&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;测试结果&quot;&gt;测试结果&lt;/h2&gt;

&lt;h3 id=&quot;停不下来寻因探究&quot;&gt;停不下来寻因探究&lt;/h3&gt;

&lt;p&gt;​	ICML 2020的文章&lt;a href=&quot;https://arxiv.org/abs/2002.02492&quot;&gt;《Consistency of a Recurrent Language Model With Respect to Incomplete Decoding》&lt;/a&gt;比较系统地讨论了这个现象，并提出了一些对策。&lt;/p&gt;

&lt;p&gt;​	seq2seq解码是一个”自回归“生成模型，$\color{blue}{p(y_t)|y&amp;lt;t,x}$，那么解码过程就是给定$\color{blue}{x}$来输出对应的$\color{blue}{y=(y_1,y_2,…,y_T)}$，解码算法大致分为$\color{red}{确定性解码和随机性解码}$，下面简单介绍以下这两种算法。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;确定性解码&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;确定性解码就是输入固定的文本，解码出来的结果也是固定的，这类算法包括贪心搜索(Greedy Search)和束搜索(Beam Search)，其中贪心搜索就是束搜索的极端情况，Beam Search在前面blog中简单提过，其实就是每次选取top K概率最大的。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;随机解码&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;随机性解码，就是输入固定输出却不固定，主要包括下面三种情况&lt;/p&gt;

  &lt;p&gt;1、原始采样随机解码，原始采样法是一种针对有向图模型的基础采样方法，指从模型所表示的联合分布中产生样本，又称祖先采样法。该方法所得出的结果即视为原始采样。具体的每步按概率随机采样一个token，比如第一步算$\color{blue}{p(y_1|y_0,x)}$，然后按照概率随机采样一个token，比如c；计算第二不，$\color{blue}{p(y_1|y_0,c,x)}$，再按随机概率采样一个token，比如a，依次下去，知道采样到&lt;eos&gt;停止。&lt;/eos&gt;&lt;/p&gt;

  &lt;p&gt;2、&lt;em&gt;top-k&lt;/em&gt; 随机解码：，出自文章&lt;a href=&quot;https://arxiv.org/abs/1805.04833&quot;&gt;$\color{yellow}{《Hierarchical Neural Story Generation》}$&lt;/a&gt;，就是再原生采样随机解码的基础上加了个截断，每一步只保留概率最高的$\color{blue}k$个token，然后重新归一化在采样，这么做是希望再“得分高”和“多样性”做了一个这种。$\color{bule}k=1$时，就是贪心搜索。&lt;/p&gt;

  &lt;p&gt;3、Nucleus随机解码：出自文章&lt;a href=&quot;https://arxiv.org/abs/1904.09751&quot;&gt;$\color{yellow}{《The Curious Case of Neural Text Degeneration》}$&lt;/a&gt;,跟&lt;em&gt;top-k&lt;/em&gt; 随机解码类似，也是对采样空间做了截断，阶段方式是——固定$\color{blue}p\in(0,1)$，然后只保留概率最高的、概率和刚好超过$\color{blue}p$的若干token，所以也叫&lt;em&gt;top-p&lt;/em&gt; 采样。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;​	&lt;strong&gt;从seq2seq的模型设计和上面解码算法来看，理论上不能保证解码过程一定能停下来，只能靠模型学习，当模型学习不好的时候，就会出现根本停不下来的现象&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;停不下来怎么解决(这部分不知道咋回事公式识别不好，遂改成图片展示)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1、有界的隐向量，$\color{blue}{p(y_t|y&amp;lt;t,x)=softmax(Wh_t+b),h_t=f(y&amp;lt;t,x)}$是建模概率，计算个隐向量，然后全连接，softmax激活，如果对任意的$\color{blue}t,| |h_t| |$是有上界的，那么原始采样解码就能”停止“。说简单点就是只要你采样够多，就能停，实际应用价值就不大了。&lt;/p&gt;

&lt;p&gt;2、1的方法是针对原始采样方法的，对&lt;em&gt;top-k&lt;/em&gt; 随机解码和Nucleus随机解码就不一定成功了，因为&lt;eos&gt;不一定会出现再采样空间，要想使用只能手动把&lt;eos&gt;添加到采样空间。&lt;/eos&gt;&lt;/eos&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;这两个结论只能用于随机解码，确定性解码就不能用了，因为没法保证一定能碰到，因此可以使用下面方法&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;3、自截断：这是论文稍微有意义的地方，自截断说白了就是随着解码过程，我们希望能碰到&lt;eos&gt;，这样就能停下来了，并且希望随着解码长度的增加碰到&lt;eos&gt;的概率也越来越多大，最终趋于1，这样就可以停下来了。定义&lt;/eos&gt;&lt;/eos&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/自截断.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里$\color{blue}{\sigma(·)}$将$\color{blue}R$映射到$\color{blue}{[0,1-ϵ]}$，例如可以用$\color{blue}{\sigma(·)=(1-ϵ)sigmoid(·)}$。设计好$\color{blue}{p(&lt;eos&gt;|y&amp;lt;t,x)}$，剩下的token概率按照原来的softmax方式计算，然后乘以$\color{blue}{ \alpha (h_t)}$即可。现在有&lt;/eos&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/自截断-1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/自截断-2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;预测文本&lt;/th&gt;
      &lt;th&gt;hyp.text :&lt;START&gt; 3 定期 组织 绩效考核 指标 目标 完成 情况 进行 过程 管控 &lt;STOP&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/STOP&gt;&lt;/START&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;top 0 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; 绩效考核 指标 考核 指标 &lt;STOP&gt;&lt;/STOP&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;top 1 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; 绩效考核 指标 考核 指标 目标 &lt;STOP&gt;&lt;/STOP&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;top 2 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; 绩效考核 指标 考核 指标 目标 指标 &lt;STOP&gt;&lt;/STOP&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;问题：重复&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;重复问题寻因探究&quot;&gt;重复问题寻因探究&lt;/h3&gt;

&lt;p&gt;​	AAAI2021有一篇论文从理论上分析了seq2seq重复解码的现象，本质上，重复和不停是同理的，算是填补了上面论文的空白，论文链接&lt;a href=&quot;https://arxiv.org/abs/2012.14660&quot;&gt;A Theoretical Analysisi of the repetition Problem in Text Generation&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;​	先量化以下问题，解码过程中子序列$\color{blue}{s=[w_1,w_2,…,w_n]}$后面接子序列$\color{blue}{t=[w_1,w_2,…,w_n,w_1]}$，我们就称$\color{blue}{w_1,w_2,…,w_n}$是一个$\color{red}{”重复子序列”}$，而我们要做的就是分析解码过程中重复子序列的概率。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;二元解码&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;​	一般的自回归形式为: $\color{blue}{p(y|x)=\prod_{t=1}^lp(y_t|y_{y\lt t,x})}$，可以看出位置&lt;em&gt;t&lt;/em&gt; 的解码不仅依赖输入&lt;em&gt;x&lt;/em&gt; ，还依赖&lt;em&gt;t&lt;/em&gt; 之前已经获得的所有解码结果，我们考虑简单的情况，做个马尔可夫假设，每一步解码只依赖前一时刻的结果，与再之前的无关，即$\color{blue}{\prod_{t=1^lp(y_t|y_{t-1},x)}}$，这样对于固定的&lt;em&gt;x&lt;/em&gt; 而言，解码器实际上就是一个$\color{blue}{n\times n}$的转移矩阵$\color{blue}{P=(P_{i,j})}$，其中$\color{blue}{P_{i,j}}$表示从&lt;em&gt;i&lt;/em&gt; 后面接&lt;em&gt;j&lt;/em&gt; 的概率，&lt;em&gt;n&lt;/em&gt; 代表词表大小，还需要一个结束标记&lt;eos&gt;，遇到&lt;eos&gt;就停止解码，所以实际上转移矩阵是$\color{blue}{(n+1)\times(n+1)}$，但是我们考虑的重复解码都是终止之前，所以只需要考虑$\color{blue}{n\times n}$就可以了。&lt;/eos&gt;&lt;/eos&gt;&lt;/p&gt;

  &lt;p&gt;​	我们要计算的是重复子序列出现的概率，假设$\color{blue}{[i,j,k]}$，是一个三元重复子序列，那么它出现概率就是序列$\color{blue}{[i,j,k,i,j,k,i]}$出现的概率：&lt;/p&gt;

  &lt;p&gt;​									$\color{blue}{P_{i,j}P_{j,k}P_{k,i}P_{i,j}P_{j,k}P_{k,i}=P_{i,j}^2p_{j,k}^2P_{k,i}^2}$&lt;/p&gt;

  &lt;p&gt;因此所有的三元重复子序列的概率为：&lt;/p&gt;

  &lt;p&gt;​									$\color{blue}{\sum_{i,j,k}P_{i,j}^2P_{j,k}^2P_{k,i}^2=Tr(P\otimes P)^3}$&lt;/p&gt;

  &lt;p&gt;这里的$\color{blue}{\otimes}$表示逐位元素对应相乘，而$\color{blue}{Tr}$则是矩阵的迹，即对角线元素之和。最后我们将所有长度的重复子序列概率都加起来：&lt;/p&gt;

  &lt;p&gt;​									$\color{blue}{R=\sum_{k=1}^{\infty}}Tr(P\otimes P)^k=Tr(\sum_{k=1}^\infty(P\otimes P)^k)$&lt;/p&gt;

  &lt;blockquote&gt;
    &lt;p&gt;两个矩阵对应位置$\color{blue}{\otimes}$的目的就是为了使得每个位置上的值变为原来的平方，因此就实现了$\color{blue}{P_{i,j}P_{j,k}P_{k,i}P_{i,j}P_{j,k}P_{k,i}=P_{i,j}^2p_{j,k}^2P_{k,i}^2}$的目的。&lt;/p&gt;

    &lt;p&gt;$\color{blue}{(P\otimes P)^k}$中的&lt;em&gt;k&lt;/em&gt; 实际上表示&lt;em&gt;k&lt;/em&gt; 元重复子序列，那为什么这个&lt;em&gt;k&lt;/em&gt; 次方的矩阵求迹就是结果了？以$\color{blue}{k=2}$为例（其中蓝色矩阵中的每一项都是平方项，只不过我并没有显式的标出平方）&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/迹.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;对角线上每一个元素代表固定&lt;em&gt;i&lt;/em&gt; 而遍历&lt;em&gt;j&lt;/em&gt; 的结果，那么把所有的对角线都加起来（迹），实际上就是做了一个$\color{blue}{\sum_i}$的操作。&lt;/p&gt;
  &lt;/blockquote&gt;

  &lt;p&gt;​	这就是二元解码出现&lt;strong&gt;重复解码的概率&lt;/strong&gt;，目前只是一个理论公式，不过是我们重要的出发点，分别推到它的上下界。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;一个下界&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;直接看$\color{blue}{R}$的公式看不出啥，我们可以给他先推到一个直观的下界，以三元重复子序列为例可以得到如下：&lt;/p&gt;

  &lt;p&gt;​								$\color{blue}{\sum_{i,j,k}P_{i,j}^2P_{j,k}^2P_{k,j}^2=n^3\times\frac{\sum_{i,j,k}P_{i,j}^2P_{j,k}^2P_{k,i}^2}{n^3}\gt n^3\times(\frac{\sum_{i,j,k}P_{i,j}^2P_{j,k}^2P_{k,i}^2}{n^3})^2=\frac{(TrP^3)^2}{n^3}}$&lt;/p&gt;

  &lt;blockquote&gt;
    &lt;p&gt;上面的推到可以通过均值不等式得到，$\color{blue}{\sqrt{\frac{\sum x^2}{m}}\ge \frac{\sum x}{m}}$两边同时平方就医得到$\color{blue}{\frac{\sum x^2}{m}\ge (\frac{\sum x}{m})^2}$。&lt;/p&gt;
  &lt;/blockquote&gt;

  &lt;p&gt;$\color{blue}{\frac{(TrP^3)^2}{n^3}}$就是下界，但是我们可以做的更精细一点。假设&lt;em&gt;P&lt;/em&gt; 矩阵中有一些元素为0，那么$\color{blue}{P_{i,j}^2P_{j,k}^2P_{k,j}^2}$中非零元素的个数就不是$\color{blue}{n^3}$，我们假设非零元素的个数为$\color{blue}{N_3(P)\lt n^3}$，那么我们在利用均值不等式的时候，可以只对非零元素进行，替换后如下：&lt;/p&gt;

  &lt;p&gt;​									$\color{blue}{\sum_{i,j,k}{P_{i,j}^2P_{j,k}^2P_{k,i}^2}\ge\frac{(TrP^3)^2}{N_3(P)}}$&lt;/p&gt;

  &lt;p&gt;其中$\color{blue}{N_3(P)}$的直接计算也比较困难，没有一般的通项公式，但我们可以做一个简单的估算：设$\color{blue}P$的非零元素的比例为$\color{blue}{\zeta}$，也就是非零元素的个数为$\color{blue}{\zeta n^2}$，那么我们一个认为$\color{blue}{P_{i,j}^2P_{j,k}^2P_{k,i}^2}$的非零元素比例近似为$\color{blue}{\zeta^3}$，而总的排列数为$\color{blue}{n^3}$，所以我们可以可以认为$\color{blue}{N_3(P)\backsim\zeta^3n^3}$。注意可以举例说明这个既不能保证上界，也不能保证下界，所以替换后无法保证等号成立。不过如果我们愿意相信是一个足够好的近似，那么可以写下&lt;/p&gt;

  &lt;p&gt;​								$\color{blue}{\sum_{i,j,k}{P_{i,j}^2P_{j,k}^2P_{k,i}^2}\ge\frac{(TrP^3)^2}{\zeta^3n^3}}$&lt;/p&gt;

  &lt;p&gt;以及&lt;/p&gt;

  &lt;p&gt;​								$\color{blue}{R=\sum_{k=1}^{\infty}Tr(P\otimes P)^k\ge\sum_{k=1}^{\infty}\frac{(TrP^k)^2}{\zeta^kn^k}}$&lt;/p&gt;

  &lt;p&gt;或者我们不关心等号，而将最右面的结果视为&lt;em&gt;R&lt;/em&gt; 的一个估计。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;初步结论&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;解码概率经过softmax出来，结果都不等于0，那么$\color{blue}{\zeta}$应该恒等于1，因此引入$\color{blue}{\zeta}$似乎没有什么价值。&lt;/p&gt;

  &lt;p&gt;事实上，softmax确实不会产生0概率值，但是解码算法通常会强制为零，在三种随机性解码以及两种确定性解码中，除了不靠谱的随机性解码中的原始采样随机解码外，其他都会或多或少确定若干个最优结果来作为候选值，这就相当于&lt;strong&gt;直接截断了转移概率矩阵&lt;/strong&gt;，大大的降低了$\color{blue}{\zeta}$（非零概率）。&lt;/p&gt;

  &lt;p&gt;比如Greedy Search中，容易推出实际上对应这最小的非零概率$\color{blue}{\zeta=1/n}$，由于$\color{blue}{\zeta}$在公式的分母位置，因此$\color{blue}{\zeta}$的缩小就意味着重复了$\color{blue}{R}$的增加，这也就证明了Greedy Search的重复解码风险是相当高的，这也与实际中现象一致，也证明了在Beam Search中，我们Beam size取得越大，越不容易重复（仍会出现），但是会增加计算量。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;一个上界&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\color{yellow}{下界能帮我们解释一些现象，而上界则可以给我们提供改进思路。}$&lt;/p&gt;

  &lt;p&gt;为了推导上界，我们利用如下的两个结论&lt;/p&gt;

  &lt;blockquote&gt;
    &lt;p&gt;1、矩阵的迹等于它所有特征值的和&lt;/p&gt;

    &lt;p&gt;2、如果$\color{blue}{\lambda_1(A)\ge\lambda_2(A)\ge…\ge\lambda_n(A)}$是矩阵$\color{blue}{A}$的所有特征值，那么$\color{blue}{\lambda_1^k(A)\ge\lambda_2^k(A)\ge…\ge\lambda_n^k(A)}$是矩阵$\color{blue}{A^k}$的所有特征值。&lt;/p&gt;
  &lt;/blockquote&gt;

  &lt;p&gt;所以可以推导：&lt;/p&gt;

  &lt;p&gt;$\color{blue}{R=\sum_{k=1}^{\infty}Tr(P\otimes P)=\sum_{k=1}^{\infty}\sum_{i=1}^n\lambda_i((P\otimes P)^k)}$&lt;/p&gt;

  &lt;p&gt;$\color{blue}{=\sum_{k=1}^{\infty}\sum_{i=1}^{n}\lambda_i^k(P\otimes P)=\sum_{i=1}^{n}\sum_{k=1}^{\infty}\lambda_i^k(P\otimes P)}$&lt;/p&gt;

  &lt;p&gt;$\color{blue}{=\sum_{i=1}^n\frac{\lambda_i(P\otimes P)}{1-\lambda_i(P\otimes P)}}$&lt;/p&gt;

  &lt;p&gt;​	上述过程用到了级数$\color{blue}{\frac{x}{1-x}=\sum_{k=1}^{\infty}x^k}$，该级数只有在$\color{blue}{|x|\lt1}$才收敛，而很巧的是，我们可以证明$\color{blue}{P\otimes P}$的特征值绝对值必然不大于1，且通常都小于1：由于$\color{blue}{P}$是转移矩阵，因此它的每一行之和都为1，因此$\color{blue}{(P\otimes P)x=\lambda x}$，不失一般性，设$\color{blue}{x}$绝对值最大的元素为$\color{blue}{x_1,P\otimes P}$的第一个行向量为$\color{blue}{q_1^\top}$，那么我们有$\color{blue}|\lambda| |x_1|\le|q_1^{\top}x|\le|x_1|$，从而$\color{blue}{|\lambda|\le 1}$，并且等号成立的条件还是比较苛刻的，所以通常来说$\color{blue}{|\lambda|\lt 1}$。&lt;/p&gt;

  &lt;p&gt;​	注意函数$\color{blue}{\frac{x}{1-x}}$在区间[-1,1]区间是单调递增的，所以公式中主导的是第一项，如果非要给整体弄上一个上界，那么可以是$\color{blue}{\frac{n\lambda_i(P\otimes P)}{1-\lambda_i(P\otimes P)}}$。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;再次结论&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;由此可见，如果想要降低重复率$\color{blue}{R}$，那么我们需要想办法降低矩阵$\color{blue}{P\otimes P}$的最大特征值。$\color{blue}{P\otimes P}$是一个非负矩阵，根据非负矩阵的$\color{yellow}Frobenius介值定理(非负矩阵的最大特征值在它每一行的和的最小值与最大值之间)$，我们有：&lt;/p&gt;

  &lt;p&gt;$\color{blue}{min_i\sum_{j}P_{i,j}^2\le\lambda_1(P\otimes P)\le max_i\sum_jP_{i,j}^2}$&lt;/p&gt;

  &lt;p&gt;现在我们知道，为了降低$\color{blue}{P\otimes P}$的最大特征值，我们需要想办法降低它的每一行之和，即$\color{blue}{\sum_jP_{i,j}^2}$，并且由于均值不等式&lt;/p&gt;

  &lt;p&gt;$\color{blue}{\sum_jP_{i,j}^2\ge n(\frac{\sum_jP_{i,j}}{n})^2=1/n}$&lt;/p&gt;

  &lt;p&gt;知它的最小值为$\color{blue}{1/n}$，在$\color{blue}{P_{i,1}=P_{i,2}=…=P_{i,n}}$时取到，因此最终我们得出结论：$\color{yellow}{要降低最大特征值，就要使得矩阵P每一行尽可能均匀，换言之，要降低P每一行的方差}$。&lt;/p&gt;

  &lt;p&gt;​	怎么降低方差呢，简单点来说就不能出现过高的概率值即可，比如某一行接近one hot的形式，那么平方之后仍然接近one hot的形式，那么求和就接近1，远远大于理论最小值$\color{blue}{1/n}$。实际中什么情况会出现过高的概率值呢，就是在某些字后面可以接的字很少，甚至只有一个候选项的时候，比如“忐”后面接“忑”，那么$\color{blue}{P_{i=忐}P_{j=忑}}$就相当高。那么怎么来避免出现这种过高的概率值呢，就是将高概率值得字合并起来，当作一个新词来看，那么“忐”哪一行就不存在，也就是无所谓得方差大了。&lt;/p&gt;

  &lt;p&gt;​	说白了，在文本生成任务中，以词为单位比以字为单位更加靠谱，更加不容易出现重复解码。适当地合并一些相关程度比较高的词作为新词加入到词表中，降低转移矩阵的方差，有助于降低重复解码的风险，原论文还给这个操作起了个很高端的名字，叫做Rebalanced Encoding Algorithm，事实上就是这个意思。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;一般解码&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;​	那这个证明过程容易推广到一般的自回归模型中吗？很遗憾，并不容易。对于一般的自回归模型来说，它相当于每一步的$\color{blue}{P}$都是不一样的，因此只要模型的性能足够好，其实基本上不会出现重复解码，事实上经过充分预训练的生成式模型，确实很少出现重复解码了。但是，我们又能观察到，哪怕是一般的自回归解码，偶尔也能观察到重复解码现象，尤其是没有经过预训练的模型，这又该怎么解释呢？&lt;/p&gt;

      &lt;p&gt;​	前面的小节是基于二元解码模型的，结论是二元解码模型确实容易出现重复解码，那么我们或许可以反过来想，一般的自回归模型出现重复解码现象，是因为它此时退化为了二元解码模型？对于难度比较高的输入，模型可能无法精细捕捉好每一步的转移概率，从而只能将转移矩阵退化为二元解码，这是有可能的。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;预测文本&lt;/th&gt;
      &lt;th&gt;hyp.text :&lt;START&gt; 负责 人员 招募 工作 确保 公司 业务 可持续性 发展 &lt;STOP&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/STOP&gt;&lt;/START&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;top 0 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; &lt;UNK&gt; 业务 发展 策略 &lt;STOP&gt;&lt;/STOP&gt;&lt;/UNK&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;top 1 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; &lt;UNK&gt; 业务 发展 策略 公司 &lt;STOP&gt;&lt;/STOP&gt;&lt;/UNK&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;top 2 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; &lt;UNK&gt; 业务 发展 策略 公司 发展 &lt;STOP&gt;&lt;/STOP&gt;&lt;/UNK&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;问题：生成原文没有的词（统称为产生新词），这并不是想要的&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;​	引入先验知识，我们生成得词都是句子中出现得（出现并不一定连续）。这样以来，我们可以利用句子（文章）中得词集作为一个先验分布，加到解码过程得分类模型中，使得模型在解码输出时更倾向选用文章中已有得字词。&lt;/p&gt;

  &lt;p&gt;​	具体来说，就是在每一步预测时，我们得到总向量$\color{blue}{x}$（它应该是decoder当前的隐层向量、encoder的编码向量、当前decoder与encoder的Attention编码三者的拼接），然后得到全连接层，最终得到一个大小为$\color{blue}{|V|}$得向量$\color{blue}{y=(y_1,y_2,…,y_{|V|})}$，其中$\color{blue}{|V|}$就是词表得词数。经过softmax后得到原本得概率$\color{blue}{p_i=\frac{e^{y_i}}{\sum_ie^{y_i}}}$，这就是原始得分类方案。引入先验分布得方案是，对于每个句子（文章）我们得到一个大小为$\color{blue}{|V|}$得0/1向量$\color{blue}{x=(x_1,x_2,…,x_{|V|})}$，其中$\color{blue}{x_i=1}$意味该词在文章中出现过，负则为0。将这样一个0/1向量经过缩放平移层得到：&lt;/p&gt;

  &lt;p&gt;$\color{blue}{\hat y=s\otimes x+t=(s_1x_1+t_1,s_2x_2+t_2,…,s_{|V|}x_{|V|}+t+{|V|})}$&lt;/p&gt;

  &lt;p&gt;其中$\color{blue}{s,t}$为训练参数，然后将这个向量与原来得$\color{blue}{y}$取平均后才做softmax：&lt;/p&gt;

  &lt;p&gt;$\color{blue}{y = \frac{y+\hat y}{2},p_i=\frac{e^{y_i}}{\sum_ie^{y_i}} }$&lt;/p&gt;

  &lt;p&gt;经实验，这样先验分布得引入，有助于加快收敛，生成更好得短句（标题）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;参考&quot;&gt;参考&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://kexue.fm/archives/7500&quot;&gt;1、如何应对seq2seq中“根本停不下来”问题？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.02492&quot;&gt;2、Consistency of a Recurrent Language Model With Respect to Incomplete Decoding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.04833&quot;&gt;3、Hierarchical Neural Story Generation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.09751&quot;&gt;4、《The Curious Case of Neural Text Degeneration》&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kexue.fm/archives/8128&quot;&gt;5、seq2seq重复解码的理论分析&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV13b4y1f7jx/&quot;&gt;6、视频讲解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://wmathor.com/index.php/archives/1550/&quot;&gt;7、Seq2Seq 重复解码问题追根溯源&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://spaces.ac.cn/archives/5861&quot;&gt;8、玩转Kearas之seq2seq自动生成标题&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 20 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E6%8E%A2%E7%B4%A2/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E6%8E%A2%E7%B4%A2/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>Glove</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;早上看到了Glove，细思之下发现了解不多，于是抽空网上搬砖。&lt;/p&gt;

&lt;h2 id=&quot;glove是何方妖孽&quot;&gt;Glove是何方妖孽&lt;/h2&gt;

&lt;p&gt;​	Glove 全称 Global Vectors for Word Representation，基于&lt;strong&gt;全局词频统计&lt;/strong&gt;的词表征，是一种词嵌入方式，将高维稀疏向量表示为低维稠密向量，并且保留了词与词之间的相似性（即cos距离更近）。&lt;/p&gt;

&lt;h2 id=&quot;glove来龙去脉&quot;&gt;Glove来龙去脉&lt;/h2&gt;

&lt;p&gt;​	三步曲实现Glove&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一步预备&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​	根据corpus构建一个共现矩阵（Co-ocurrence Matrix）$\color{blue}{X}$，什么是共现矩阵呢？先补充一个基本概念；&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;共现：给定一个语料库，一对单词的再上下文窗口同时出现的次数&lt;/p&gt;

  &lt;p&gt;上下文窗口：给定一个单词的上下文几个单词以内范围的大小，例如指定2，$\color{blue}{w_1,w_2,w_3,w_4,w_5,…}$，$\color{blue}{w_3}$的上下文就包含了$\color{blue}{w_1到w_5}$&lt;/p&gt;

  &lt;p&gt;假设我们有一个corpus如下：&lt;/p&gt;

  &lt;div class=&quot;language-tex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;我爱中国；
我爱北京；
我爱天安门；
我爱我家；
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;p&gt;假设content window size = 2，就可以得到如下的矩阵&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt; &lt;/th&gt;
        &lt;th&gt;我&lt;/th&gt;
        &lt;th&gt;爱&lt;/th&gt;
        &lt;th&gt;中&lt;/th&gt;
        &lt;th&gt;国&lt;/th&gt;
        &lt;th&gt;北&lt;/th&gt;
        &lt;th&gt;京&lt;/th&gt;
        &lt;th&gt;天&lt;/th&gt;
        &lt;th&gt;安&lt;/th&gt;
        &lt;th&gt;门&lt;/th&gt;
        &lt;th&gt;家&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;我&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;爱&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;中&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;国&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;北&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;京&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;天&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;安&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;门&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;家&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;共现矩阵是一个对阵矩阵，因此下三角省了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;​	矩阵中每一个元素$\color{blue}{X_{ij}}$代表单词$i$和单词$j$再content window size内共现的次数，一般而言，content window size最小位1，而Glove中根据两个单词再窗口的距离$d$提出了一个衰减函数$\color{blue}{decay = 1/d}$来计算权重，距离越远的单词再总计数的权重越小，这也符合常理，越“远”越“远”。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;二步走&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​	根据共现关系构建词向量，两者关系如下:&lt;/p&gt;

&lt;p&gt;​                                  $\color{blue}{ {w_i^T} \hat{w_j} + b_i+\hat{b_j}=log(X_{ij})—–(1)}$&lt;/p&gt;

&lt;p&gt;其中：$\color{blue}{w_i和\hat{w_j}}$是最终的词向量，$\color{blue}{b_i和b_j}$分别是两个词向量的偏置。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;三步停&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​	根据公式(1)构造损失函数如下：&lt;/p&gt;

&lt;p&gt;​            $\color{blue}{J=\sum_{i,j}^Vf(X_{ij})(W_i^T\hat{w_j}+b_i+\hat{b_j}-log(X_{i,j}))^2—–(2)}$&lt;/p&gt;

&lt;p&gt;可以看出是一个均方误差函数（mean square loss），额外加了个权重函数$\color{blue}{f}$，这个权重函数如下：&lt;/p&gt;

&lt;p&gt;​            &lt;img src=&quot;/images/posts/自然语言处理/glove 公式.PNG&quot; alt=&quot;权重函数&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$f(x)$的原因如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;经常一起出现的单词对权重要大于很少出现的单词对，所以是个非递减函数&lt;/li&gt;
    &lt;li&gt;随着单词对一起出现的次数越多，希望权重不要一直增大&lt;/li&gt;
    &lt;li&gt;未出现的单词对，不要参与loss的计算，即为0&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/glove loss权重函数.jpg&quot; alt=&quot;glove loss权重函数&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	论文中$\color{blue}\alpha$取值为0.75，$\color{blue}{x_{max}}$取值100。&lt;/p&gt;

&lt;h2 id=&quot;glove的运动方式&quot;&gt;Glove的运动方式&lt;/h2&gt;

&lt;p&gt;​	Glove是一种无监督的学习方式，但是其实是有label的，这一点和word2vec一样；而Glove的label就是公式(2)中的$\color{blue}{log(X_{ij})}$，两个$w$就是需要学习更新的参数，所以本质上和监督学习的方式没什么不同，都是基于梯度下降。&lt;/p&gt;

&lt;p&gt;​	论文中采用AdaGrad梯度下降法，对矩阵$X$中所有非0元素进行随机采样，学习率0.05，再vector size 300的情况下迭代50次，再其他vector size上迭代100次，直至收敛；最后得到两个vector， 分别是$\color{blue}{w}$,$\color{blue}{\hat{w}}$，因为$X$是对称的，因此理论上,$\color{blue}{w}$,$\color{blue}{\hat{w}}$也是对称的，他们唯一的区别是初始化时参数不同，导致最后的结果有差异，为了&lt;strong&gt;更好的鲁棒性，我们最终将两者之和作为最终的vector&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/glove实验结果.jpg&quot; alt=&quot;glove实验结果&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	这个图一共采用了三个指标：语义准确度，语法准确度以及总体准确度。那么我们不难发现Vector Dimension在300时能达到最佳，而context Windows size大致在6到10之间。&lt;/p&gt;

&lt;h2 id=&quot;glove-公式1探因&quot;&gt;Glove 公式(1)探因&lt;/h2&gt;

&lt;p&gt;​	公式(1)不得不写下来呀，抄也得抄下来，遵循书读百遍，其意自现，下面就开始愉快的抄写吧！&lt;/p&gt;

&lt;p&gt;定义一些变量：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\color{blue}{X_{ij}}$：单词$j$出现再单词$i$的上下文的次数&lt;/p&gt;

  &lt;p&gt;$\color{blue}{X_i}$：单词$i$的上下文中出现所有单词出现的总次数，即$\color{blue}{X_i=\sum^kX_{ik}}$&lt;/p&gt;

  &lt;p&gt;$\color{blue}{P_{ij}}$：$\color{blue}{P(j|i)=\frac{X_{ij}}{X_i}}$，表示单词$j$出现在单词$i$的上下文中的概率&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Probability and Ratio&lt;/th&gt;
      &lt;th&gt;&lt;em&gt;k=solid&lt;/em&gt;&lt;/th&gt;
      &lt;th&gt;&lt;em&gt;k=gas&lt;/em&gt;&lt;/th&gt;
      &lt;th&gt;&lt;em&gt;k=water&lt;/em&gt;&lt;/th&gt;
      &lt;th&gt;&lt;em&gt;k=fashion&lt;/em&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$P(k|ice)$&lt;/td&gt;
      &lt;td&gt;$\color{blue}{1.9*10^{-4}}$&lt;/td&gt;
      &lt;td&gt;$6.6*10^{-5}$&lt;/td&gt;
      &lt;td&gt;$3.0*10^{-3}$&lt;/td&gt;
      &lt;td&gt;$1.7*10^{-5}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$P(k|steam)$&lt;/td&gt;
      &lt;td&gt;$\color{blue}{2.2*10^{-5}}$&lt;/td&gt;
      &lt;td&gt;$7.8*10^{-4}$&lt;/td&gt;
      &lt;td&gt;$2.2*10^{-3}$&lt;/td&gt;
      &lt;td&gt;$1.8*10^{-5}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$P(k|ice)/P(k|stream)$&lt;/td&gt;
      &lt;td&gt;$\color{blue}{8.9}$&lt;/td&gt;
      &lt;td&gt;$8.5*10^{-2}$&lt;/td&gt;
      &lt;td&gt;$1.36$&lt;/td&gt;
      &lt;td&gt;$0.96$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;​	通过上面的表格可以分析得出，当&lt;em&gt;k=solid&lt;/em&gt;时，$\color{blue}{P(solid|ice)/P(solid|stream)}$比1大很多，这说明&lt;em&gt;solid&lt;/em&gt;和&lt;em&gt;ice&lt;/em&gt;相比&lt;em&gt;solid&lt;/em&gt;和&lt;em&gt;steam&lt;/em&gt;更相关，其他同理。由此得出结论，通过&lt;strong&gt;概率比率相比概率本身学习词向量可能是一个更恰当的方法&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;​	为了达到上述目标，构造函数&lt;/p&gt;

&lt;p&gt;​							$\color{blue}{F(w_i,w_j,\hat{w_k})=\frac{P_{ik}}{P_{jk}}—–(4)}$&lt;/p&gt;

&lt;p&gt;利用向量空间的线性结构做差，可以得到&lt;/p&gt;

&lt;p&gt;​						$\color{blue}{F(w_i-w_j,\hat{w_k})=\frac{P_{ik}}{P_{jk}}—–(5)}$&lt;/p&gt;

&lt;p&gt;我们可以发现公式(5)右边是一个常数，左边为了表达两个概率的比例差，左边变成两个向量的内积，如下&lt;/p&gt;

&lt;p&gt;​						$\color{blue}{F((w_i-w_j)^T,\hat{w_k})=\frac{P_{ik}}{P_{jk}}—–(6)}$&lt;/p&gt;

&lt;p&gt;接下来就到高能时刻了，先补充一波电，&lt;a href=&quot;http://www.ubinec.org/index.php?c=download&amp;amp;id=2209&quot;&gt;同态特性&lt;/a&gt;，首先$X$是对称矩阵，单词和上下文单词是相对的，因此我们可以做如下交换$\color{blue}{w&amp;lt;-&amp;gt;\hat{w_k},X&amp;lt;-&amp;gt;X^T}$，公式(6)应该保持不变，为了满足这个条件，要求函数$F$满足同态特性：&lt;/p&gt;

&lt;p&gt;​					$\color{blue}{F((w_i-w_j)^T,\hat{w_k})=\frac{F(w_i^T\hat{w_k})}{F(w_j^T\hat{w_k})}—–(7)}$&lt;/p&gt;

&lt;p&gt;结合公式(6)，得到&lt;/p&gt;

&lt;p&gt;​						$\color{blue}{F(w_i^T)=P_{ik}=\frac {X_{ik}} {X_i} —–(8)}$&lt;/p&gt;

&lt;p&gt;令F=exp,可以得到下面公式(9)&lt;/p&gt;

&lt;p&gt;​						$\color{blue}{w_i^T\hat{w_k}=log(P_{ik})=log(X_{ik})-log(X_i)—–(9)}$&lt;/p&gt;

&lt;p&gt;===&amp;gt;&lt;/p&gt;

&lt;p&gt;​						$\color{blue}{log(X_{ik})=w_i^T\hat{w_k}+log(X_i)—–(10)}$&lt;/p&gt;

&lt;p&gt;这里就和(1)很像了，我们把$log(X_i)$纳入到$w_i$的偏置中，为了对称性，也加到$\hat{w_k}$中，得到如下公式&lt;/p&gt;

&lt;p&gt;​					$\color{blue}{log(X_{ik})=w_i^T\hat{w_k}+b_i+\hat{b_k}—–(11)}$&lt;/p&gt;

&lt;h2 id=&quot;参考链接&quot;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.fanyeong.com/2018/02/19/glove-in-detail/&quot;&gt;1、GloVe详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/mr_tyting/article/details/80180780&quot;&gt;2、论文分享–&amp;gt;GloVe: Global Vectors for Word Representation&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 16 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/Glove/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/Glove/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>A Neural Probabilistic Language Model</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;在摘要中作者提到，通过学习一个分布式的词表示来克服维数的诅咒，它允许每个训练句子向模型告知一个指数数量的语义相邻句子。该模型同时学习 (1) 每个单词的分布式表示，以及 (2) 用这些表示 表示的单词序列的概率函数。&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;在介绍中，作者举例到，如果相对10000个词建立连续10个词得联合分布，那么可能需要得参数是$10000^{10}-1$，当对连续变量进行建模时，我们比较容易获得泛化（光滑得函数类，神经网路模型，高斯模型等），对于离散空间泛化结构就不明显了，每个离散变量得取值很大时，大多观测对象在汉明距离上几乎是最大得。&lt;/p&gt;

&lt;p&gt;本文提出在高维情况下，重要的是将概率质量均匀分布在每个训练点周围的各个方向上。这里提出得泛化方式也与之前最先进得统计方法模型不同。&lt;/p&gt;

&lt;p&gt;语言得统计模型公式如下$P({w_1}^T)=\prod_{t=1}^T(P(w_t|w_1^{t-1}))$我们可以看出，每次计算t得概率时，都需要从1~t-1的概率，这无形中就增加了很大的计算量，本文利用统计上词序更加依赖暂时距离较近的词这一事实，因此提出了n-gram模型结构，计算公式如下$P(w_t|w_1^{t-1})=P(w_t|w_{t-n+1}^{t-1})$。&lt;/p&gt;

&lt;p&gt;在文本中总会出现上下文连续的词但预料中没有出现的情况，一种简单的解决办法就是将n回退到三元模型。从本质上说一个新的单词序列是通过“粘合”非常短的长度为1,2…或者在训练数据中经常出现的单词上。获取下一个片段概率的规则隐含在后退或插值n-gram算法的细节中。研究中一般采用3，但显然，单词前面的序列中又更多的信息需要预测，并不仅仅时单词前的两个单词。因此该方法至少需要两点需要改进&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;它不考虑超过1到2个单词的上下文&lt;/li&gt;
  &lt;li&gt;不考虑单词之间的“相似性”&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fighting-the-curse-of-dimensionality-with-distributed-representations&quot;&gt;Fighting the Curse of Dimensionality with Distributed Representations&lt;/h2&gt;

&lt;p&gt;论文提出方法思想总结如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;将vocabulary的每个单词关联到一个分布式单词特征上（$R^m$）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将词序列中的词的特征向量表示为词序列的联合概率函数&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;同时学习单词特征向量和联合概率函数的参数&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;特征向量的不同维度表示单词的不同方位，每个单词的都与向量空间中的一个点相关联，特征的维度m远小于词汇表大小，概率函数表述为给定前一个词的情况下下个词的条件概率的乘积&lt;/p&gt;

&lt;h2 id=&quot;a-neural-model&quot;&gt;A Neural Model&lt;/h2&gt;

&lt;p&gt;论文提出的模型结构如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-MiUNPZwHQJTWEwsPhe9%2Fuploads%2FhxjpdeHfXxC46KhUdDNy%2Fimage.png?alt=media&amp;amp;token=140d8d29-01a9-435b-bfe0-5ca432a8a846&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Neural architecture&lt;/p&gt;

&lt;p&gt;上图输入是$W_{t-n+1}、…、w_{t-2}、w_t$，本质上就是通过前面n-1个词预测第t个词，这也就是n-gram模型&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$C(w_{t-n+1})$：t-n+1单词对应的词向量，也是模型最终的收获；&lt;/li&gt;
  &lt;li&gt;C：矩阵，行是vocab，列是word对应的词向量$|V*m|$；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;模型的输入即把单词concat通过隐藏层，最后通过tanh激活，模型最后输出层公是如下：$y=b+Wx+Utanh(d+Hx)$，最后加的这一项是中间隐藏层的表示，其中x的表示如下：$x=(C(w_{t-1}),C(w_{t-2}),…,C(w_{t-n+1}))$。&lt;/p&gt;

&lt;p&gt;【论文第三部分提出两种训练加速方式，一种是内存共享，另一种是参数共享】&lt;/p&gt;

</description>
        <pubDate>Sun, 14 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/A-Neural-Probabilistic-Language-Model/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/A-Neural-Probabilistic-Language-Model/</guid>
        
        <category>论文</category>
        
        
      </item>
    
      <item>
        <title>文本分类</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;三步骤&quot;&gt;三步骤&lt;/h2&gt;

    &lt;h3 id=&quot;特征工程&quot;&gt;特征工程&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;常用的特征主要是词袋特征，复杂的特征词性标签、名词短语和树核&lt;/li&gt;
    &lt;/ol&gt;

    &lt;h3 id=&quot;特征选择&quot;&gt;特征选择&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;特征选择旨在删除噪音特征，常用的就是移除法&lt;/li&gt;
      &lt;li&gt;信息增益、L1正则&lt;/li&gt;
    &lt;/ol&gt;

    &lt;h3 id=&quot;分类器&quot;&gt;分类器&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;机器学习方面主要有逻辑回归、朴素贝叶斯、支持向量机 —– 数据稀疏性问题&lt;/li&gt;
      &lt;li&gt;深度学习和表征学习为解决数据稀疏问题&lt;/li&gt;
    &lt;/ol&gt;

    &lt;h2 id=&quot;rcnn&quot;&gt;RCNN&lt;/h2&gt;

    &lt;ol&gt;
      &lt;li&gt;在我们的模型中，当学习单词表示时，我们应用递归结构来尽可能多地捕获上下文信息，与传统的基于窗口的神经网络相比，这可以引入相当少的噪声。&lt;/li&gt;
      &lt;li&gt;还使用了一个最大池层，它自动判断哪些单词在文本分类中起关键作用，以捕获文本中的关键成分。&lt;/li&gt;
      &lt;li&gt;双向递归神经网络来捕捉上下文。&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/v2-198c6d37d18d1708f22fedd3043a7340_r.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$c_l(w_i)$用公式一计算，其中$c_l(w_{i-1})$是当前 word 的左侧的 context，$e(w_{i-1})$是单词的嵌入，稠密向量&lt;/li&gt;
    &lt;/ul&gt;

    &lt;ol&gt;
      &lt;li&gt;TextCNN比较类似，都是把文本表示为一个嵌入矩阵，再进行卷积操作。不同的是TextCNN中的文本嵌入矩阵每一行只是文本中一个词的向量表示，而在RCNN中，文本嵌入矩阵的每一行是当前词的词向量以及上下文嵌入表示的拼接&lt;/li&gt;
    &lt;/ol&gt;

    &lt;h2 id=&quot;han&quot;&gt;HAN&lt;/h2&gt;

    &lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/54165155&quot;&gt;论文解读&lt;/a&gt;&lt;/p&gt;

    &lt;h2 id=&quot;gcn&quot;&gt;GCN&lt;/h2&gt;

    &lt;p&gt;&lt;a href=&quot;https://lsvih.com/2019/06/27/Graph Convolutional Networks for Text Classification/&quot;&gt;论文解读&lt;/a&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;提出了一种新的文本分类图形神经网络方法。据我们所知，这是第一次将整个语料库建模为异构图，并利用图神经网络联合学习单词和文档嵌入的研究。&lt;/li&gt;
      &lt;li&gt;在几个基准数据集上的结果表明，我们的方法优于现有的文本分类方法，不使用预先训练的单词嵌入或外部知识。我们的方法还自动学习预测单词和文档嵌入。&lt;/li&gt;
    &lt;/ol&gt;

    &lt;div class=&quot;language-tex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;输入特征矩阵X=I：one-hot
图边计算方式
    word-document：tf-idf
    word-word：PMI-为了利用全局词共现信息，我们对语料库中的所有文档使用固定大小的滑动窗口来收集共现统计。
A&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;ij计算方式如下：
    PMI(i, j)； i, j are words, PMI(i, j) &amp;gt; 0
    TF-IDF&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;ij；  i is document, j is word
    1； i = j
    0； otherwise
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;不直接用$D^{-1}A$而选用$D^{-1/2}AD^{-1/2}$，是因为$D^{-1}A$的结果不是对称矩阵，这个大家动手算一下就知道了。虽然两者结果不相同，但是$D^{-1/2}AD^{-1/2}$已经做到了近似的归一化，而且保持了矩阵的对称性，我想这就是选用对称归一化的拉普拉斯矩阵的原因&lt;a href=&quot;https://blog.csdn.net/qq_35516657/article/details/108225441&quot;&gt;链接&lt;/a&gt;，属于拉普拉斯对称归一化&lt;a href=&quot;https://zhuanlan.zhihu.com/p/362416124&quot;&gt;链接&lt;/a&gt;。&lt;/p&gt;

    &lt;h2 id=&quot;损失函数&quot;&gt;损失函数&lt;/h2&gt;

    &lt;h3 id=&quot;交叉熵损失&quot;&gt;交叉熵损失&lt;/h3&gt;

    &lt;p&gt;KL距离常用来度量两个分布之间的距离，其具有如下形式其中p是真实分布，q是拟合分布，H(p)是p的熵，为常数。因此 度量了p和q之间的距离，叫做交叉熵损失。&lt;/p&gt;

    &lt;h2 id=&quot;优化方法&quot;&gt;优化方法&lt;/h2&gt;

    &lt;h3 id=&quot;数据优化&quot;&gt;数据优化&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;训练集合和测试集合部分特征抽取方式不一致&lt;/li&gt;
      &lt;li&gt;最后的结果过于依赖某一特征&lt;/li&gt;
    &lt;/ol&gt;

    &lt;ul&gt;
      &lt;li&gt;优化方法
        &lt;ul&gt;
          &lt;li&gt;在全连接层 增加dropout层，设置神经元随机失活的比例为0.3，即keep_rate= 0.7&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;在数据预处理的时候，随机去掉10%的A特征&lt;/li&gt;
    &lt;/ul&gt;

    &lt;ol&gt;
      &lt;li&gt;泛化能力较差&lt;/li&gt;
    &lt;/ol&gt;

    &lt;ul&gt;
      &lt;li&gt;优化方法
        &lt;ul&gt;
          &lt;li&gt;增加槽位抽取：针对部分query, 增加槽位抽取的处理，比如将英文统一用&lt;ENG&gt;表示，模型见到的都是一样的，不存在缺乏泛化能力的问题. 瓶颈在于槽位抽取的准确率。&lt;/ENG&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;ol&gt;
      &lt;li&gt;缺乏先验知识&lt;/li&gt;
    &lt;/ol&gt;

    &lt;h3 id=&quot;模型优化&quot;&gt;模型优化&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;embedding_dim长度&lt;/li&gt;
      &lt;li&gt;在全连接层增加dropout层&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 14 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>信息抽取-seq2seq_DGCNN</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;数据类型&quot;&gt;数据类型&lt;/h2&gt;

&lt;h3 id=&quot;原始数据类型&quot;&gt;原始数据类型&lt;/h3&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;九玄珠是在纵横中文网连载的一部小说，作者是龙马&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;spo_list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;九玄珠&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;连载网站&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;纵横中文网&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;九玄珠&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;作者&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;龙马&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;笔者实际任务&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2、负责oa系统建设、运营及维护&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;spo_list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;oa系统&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;建设&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;oa系统&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;运营&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;oa系统&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;维护&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2、策划并运营公司品牌及产品在线上的推广与管理&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;spo_list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;策划&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;公司品牌&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;运营&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;公司品牌&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;产品&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;线上&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;线上&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;推广&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;线上&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;管理&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;数据输出就是三元组（s，p，o）的形式，s就是subject，即主实体，o是object，即客实体，p是predicate，实体关系，最终从主实体链接到客实体，客实体若还是另一三元组的主实体，则继续链接到客实体作为主实体的客实体，即s-&amp;gt;o(s)-&amp;gt;o。&lt;/p&gt;

&lt;h3 id=&quot;样本特点&quot;&gt;样本特点&lt;/h3&gt;

&lt;p&gt;观察数据发现“一对多”的抽取+分类任务，数据有如下特点&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;s和o的实体并不一定都能被切词工具切出来，因此存在切错边界的问题，所以我们采用字来做；&lt;/li&gt;
  &lt;li&gt;样本存在多种情况，有一个s对应多个o的情况，eg：线上推广与管理-&amp;gt;线上推广、线上管理，也存在多个s对应一个o的情况，eg：软件开发以及调试-&amp;gt;软件开发、软件调试；&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;模型设计&quot;&gt;模型设计&lt;/h2&gt;

&lt;p&gt;seq2seq解码器建模如下$P(y_1,y_2,…,y_n∣x)=P(y_1∣x)P(y_2∣x,y_1)…P(y_n∣x,y_1,y_2,…,y_{n−1})$&lt;/p&gt;

&lt;p&gt;实际预测的时候，先通过x来预测第一个单词，然后假设第一个词已知预测第二个单词，以此类推，知道结果标记出现。在三元组中参考此思路。&lt;/p&gt;

&lt;p&gt;$P(s,p,o)=P(s)P(o∣s)P(p∣o,s)P(s,p,o)$&lt;/p&gt;

&lt;p&gt;通过先预测s，然后传入s来预测s对应的o，在传入s，o来预测关系p，实际应用中，我们还可以把o，p的预测合并为一步。&lt;/p&gt;

&lt;p&gt;理论上，上述模型只能抽取单一一个三元组，为了处理多个情况，我们全部使用“半指针-半标注”结构，即将softmax换成sigmoid，在DGCNN中介绍。&lt;/p&gt;

&lt;div class=&quot;language-tex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# copy
注1：为什么不先预测o然后再预测s及对应的p？

那是因为引入在第二步预测的时候要采样传入第一步的结果（而且只采样一个），而前面已经分析了，
多数样本的o的数目比s的数目要多，所以我们先预测s，然后传入s再预测o、p的时候，对s的采样就
很容易充分了（因为s少），反过来如果要对o进行采样就不那么容易充分（因为o可能很多）。

带着这个问题继续读下去，读者会更清楚地认识到这一点。

注2：刷到最近的arxiv论文，发现在思想上，本文的这种抽取设计与文章《Entity-Relation 
Extraction as Multi-Turn Question Answering》类似。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;模型结构&quot;&gt;模型结构&lt;/h2&gt;

&lt;p&gt;模型结构示意图如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/信息抽取1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;模型处理流程如下:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;输入字id序列，然后通过字词混合Embedding得到对应的子向量序列，然后加上Position Embedding；&lt;/li&gt;
  &lt;li&gt;将得到的“字-词-位置Embedding”输入到12层DGCNN进行编码，得到编码后的序列（记为$H$）；&lt;/li&gt;
  &lt;li&gt;将$H$传入一层Self Attention后，将输出结果与先验特征进行拼接（先验可加可不加）；&lt;/li&gt;
  &lt;li&gt;将拼接后的结果传入CNN、Dense，用“半指针-半标注”结构预测s的首、尾位置；&lt;/li&gt;
  &lt;li&gt;训练时随机采样一个标注的s（预测时逐一遍历所有的s），然后将HHH对应此s的子序列传入到一个双向LSTM中，得到s的编码向量，然后加上相对位置的Position Embedding，得到一个与输入序列等长的向量序列；&lt;/li&gt;
  &lt;li&gt;将HHH传入另一层Self Attention后，将输出结果与第5步输出的向量序列、先验特征进行拼接（先验特征可加可不加，构建方式后面再详细介绍）；&lt;/li&gt;
  &lt;li&gt;将拼接后的结果传入CNN、Dense，对于每一种p，都构建一个“半指针-半标注”结构来预测对应的o的首、尾位置，这样就同时把o、p都预测出来了。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;问题一：为啥第5步只采样一个s？&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1、采样一个就够了，采样多个相当于等效增大batch size；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;2、采样一个比较好操作。&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;模型细节&quot;&gt;模型细节&lt;/h2&gt;

&lt;h3 id=&quot;字词混合embedding&quot;&gt;字词混合Embedding&lt;/h3&gt;

&lt;p&gt;单纯的字向量难以表达语义信息，而词向量有语音信息，但随之有很多切词问题需要解决，比如如何切准确；文章提出一种自行设计的字词混合方式，在多个任务中均取得了有效的提升，具体做法如下：&lt;/p&gt;

&lt;p&gt;首先以字为单位的文本序列，经过Embedding后得到字向量序列；然后将文本分词，通过预训练好的模型来提取对应的词向量，为了得到字向量对齐到词向量序列，我们可将每个词重复“词的字数”那么多次；得到对齐的词向量序列后，我们将词向量序列经过一个矩阵变换到跟字向量一样的维度，并将两者相加。流程图如下：&lt;/p&gt;

&lt;p&gt;字词混合Embedding方式图示&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/字词编码.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;实现上，文章使用pyhanlp作为分词工具，训练了一个Word2Vec模型（Skip Gram + 负采样），而字向量则使用随机初始化的字Embedding层，在模型训练过程中，固定Word2Vec词向量不变，只优化变换矩阵和字向量，从另一个角度看也可以认为是我们是&lt;strong&gt;通过字向量和变换矩阵对Word2Vec的词向量进行微调&lt;/strong&gt;。这样一来，我们既融合了预训练词向量模型所带来的&lt;strong&gt;先验语义信息，又保留了字向量的灵活性&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;position-embedding&quot;&gt;Position Embedding&lt;/h3&gt;

&lt;p&gt;模型主要使用CNN+Attention进行编码，所以编码出的向量位置信息并不敏感，信息抽取中，往往s在于开头，o在s附近，加入一个有效的位置编码信息是有用的；&lt;/p&gt;

&lt;p&gt;具体做法是设定一个最大长度为512，然后全零初始化一个新的Embedding层（维度和字向量一样），传入位置ID后输出对应的Position Embedding，并把这个Position Embedding加到前面的字词混合Embedding中，作为完整的Embedding结果，传入到下述DGCNN中。&lt;/p&gt;

&lt;p&gt;模型另一处用到了Position Embedding是在编码s的时候，采样得到的s经过BiLSTM进行编码后，得到一个固定大小的向量，然后我们将它复制拼接到原来的编码序列中，作为预测o、p的条件之一。不过考虑到o更可能是s附近的词，所以笔者并非直接统一复制，而是复制同时还加上了当前位置相对于s所谓位置的“相对位置向量”（如果对此描述还感觉模糊，请直接阅读源码），它跟开头的输入共用同一个Embedding层。&lt;/p&gt;

&lt;h3 id=&quot;dgcnn&quot;&gt;DGCNN&lt;/h3&gt;

&lt;p&gt;这部分接受另起一篇介绍&lt;/p&gt;

&lt;h2 id=&quot;知识蒸馏&quot;&gt;知识蒸馏&lt;/h2&gt;

&lt;p&gt;该部分内容是作者在实验阶段的处理手法，基于任务不同数据不同，笔者只单纯对知识蒸馏做一总结，在实际任务中，我们训练集往往有一定的缺陷以及不规范，因此可以使用类似知识蒸馏的方式来重新整理训练集，改善训练集质量。&lt;/p&gt;

&lt;p&gt;具体的，我们通过对原始训练集以交叉验证的方式，得到k个模型，然后利用k个模型对训练集进行预测，得到关于训练集的k份预测结果，如果某个样本同时在k份预测结果中但没有在训练集中标注，那么可以对此进行标注，同样的，某个样本在k份预测中都没有出现，但标注了，那么可以对此标注删除，这样以增一减之后训练集就会完善很多，这种方式也在我们实际各个任务中可以使用。&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;

&lt;p&gt;https://kexue.fm/archives/6671&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96-seq2seq_DGCNN)/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96-seq2seq_DGCNN)/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
  </channel>
</rss>
