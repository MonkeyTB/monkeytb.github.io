<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YiYao</title>
    <description>欢迎来到我的个人站~</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 05 Dec 2021 13:51:34 +0800</pubDate>
    <lastBuildDate>Sun, 05 Dec 2021 13:51:34 +0800</lastBuildDate>
    <generator>Jekyll v4.2.1</generator>
    
      <item>
        <title>Transformer探索</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;1什么是transformer&quot;&gt;1、什么是Transformer&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;《Attention is All You Need》是一篇谷歌提出的将attention思想发挥到极致的论文，这篇论文提出一个全新的模型，叫Transformer，抛弃了以往深度学习中的CNN和RNN。包括后来的Bert、GPT等都是基于此构建的，广泛的应用于NLP领域。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2transformer结构&quot;&gt;2、Transformer结构&lt;/h2&gt;

&lt;h3 id=&quot;21-总体结构&quot;&gt;2.1 总体结构&lt;/h3&gt;

&lt;p&gt;​	Transformer的结构和Attention模型一样，Transformer模型中也采用了encoder-decoder结构。论文中encoder层由6个encoder堆叠在一起，decoder层也一样。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点仅仅关注当前的词，从而能获得上下文语意&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;decoder，包含encoder提到的两层，但这两层（encoder-decoder）中间还有一层attention层，帮助当前节点获得需要关注的重点内容。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/transformer.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-encoder&quot;&gt;2.2 Encoder&lt;/h3&gt;

&lt;p&gt;​	如图所示，encoder由6个相同的transformer block构成，针对一个block来讲，我们可以看出，首先对其输入进行embedding编码输入，这一步呢类似于word2vec编码，然后通过self-attention送到前馈神经网络，然后到下一个block。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/transformer-encoder.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面对encoder每个模块进行说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;输入embedding&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;对输入文本进行向量编码，值得注意的是，编码维度为$\color{blue}{d_{model}}$，encoder的输入和decoder的输入我们进行了权值共享，并且在embedding层每个权重都乘了一个$\color{blue}{\sqrt{d_{model}}}$。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;位置embedding&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;Transformer结构由于天然无法感知空间，因此对于文本位置信息并不明确，而对于我们的翻译类任务，顺序信息尤为重要，因此引入位置信息的表达也就迫在眉睫，论文中引入了绝对位置编码，公式如下：&lt;/p&gt;

      &lt;p&gt;$\color{blue}{PE_{(pos,2i)}=sin(pos/10000^{\frac{2i}{d_{model}}})}$&lt;/p&gt;

      &lt;p&gt;$\color{blue}{PE_{(pos,2i)}=cos(pos/10000^{\frac{2i}{d_{model}}})}$&lt;/p&gt;

      &lt;p&gt;其中$\color{blue}{pos}$代表位置，$\color{blue}i$代表向量维度中的index，这样对每个维度都有一个位置编码，偶数位置用正弦编码，奇数位置用余弦编码，最后将位置embedding和输入embedding相加送入block模块中。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;layer normalization&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;这里主要说明一下Layer normalization 和Batch normalization的区别，他们都是数据归一化的一种方式，本质都是为了解决梯度消失或者梯度爆炸，主要是将模型中每层偏离的数据拉回归一化数据，这样有利于模型的学习（梯度更新）。区别如图所示：&lt;/p&gt;

      &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/Layer Normalization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

      &lt;p&gt;图中可以看出，假设输入为三个字符，每个字符的embedding为6维，那么Batch Normalization是在特征维度进行归一化，而Layer Normalization不同的是在seq也就是图中的batch方向归一化，既对每个字符进行归一化。&lt;/p&gt;

      &lt;p&gt;（小声bb：图中的batch当作sequence理解，纵向当作每个word的embedding理解，理解二维后再提升到三维理解（带上batch））&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;23-decoder&quot;&gt;2.3 Decoder&lt;/h3&gt;

&lt;p&gt;​	decoder和encoder结构大同小异，基本一致，也是对输入做了embedding和位置编码，可以参照encoder，不同之处在于decoder中用到了mask attention，这是较为关键的一点，下面我们对论文提到的三种attention进行说明。&lt;/p&gt;

&lt;h3 id=&quot;24-attention&quot;&gt;2.4 Attention&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;self attention&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\color{yellow}{self-attention}$再论文中用于encoder阶段，既每个block模块中对输入进行了$\color{yellow}{self-attention}$，也叫自注意力机制，自注意力机制输入的$\color{blue}{query、key、value}$是相同的，简单来讲，论文中的输入为（None，512），copy为三份，对每一份都用不同的权重矩阵（$\color{blue}{W^q、W^k、W^v\in R^{64\times 512}}$)，如图所示：&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/self attention 1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p&gt;对于$\color{yellow}{self-attention}$来讲，就是要对当前query的词计算对每个词的注意力权重，这个计算由query和key计算得来，计算方式如下，对$\color{blue}{q_1}$来讲，分别计算每个k的Score，论文中提到，对于$\color{blue}{q}$和$\color{blue}{k}$的点乘结果除以一个8（$\color{blue}{\sqrt{64}}$），最后做一个softmax，就得到当前q对每个q的注意力分值，对应分值再乘以value向量，然后相加，便得到当前节点的self attention的值。&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/self attention 2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p&gt;实际计算中，为了提升效率，q、k、v都为矩阵，计算公式如下：$\color{blue}{Attention(K,Q,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V}$。论文中也把这种方式叫做$\color{yellow}{Scaled Dot-Product Attention}$。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;attention&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;解码器和编码器之间也用到attention，我们暂叫做普通attention，就是相当于把$\color{yellow}{self-attention}$的query向量变为了decoder中的输入编码，encoder的输出向量作为key、value向量。计算方式同上。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;mask attention&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\color{yellow}{mask-attention}$用于解码器中，由于再解码的过程中，实际上我们并不知道下文会出现什么，而天然的attention会自动学习到后面的信息，这对于实际预测来讲并不公平，因此采用了$\color{yellow}{mask-attention}$，故名思意，就是对当前节点我们通过mask的方式不让attention看到后面的信息，transformer中具体用到了两种mask，一种是$\color{yellow}{padding-mask}$，这部分再$\color{yellow}{self-attention}$中用到，一个是$\color{yellow}{sequence-mask}$，这部分再解码器中用到，具体做法如下：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;padding mask&lt;/li&gt;
  &lt;/ul&gt;

  &lt;blockquote&gt;
    &lt;p&gt;$\color{yellow}{padding-mask}$是因为模型接受到的输入长度不同，为了便于计算我们通常要padding到相同的长度，这就会有一些短的会补0，而这些是没有意义的，因此再计算attention的时候我们需要把这部分无用的剔除。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！而我们的$\color{yellow}{padding-mask}$实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。&lt;/p&gt;
  &lt;/blockquote&gt;

  &lt;ul&gt;
    &lt;li&gt;sequence mask&lt;/li&gt;
  &lt;/ul&gt;

  &lt;blockquote&gt;
    &lt;p&gt;$\color{yellow}{sequence-mask}$是为了解码的时候不看到未来的信息，具体做法就是我们构造一个上三角全为0的的矩阵作用到序列上，这样我们再解码计算的时候就不会看到但前节点以后的信息了。&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;25-multi-head-attention&quot;&gt;2.5 Multi-head Attention&lt;/h3&gt;

&lt;p&gt;​	原论文中说到进行Multi-head Attention的原因是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，最后再将各个方面的信息综合起来。其实直观上也可以想到，如果自己设计这样的一个模型，必然也不会只做一次attention，多次attention综合的结果至少能够起到增强模型的作用，也可以类比CNN中同时使用&lt;strong&gt;多个卷积核&lt;/strong&gt;的作用，直观上讲，多头的注意力&lt;strong&gt;有助于网络捕捉到更丰富的特征/信息&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&quot;3transformer相比于rnnlstm有什么优势为什么&quot;&gt;3、Transformer相比于RNN/LSTM，有什么优势？为什么？&lt;/h2&gt;

&lt;p&gt;​	1、Transformer 相比RNN/LSTM并行计算的能力更好，序列信息由于要等信息的传递，所以并行度不高。&lt;/p&gt;

&lt;p&gt;​	2、Transformer 相比RNN/LSTM特征提取能力更强，但不代表可以完全取代RNN/LSTM，任何模型都有其特定的使用场景。&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/mantch/p/11591937.html&quot;&gt;1、Transformer各层网络结构详解！面试必备！(附代码实现)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://space.bilibili.com/1567748478/video&quot;&gt;2、视频&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://readpaper.com/pdf-annotate/note?noteId=627057414680178688&amp;amp;pdfId=4557871218548547585&quot;&gt;3、论文&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 04 Dec 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/12/Transformer/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/12/Transformer/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>生成式模型探索</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h1 id=&quot;简述&quot;&gt;简述&lt;/h1&gt;

&lt;p&gt;​	工作中需要对jd职位描述生成一些具有总结性的话语，可以理解为自动文本摘要，因此尝试了如下模型，记录以下碰到的问题以及改进，话不多说，直接上干货吧~&lt;/p&gt;

&lt;h1 id=&quot;先知&quot;&gt;先知&lt;/h1&gt;

&lt;p&gt;​	由于是工作探索阶段，因此没有很好的数据，具体方式如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;通过训练NER提取关键词（这一步怎么来的就不再介绍），关键词组成的语句作为label；&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;seq2seqattention&quot;&gt;Seq2seq+Attention&lt;/h1&gt;

&lt;h2 id=&quot;训练方式&quot;&gt;训练方式&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;jieba切词，词为单元训练&lt;/li&gt;
  &lt;li&gt;训练使用 teach forcing，预测使用 Beam search&lt;/li&gt;
  &lt;li&gt;encoder 和 decoder都使用 lstm&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;测试结果&quot;&gt;测试结果&lt;/h2&gt;

&lt;h3 id=&quot;停不下来寻因探究&quot;&gt;停不下来寻因探究&lt;/h3&gt;

&lt;p&gt;​	ICML 2020的文章&lt;a href=&quot;https://arxiv.org/abs/2002.02492&quot;&gt;《Consistency of a Recurrent Language Model With Respect to Incomplete Decoding》&lt;/a&gt;比较系统地讨论了这个现象，并提出了一些对策。&lt;/p&gt;

&lt;p&gt;​	seq2seq解码是一个”自回归“生成模型，$\color{blue}{p(y_t)|y&amp;lt;t,x}$，那么解码过程就是给定$\color{blue}{x}$来输出对应的$\color{blue}{y=(y_1,y_2,…,y_T)}$，解码算法大致分为$\color{red}{确定性解码和随机性解码}$，下面简单介绍以下这两种算法。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;确定性解码&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;确定性解码就是输入固定的文本，解码出来的结果也是固定的，这类算法包括贪心搜索(Greedy Search)和束搜索(Beam Search)，其中贪心搜索就是束搜索的极端情况，Beam Search在前面blog中简单提过，其实就是每次选取top K概率最大的。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;随机解码&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;随机性解码，就是输入固定输出却不固定，主要包括下面三种情况&lt;/p&gt;

  &lt;p&gt;1、原始采样随机解码，原始采样法是一种针对有向图模型的基础采样方法，指从模型所表示的联合分布中产生样本，又称祖先采样法。该方法所得出的结果即视为原始采样。具体的每步按概率随机采样一个token，比如第一步算$\color{blue}{p(y_1|y_0,x)}$，然后按照概率随机采样一个token，比如c；计算第二不，$\color{blue}{p(y_1|y_0,c,x)}$，再按随机概率采样一个token，比如a，依次下去，知道采样到&lt;eos&gt;停止。&lt;/eos&gt;&lt;/p&gt;

  &lt;p&gt;2、&lt;em&gt;top-k&lt;/em&gt; 随机解码：，出自文章&lt;a href=&quot;https://arxiv.org/abs/1805.04833&quot;&gt;$\color{yellow}{《Hierarchical Neural Story Generation》}$&lt;/a&gt;，就是再原生采样随机解码的基础上加了个截断，每一步只保留概率最高的$\color{blue}k$个token，然后重新归一化在采样，这么做是希望再“得分高”和“多样性”做了一个这种。$\color{bule}k=1$时，就是贪心搜索。&lt;/p&gt;

  &lt;p&gt;3、Nucleus随机解码：出自文章&lt;a href=&quot;https://arxiv.org/abs/1904.09751&quot;&gt;$\color{yellow}{《The Curious Case of Neural Text Degeneration》}$&lt;/a&gt;,跟&lt;em&gt;top-k&lt;/em&gt; 随机解码类似，也是对采样空间做了截断，阶段方式是——固定$\color{blue}p\in(0,1)$，然后只保留概率最高的、概率和刚好超过$\color{blue}p$的若干token，所以也叫&lt;em&gt;top-p&lt;/em&gt; 采样。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;​	&lt;strong&gt;从seq2seq的模型设计和上面解码算法来看，理论上不能保证解码过程一定能停下来，只能靠模型学习，当模型学习不好的时候，就会出现根本停不下来的现象&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;停不下来怎么解决(这部分不知道咋回事公式识别不好，遂改成图片展示)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1、有界的隐向量，$\color{blue}{p(y_t|y&amp;lt;t,x)=softmax(Wh_t+b),h_t=f(y&amp;lt;t,x)}$是建模概率，计算个隐向量，然后全连接，softmax激活，如果对任意的$\color{blue}t,| |h_t| |$是有上界的，那么原始采样解码就能”停止“。说简单点就是只要你采样够多，就能停，实际应用价值就不大了。&lt;/p&gt;

&lt;p&gt;2、1的方法是针对原始采样方法的，对&lt;em&gt;top-k&lt;/em&gt; 随机解码和Nucleus随机解码就不一定成功了，因为&lt;eos&gt;不一定会出现再采样空间，要想使用只能手动把&lt;eos&gt;添加到采样空间。&lt;/eos&gt;&lt;/eos&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;这两个结论只能用于随机解码，确定性解码就不能用了，因为没法保证一定能碰到，因此可以使用下面方法&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;3、自截断：这是论文稍微有意义的地方，自截断说白了就是随着解码过程，我们希望能碰到&lt;eos&gt;，这样就能停下来了，并且希望随着解码长度的增加碰到&lt;eos&gt;的概率也越来越多大，最终趋于1，这样就可以停下来了。定义&lt;/eos&gt;&lt;/eos&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/自截断.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里$\color{blue}{\sigma(·)}$将$\color{blue}R$映射到$\color{blue}{[0,1-ϵ]}$，例如可以用$\color{blue}{\sigma(·)=(1-ϵ)sigmoid(·)}$。设计好$\color{blue}{p(&lt;eos&gt;|y&amp;lt;t,x)}$，剩下的token概率按照原来的softmax方式计算，然后乘以$\color{blue}{ \alpha (h_t)}$即可。现在有&lt;/eos&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/自截断-1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/自截断-2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;预测文本&lt;/th&gt;
      &lt;th&gt;hyp.text :&lt;START&gt; 3 定期 组织 绩效考核 指标 目标 完成 情况 进行 过程 管控 &lt;STOP&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/STOP&gt;&lt;/START&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;top 0 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; 绩效考核 指标 考核 指标 &lt;STOP&gt;&lt;/STOP&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;top 1 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; 绩效考核 指标 考核 指标 目标 &lt;STOP&gt;&lt;/STOP&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;top 2 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; 绩效考核 指标 考核 指标 目标 指标 &lt;STOP&gt;&lt;/STOP&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;问题：重复&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;重复问题寻因探究&quot;&gt;重复问题寻因探究&lt;/h3&gt;

&lt;p&gt;​	AAAI2021有一篇论文从理论上分析了seq2seq重复解码的现象，本质上，重复和不停是同理的，算是填补了上面论文的空白，论文链接&lt;a href=&quot;https://arxiv.org/abs/2012.14660&quot;&gt;A Theoretical Analysisi of the repetition Problem in Text Generation&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;​	先量化以下问题，解码过程中子序列$\color{blue}{s=[w_1,w_2,…,w_n]}$后面接子序列$\color{blue}{t=[w_1,w_2,…,w_n,w_1]}$，我们就称$\color{blue}{w_1,w_2,…,w_n}$是一个$\color{red}{”重复子序列”}$，而我们要做的就是分析解码过程中重复子序列的概率。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;二元解码&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;​	一般的自回归形式为: $\color{blue}{p(y|x)=\prod_{t=1}^lp(y_t|y_{y\lt t,x})}$，可以看出位置&lt;em&gt;t&lt;/em&gt; 的解码不仅依赖输入&lt;em&gt;x&lt;/em&gt; ，还依赖&lt;em&gt;t&lt;/em&gt; 之前已经获得的所有解码结果，我们考虑简单的情况，做个马尔可夫假设，每一步解码只依赖前一时刻的结果，与再之前的无关，即$\color{blue}{\prod_{t=1^lp(y_t|y_{t-1},x)}}$，这样对于固定的&lt;em&gt;x&lt;/em&gt; 而言，解码器实际上就是一个$\color{blue}{n\times n}$的转移矩阵$\color{blue}{P=(P_{i,j})}$，其中$\color{blue}{P_{i,j}}$表示从&lt;em&gt;i&lt;/em&gt; 后面接&lt;em&gt;j&lt;/em&gt; 的概率，&lt;em&gt;n&lt;/em&gt; 代表词表大小，还需要一个结束标记&lt;eos&gt;，遇到&lt;eos&gt;就停止解码，所以实际上转移矩阵是$\color{blue}{(n+1)\times(n+1)}$，但是我们考虑的重复解码都是终止之前，所以只需要考虑$\color{blue}{n\times n}$就可以了。&lt;/eos&gt;&lt;/eos&gt;&lt;/p&gt;

  &lt;p&gt;​	我们要计算的是重复子序列出现的概率，假设$\color{blue}{[i,j,k]}$，是一个三元重复子序列，那么它出现概率就是序列$\color{blue}{[i,j,k,i,j,k,i]}$出现的概率：&lt;/p&gt;

  &lt;p&gt;​									$\color{blue}{P_{i,j}P_{j,k}P_{k,i}P_{i,j}P_{j,k}P_{k,i}=P_{i,j}^2p_{j,k}^2P_{k,i}^2}$&lt;/p&gt;

  &lt;p&gt;因此所有的三元重复子序列的概率为：&lt;/p&gt;

  &lt;p&gt;​									$\color{blue}{\sum_{i,j,k}P_{i,j}^2P_{j,k}^2P_{k,i}^2=Tr(P\otimes P)^3}$&lt;/p&gt;

  &lt;p&gt;这里的$\color{blue}{\otimes}$表示逐位元素对应相乘，而$\color{blue}{Tr}$则是矩阵的迹，即对角线元素之和。最后我们将所有长度的重复子序列概率都加起来：&lt;/p&gt;

  &lt;p&gt;​									$\color{blue}{R=\sum_{k=1}^{\infty}}Tr(P\otimes P)^k=Tr(\sum_{k=1}^\infty(P\otimes P)^k)$&lt;/p&gt;

  &lt;blockquote&gt;
    &lt;p&gt;两个矩阵对应位置$\color{blue}{\otimes}$的目的就是为了使得每个位置上的值变为原来的平方，因此就实现了$\color{blue}{P_{i,j}P_{j,k}P_{k,i}P_{i,j}P_{j,k}P_{k,i}=P_{i,j}^2p_{j,k}^2P_{k,i}^2}$的目的。&lt;/p&gt;

    &lt;p&gt;$\color{blue}{(P\otimes P)^k}$中的&lt;em&gt;k&lt;/em&gt; 实际上表示&lt;em&gt;k&lt;/em&gt; 元重复子序列，那为什么这个&lt;em&gt;k&lt;/em&gt; 次方的矩阵求迹就是结果了？以$\color{blue}{k=2}$为例（其中蓝色矩阵中的每一项都是平方项，只不过我并没有显式的标出平方）&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/迹.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;对角线上每一个元素代表固定&lt;em&gt;i&lt;/em&gt; 而遍历&lt;em&gt;j&lt;/em&gt; 的结果，那么把所有的对角线都加起来（迹），实际上就是做了一个$\color{blue}{\sum_i}$的操作。&lt;/p&gt;
  &lt;/blockquote&gt;

  &lt;p&gt;​	这就是二元解码出现&lt;strong&gt;重复解码的概率&lt;/strong&gt;，目前只是一个理论公式，不过是我们重要的出发点，分别推到它的上下界。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;一个下界&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;直接看$\color{blue}{R}$的公式看不出啥，我们可以给他先推到一个直观的下界，以三元重复子序列为例可以得到如下：&lt;/p&gt;

  &lt;p&gt;​								$\color{blue}{\sum_{i,j,k}P_{i,j}^2P_{j,k}^2P_{k,j}^2=n^3\times\frac{\sum_{i,j,k}P_{i,j}^2P_{j,k}^2P_{k,i}^2}{n^3}\gt n^3\times(\frac{\sum_{i,j,k}P_{i,j}^2P_{j,k}^2P_{k,i}^2}{n^3})^2=\frac{(TrP^3)^2}{n^3}}$&lt;/p&gt;

  &lt;blockquote&gt;
    &lt;p&gt;上面的推到可以通过均值不等式得到，$\color{blue}{\sqrt{\frac{\sum x^2}{m}}\ge \frac{\sum x}{m}}$两边同时平方就医得到$\color{blue}{\frac{\sum x^2}{m}\ge (\frac{\sum x}{m})^2}$。&lt;/p&gt;
  &lt;/blockquote&gt;

  &lt;p&gt;$\color{blue}{\frac{(TrP^3)^2}{n^3}}$就是下界，但是我们可以做的更精细一点。假设&lt;em&gt;P&lt;/em&gt; 矩阵中有一些元素为0，那么$\color{blue}{P_{i,j}^2P_{j,k}^2P_{k,j}^2}$中非零元素的个数就不是$\color{blue}{n^3}$，我们假设非零元素的个数为$\color{blue}{N_3(P)\lt n^3}$，那么我们在利用均值不等式的时候，可以只对非零元素进行，替换后如下：&lt;/p&gt;

  &lt;p&gt;​									$\color{blue}{\sum_{i,j,k}{P_{i,j}^2P_{j,k}^2P_{k,i}^2}\ge\frac{(TrP^3)^2}{N_3(P)}}$&lt;/p&gt;

  &lt;p&gt;其中$\color{blue}{N_3(P)}$的直接计算也比较困难，没有一般的通项公式，但我们可以做一个简单的估算：设$\color{blue}P$的非零元素的比例为$\color{blue}{\zeta}$，也就是非零元素的个数为$\color{blue}{\zeta n^2}$，那么我们一个认为$\color{blue}{P_{i,j}^2P_{j,k}^2P_{k,i}^2}$的非零元素比例近似为$\color{blue}{\zeta^3}$，而总的排列数为$\color{blue}{n^3}$，所以我们可以可以认为$\color{blue}{N_3(P)\backsim\zeta^3n^3}$。注意可以举例说明这个既不能保证上界，也不能保证下界，所以替换后无法保证等号成立。不过如果我们愿意相信是一个足够好的近似，那么可以写下&lt;/p&gt;

  &lt;p&gt;​								$\color{blue}{\sum_{i,j,k}{P_{i,j}^2P_{j,k}^2P_{k,i}^2}\ge\frac{(TrP^3)^2}{\zeta^3n^3}}$&lt;/p&gt;

  &lt;p&gt;以及&lt;/p&gt;

  &lt;p&gt;​								$\color{blue}{R=\sum_{k=1}^{\infty}Tr(P\otimes P)^k\ge\sum_{k=1}^{\infty}\frac{(TrP^k)^2}{\zeta^kn^k}}$&lt;/p&gt;

  &lt;p&gt;或者我们不关心等号，而将最右面的结果视为&lt;em&gt;R&lt;/em&gt; 的一个估计。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;初步结论&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;解码概率经过softmax出来，结果都不等于0，那么$\color{blue}{\zeta}$应该恒等于1，因此引入$\color{blue}{\zeta}$似乎没有什么价值。&lt;/p&gt;

  &lt;p&gt;事实上，softmax确实不会产生0概率值，但是解码算法通常会强制为零，在三种随机性解码以及两种确定性解码中，除了不靠谱的随机性解码中的原始采样随机解码外，其他都会或多或少确定若干个最优结果来作为候选值，这就相当于&lt;strong&gt;直接截断了转移概率矩阵&lt;/strong&gt;，大大的降低了$\color{blue}{\zeta}$（非零概率）。&lt;/p&gt;

  &lt;p&gt;比如Greedy Search中，容易推出实际上对应这最小的非零概率$\color{blue}{\zeta=1/n}$，由于$\color{blue}{\zeta}$在公式的分母位置，因此$\color{blue}{\zeta}$的缩小就意味着重复了$\color{blue}{R}$的增加，这也就证明了Greedy Search的重复解码风险是相当高的，这也与实际中现象一致，也证明了在Beam Search中，我们Beam size取得越大，越不容易重复（仍会出现），但是会增加计算量。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;一个上界&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\color{yellow}{下界能帮我们解释一些现象，而上界则可以给我们提供改进思路。}$&lt;/p&gt;

  &lt;p&gt;为了推导上界，我们利用如下的两个结论&lt;/p&gt;

  &lt;blockquote&gt;
    &lt;p&gt;1、矩阵的迹等于它所有特征值的和&lt;/p&gt;

    &lt;p&gt;2、如果$\color{blue}{\lambda_1(A)\ge\lambda_2(A)\ge…\ge\lambda_n(A)}$是矩阵$\color{blue}{A}$的所有特征值，那么$\color{blue}{\lambda_1^k(A)\ge\lambda_2^k(A)\ge…\ge\lambda_n^k(A)}$是矩阵$\color{blue}{A^k}$的所有特征值。&lt;/p&gt;
  &lt;/blockquote&gt;

  &lt;p&gt;所以可以推导：&lt;/p&gt;

  &lt;p&gt;$\color{blue}{R=\sum_{k=1}^{\infty}Tr(P\otimes P)=\sum_{k=1}^{\infty}\sum_{i=1}^n\lambda_i((P\otimes P)^k)}$&lt;/p&gt;

  &lt;p&gt;$\color{blue}{=\sum_{k=1}^{\infty}\sum_{i=1}^{n}\lambda_i^k(P\otimes P)=\sum_{i=1}^{n}\sum_{k=1}^{\infty}\lambda_i^k(P\otimes P)}$&lt;/p&gt;

  &lt;p&gt;$\color{blue}{=\sum_{i=1}^n\frac{\lambda_i(P\otimes P)}{1-\lambda_i(P\otimes P)}}$&lt;/p&gt;

  &lt;p&gt;​	上述过程用到了级数$\color{blue}{\frac{x}{1-x}=\sum_{k=1}^{\infty}x^k}$，该级数只有在$\color{blue}{|x|\lt1}$才收敛，而很巧的是，我们可以证明$\color{blue}{P\otimes P}$的特征值绝对值必然不大于1，且通常都小于1：由于$\color{blue}{P}$是转移矩阵，因此它的每一行之和都为1，因此$\color{blue}{(P\otimes P)x=\lambda x}$，不失一般性，设$\color{blue}{x}$绝对值最大的元素为$\color{blue}{x_1,P\otimes P}$的第一个行向量为$\color{blue}{q_1^\top}$，那么我们有$\color{blue}|\lambda| |x_1|\le|q_1^{\top}x|\le|x_1|$，从而$\color{blue}{|\lambda|\le 1}$，并且等号成立的条件还是比较苛刻的，所以通常来说$\color{blue}{|\lambda|\lt 1}$。&lt;/p&gt;

  &lt;p&gt;​	注意函数$\color{blue}{\frac{x}{1-x}}$在区间[-1,1]区间是单调递增的，所以公式中主导的是第一项，如果非要给整体弄上一个上界，那么可以是$\color{blue}{\frac{n\lambda_i(P\otimes P)}{1-\lambda_i(P\otimes P)}}$。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;再次结论&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;由此可见，如果想要降低重复率$\color{blue}{R}$，那么我们需要想办法降低矩阵$\color{blue}{P\otimes P}$的最大特征值。$\color{blue}{P\otimes P}$是一个非负矩阵，根据非负矩阵的$\color{yellow}Frobenius介值定理(非负矩阵的最大特征值在它每一行的和的最小值与最大值之间)$，我们有：&lt;/p&gt;

  &lt;p&gt;$\color{blue}{min_i\sum_{j}P_{i,j}^2\le\lambda_1(P\otimes P)\le max_i\sum_jP_{i,j}^2}$&lt;/p&gt;

  &lt;p&gt;现在我们知道，为了降低$\color{blue}{P\otimes P}$的最大特征值，我们需要想办法降低它的每一行之和，即$\color{blue}{\sum_jP_{i,j}^2}$，并且由于均值不等式&lt;/p&gt;

  &lt;p&gt;$\color{blue}{\sum_jP_{i,j}^2\ge n(\frac{\sum_jP_{i,j}}{n})^2=1/n}$&lt;/p&gt;

  &lt;p&gt;知它的最小值为$\color{blue}{1/n}$，在$\color{blue}{P_{i,1}=P_{i,2}=…=P_{i,n}}$时取到，因此最终我们得出结论：$\color{yellow}{要降低最大特征值，就要使得矩阵P每一行尽可能均匀，换言之，要降低P每一行的方差}$。&lt;/p&gt;

  &lt;p&gt;​	怎么降低方差呢，简单点来说就不能出现过高的概率值即可，比如某一行接近one hot的形式，那么平方之后仍然接近one hot的形式，那么求和就接近1，远远大于理论最小值$\color{blue}{1/n}$。实际中什么情况会出现过高的概率值呢，就是在某些字后面可以接的字很少，甚至只有一个候选项的时候，比如“忐”后面接“忑”，那么$\color{blue}{P_{i=忐}P_{j=忑}}$就相当高。那么怎么来避免出现这种过高的概率值呢，就是将高概率值得字合并起来，当作一个新词来看，那么“忐”哪一行就不存在，也就是无所谓得方差大了。&lt;/p&gt;

  &lt;p&gt;​	说白了，在文本生成任务中，以词为单位比以字为单位更加靠谱，更加不容易出现重复解码。适当地合并一些相关程度比较高的词作为新词加入到词表中，降低转移矩阵的方差，有助于降低重复解码的风险，原论文还给这个操作起了个很高端的名字，叫做Rebalanced Encoding Algorithm，事实上就是这个意思。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;一般解码&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;​	那这个证明过程容易推广到一般的自回归模型中吗？很遗憾，并不容易。对于一般的自回归模型来说，它相当于每一步的$\color{blue}{P}$都是不一样的，因此只要模型的性能足够好，其实基本上不会出现重复解码，事实上经过充分预训练的生成式模型，确实很少出现重复解码了。但是，我们又能观察到，哪怕是一般的自回归解码，偶尔也能观察到重复解码现象，尤其是没有经过预训练的模型，这又该怎么解释呢？&lt;/p&gt;

      &lt;p&gt;​	前面的小节是基于二元解码模型的，结论是二元解码模型确实容易出现重复解码，那么我们或许可以反过来想，一般的自回归模型出现重复解码现象，是因为它此时退化为了二元解码模型？对于难度比较高的输入，模型可能无法精细捕捉好每一步的转移概率，从而只能将转移矩阵退化为二元解码，这是有可能的。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;预测文本&lt;/th&gt;
      &lt;th&gt;hyp.text :&lt;START&gt; 负责 人员 招募 工作 确保 公司 业务 可持续性 发展 &lt;STOP&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/PAD&gt;&lt;/STOP&gt;&lt;/START&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;top 0 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; &lt;UNK&gt; 业务 发展 策略 &lt;STOP&gt;&lt;/STOP&gt;&lt;/UNK&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;top 1 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; &lt;UNK&gt; 业务 发展 策略 公司 &lt;STOP&gt;&lt;/STOP&gt;&lt;/UNK&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;top 2 best_hyp.abstract&lt;/td&gt;
      &lt;td&gt;&lt;START&gt; &lt;UNK&gt; 业务 发展 策略 公司 发展 &lt;STOP&gt;&lt;/STOP&gt;&lt;/UNK&gt;&lt;/START&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;问题：生成原文没有的词（统称为产生新词），这并不是想要的&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;​	引入先验知识，我们生成得词都是句子中出现得（出现并不一定连续）。这样以来，我们可以利用句子（文章）中得词集作为一个先验分布，加到解码过程得分类模型中，使得模型在解码输出时更倾向选用文章中已有得字词。&lt;/p&gt;

  &lt;p&gt;​	具体来说，就是在每一步预测时，我们得到总向量$\color{blue}{x}$（它应该是decoder当前的隐层向量、encoder的编码向量、当前decoder与encoder的Attention编码三者的拼接），然后得到全连接层，最终得到一个大小为$\color{blue}{|V|}$得向量$\color{blue}{y=(y_1,y_2,…,y_{|V|})}$，其中$\color{blue}{|V|}$就是词表得词数。经过softmax后得到原本得概率$\color{blue}{p_i=\frac{e^{y_i}}{\sum_ie^{y_i}}}$，这就是原始得分类方案。引入先验分布得方案是，对于每个句子（文章）我们得到一个大小为$\color{blue}{|V|}$得0/1向量$\color{blue}{x=(x_1,x_2,…,x_{|V|})}$，其中$\color{blue}{x_i=1}$意味该词在文章中出现过，负则为0。将这样一个0/1向量经过缩放平移层得到：&lt;/p&gt;

  &lt;p&gt;$\color{blue}{\hat y=s\otimes x+t=(s_1x_1+t_1,s_2x_2+t_2,…,s_{|V|}x_{|V|}+t+{|V|})}$&lt;/p&gt;

  &lt;p&gt;其中$\color{blue}{s,t}$为训练参数，然后将这个向量与原来得$\color{blue}{y}$取平均后才做softmax：&lt;/p&gt;

  &lt;p&gt;$\color{blue}{y = \frac{y+\hat y}{2},p_i=\frac{e^{y_i}}{\sum_ie^{y_i}} }$&lt;/p&gt;

  &lt;p&gt;经实验，这样先验分布得引入，有助于加快收敛，生成更好得短句（标题）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;参考&quot;&gt;参考&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://kexue.fm/archives/7500&quot;&gt;1、如何应对seq2seq中“根本停不下来”问题？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.02492&quot;&gt;2、Consistency of a Recurrent Language Model With Respect to Incomplete Decoding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.04833&quot;&gt;3、Hierarchical Neural Story Generation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.09751&quot;&gt;4、《The Curious Case of Neural Text Degeneration》&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kexue.fm/archives/8128&quot;&gt;5、seq2seq重复解码的理论分析&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV13b4y1f7jx/&quot;&gt;6、视频讲解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://wmathor.com/index.php/archives/1550/&quot;&gt;7、Seq2Seq 重复解码问题追根溯源&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://spaces.ac.cn/archives/5861&quot;&gt;8、玩转Kearas之seq2seq自动生成标题&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 20 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E6%8E%A2%E7%B4%A2/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E6%8E%A2%E7%B4%A2/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>Glove</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;早上看到了Glove，细思之下发现了解不多，于是抽空网上搬砖。&lt;/p&gt;

&lt;h2 id=&quot;glove是何方妖孽&quot;&gt;Glove是何方妖孽&lt;/h2&gt;

&lt;p&gt;​	Glove 全称 Global Vectors for Word Representation，基于&lt;strong&gt;全局词频统计&lt;/strong&gt;的词表征，是一种词嵌入方式，将高维稀疏向量表示为低维稠密向量，并且保留了词与词之间的相似性（即cos距离更近）。&lt;/p&gt;

&lt;h2 id=&quot;glove来龙去脉&quot;&gt;Glove来龙去脉&lt;/h2&gt;

&lt;p&gt;​	三步曲实现Glove&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一步预备&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​	根据corpus构建一个共现矩阵（Co-ocurrence Matrix）$\color{blue}{X}$，什么是共现矩阵呢？先补充一个基本概念；&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;共现：给定一个语料库，一对单词的再上下文窗口同时出现的次数&lt;/p&gt;

  &lt;p&gt;上下文窗口：给定一个单词的上下文几个单词以内范围的大小，例如指定2，$\color{blue}{w_1,w_2,w_3,w_4,w_5,…}$，$\color{blue}{w_3}$的上下文就包含了$\color{blue}{w_1到w_5}$&lt;/p&gt;

  &lt;p&gt;假设我们有一个corpus如下：&lt;/p&gt;

  &lt;div class=&quot;language-tex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;我爱中国；
我爱北京；
我爱天安门；
我爱我家；
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;p&gt;假设content window size = 2，就可以得到如下的矩阵&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt; &lt;/th&gt;
        &lt;th&gt;我&lt;/th&gt;
        &lt;th&gt;爱&lt;/th&gt;
        &lt;th&gt;中&lt;/th&gt;
        &lt;th&gt;国&lt;/th&gt;
        &lt;th&gt;北&lt;/th&gt;
        &lt;th&gt;京&lt;/th&gt;
        &lt;th&gt;天&lt;/th&gt;
        &lt;th&gt;安&lt;/th&gt;
        &lt;th&gt;门&lt;/th&gt;
        &lt;th&gt;家&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;我&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;4&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;爱&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;中&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;国&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;北&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;京&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;天&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;安&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;1&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;门&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;家&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;共现矩阵是一个对阵矩阵，因此下三角省了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;​	矩阵中每一个元素$\color{blue}{X_{ij}}$代表单词$i$和单词$j$再content window size内共现的次数，一般而言，content window size最小位1，而Glove中根据两个单词再窗口的距离$d$提出了一个衰减函数$\color{blue}{decay = 1/d}$来计算权重，距离越远的单词再总计数的权重越小，这也符合常理，越“远”越“远”。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;二步走&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​	根据共现关系构建词向量，两者关系如下:&lt;/p&gt;

&lt;p&gt;​                                  $\color{blue}{ {w_i^T} \hat{w_j} + b_i+\hat{b_j}=log(X_{ij})—–(1)}$&lt;/p&gt;

&lt;p&gt;其中：$\color{blue}{w_i和\hat{w_j}}$是最终的词向量，$\color{blue}{b_i和b_j}$分别是两个词向量的偏置。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;三步停&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​	根据公式(1)构造损失函数如下：&lt;/p&gt;

&lt;p&gt;​            $\color{blue}{J=\sum_{i,j}^Vf(X_{ij})(W_i^T\hat{w_j}+b_i+\hat{b_j}-log(X_{i,j}))^2—–(2)}$&lt;/p&gt;

&lt;p&gt;可以看出是一个均方误差函数（mean square loss），额外加了个权重函数$\color{blue}{f}$，这个权重函数如下：&lt;/p&gt;

&lt;p&gt;​            &lt;img src=&quot;/images/posts/自然语言处理/glove 公式.PNG&quot; alt=&quot;权重函数&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$f(x)$的原因如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;经常一起出现的单词对权重要大于很少出现的单词对，所以是个非递减函数&lt;/li&gt;
    &lt;li&gt;随着单词对一起出现的次数越多，希望权重不要一直增大&lt;/li&gt;
    &lt;li&gt;未出现的单词对，不要参与loss的计算，即为0&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/glove loss权重函数.jpg&quot; alt=&quot;glove loss权重函数&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	论文中$\color{blue}\alpha$取值为0.75，$\color{blue}{x_{max}}$取值100。&lt;/p&gt;

&lt;h2 id=&quot;glove的运动方式&quot;&gt;Glove的运动方式&lt;/h2&gt;

&lt;p&gt;​	Glove是一种无监督的学习方式，但是其实是有label的，这一点和word2vec一样；而Glove的label就是公式(2)中的$\color{blue}{log(X_{ij})}$，两个$w$就是需要学习更新的参数，所以本质上和监督学习的方式没什么不同，都是基于梯度下降。&lt;/p&gt;

&lt;p&gt;​	论文中采用AdaGrad梯度下降法，对矩阵$X$中所有非0元素进行随机采样，学习率0.05，再vector size 300的情况下迭代50次，再其他vector size上迭代100次，直至收敛；最后得到两个vector， 分别是$\color{blue}{w}$,$\color{blue}{\hat{w}}$，因为$X$是对称的，因此理论上,$\color{blue}{w}$,$\color{blue}{\hat{w}}$也是对称的，他们唯一的区别是初始化时参数不同，导致最后的结果有差异，为了&lt;strong&gt;更好的鲁棒性，我们最终将两者之和作为最终的vector&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/glove实验结果.jpg&quot; alt=&quot;glove实验结果&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	这个图一共采用了三个指标：语义准确度，语法准确度以及总体准确度。那么我们不难发现Vector Dimension在300时能达到最佳，而context Windows size大致在6到10之间。&lt;/p&gt;

&lt;h2 id=&quot;glove-公式1探因&quot;&gt;Glove 公式(1)探因&lt;/h2&gt;

&lt;p&gt;​	公式(1)不得不写下来呀，抄也得抄下来，遵循书读百遍，其意自现，下面就开始愉快的抄写吧！&lt;/p&gt;

&lt;p&gt;定义一些变量：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\color{blue}{X_{ij}}$：单词$j$出现再单词$i$的上下文的次数&lt;/p&gt;

  &lt;p&gt;$\color{blue}{X_i}$：单词$i$的上下文中出现所有单词出现的总次数，即$\color{blue}{X_i=\sum^kX_{ik}}$&lt;/p&gt;

  &lt;p&gt;$\color{blue}{P_{ij}}$：$\color{blue}{P(j|i)=\frac{X_{ij}}{X_i}}$，表示单词$j$出现在单词$i$的上下文中的概率&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Probability and Ratio&lt;/th&gt;
      &lt;th&gt;&lt;em&gt;k=solid&lt;/em&gt;&lt;/th&gt;
      &lt;th&gt;&lt;em&gt;k=gas&lt;/em&gt;&lt;/th&gt;
      &lt;th&gt;&lt;em&gt;k=water&lt;/em&gt;&lt;/th&gt;
      &lt;th&gt;&lt;em&gt;k=fashion&lt;/em&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$P(k|ice)$&lt;/td&gt;
      &lt;td&gt;$\color{blue}{1.9*10^{-4}}$&lt;/td&gt;
      &lt;td&gt;$6.6*10^{-5}$&lt;/td&gt;
      &lt;td&gt;$3.0*10^{-3}$&lt;/td&gt;
      &lt;td&gt;$1.7*10^{-5}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$P(k|steam)$&lt;/td&gt;
      &lt;td&gt;$\color{blue}{2.2*10^{-5}}$&lt;/td&gt;
      &lt;td&gt;$7.8*10^{-4}$&lt;/td&gt;
      &lt;td&gt;$2.2*10^{-3}$&lt;/td&gt;
      &lt;td&gt;$1.8*10^{-5}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$P(k|ice)/P(k|stream)$&lt;/td&gt;
      &lt;td&gt;$\color{blue}{8.9}$&lt;/td&gt;
      &lt;td&gt;$8.5*10^{-2}$&lt;/td&gt;
      &lt;td&gt;$1.36$&lt;/td&gt;
      &lt;td&gt;$0.96$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;​	通过上面的表格可以分析得出，当&lt;em&gt;k=solid&lt;/em&gt;时，$\color{blue}{P(solid|ice)/P(solid|stream)}$比1大很多，这说明&lt;em&gt;solid&lt;/em&gt;和&lt;em&gt;ice&lt;/em&gt;相比&lt;em&gt;solid&lt;/em&gt;和&lt;em&gt;steam&lt;/em&gt;更相关，其他同理。由此得出结论，通过&lt;strong&gt;概率比率相比概率本身学习词向量可能是一个更恰当的方法&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;​	为了达到上述目标，构造函数&lt;/p&gt;

&lt;p&gt;​							$\color{blue}{F(w_i,w_j,\hat{w_k})=\frac{P_{ik}}{P_{jk}}—–(4)}$&lt;/p&gt;

&lt;p&gt;利用向量空间的线性结构做差，可以得到&lt;/p&gt;

&lt;p&gt;​						$\color{blue}{F(w_i-w_j,\hat{w_k})=\frac{P_{ik}}{P_{jk}}—–(5)}$&lt;/p&gt;

&lt;p&gt;我们可以发现公式(5)右边是一个常数，左边为了表达两个概率的比例差，左边变成两个向量的内积，如下&lt;/p&gt;

&lt;p&gt;​						$\color{blue}{F((w_i-w_j)^T,\hat{w_k})=\frac{P_{ik}}{P_{jk}}—–(6)}$&lt;/p&gt;

&lt;p&gt;接下来就到高能时刻了，先补充一波电，&lt;a href=&quot;http://www.ubinec.org/index.php?c=download&amp;amp;id=2209&quot;&gt;同态特性&lt;/a&gt;，首先$X$是对称矩阵，单词和上下文单词是相对的，因此我们可以做如下交换$\color{blue}{w&amp;lt;-&amp;gt;\hat{w_k},X&amp;lt;-&amp;gt;X^T}$，公式(6)应该保持不变，为了满足这个条件，要求函数$F$满足同态特性：&lt;/p&gt;

&lt;p&gt;​					$\color{blue}{F((w_i-w_j)^T,\hat{w_k})=\frac{F(w_i^T\hat{w_k})}{F(w_j^T\hat{w_k})}—–(7)}$&lt;/p&gt;

&lt;p&gt;结合公式(6)，得到&lt;/p&gt;

&lt;p&gt;​						$\color{blue}{F(w_i^T)=P_{ik}=\frac {X_{ik}} {X_i} —–(8)}$&lt;/p&gt;

&lt;p&gt;令F=exp,可以得到下面公式(9)&lt;/p&gt;

&lt;p&gt;​						$\color{blue}{w_i^T\hat{w_k}=log(P_{ik})=log(X_{ik})-log(X_i)—–(9)}$&lt;/p&gt;

&lt;p&gt;===&amp;gt;&lt;/p&gt;

&lt;p&gt;​						$\color{blue}{log(X_{ik})=w_i^T\hat{w_k}+log(X_i)—–(10)}$&lt;/p&gt;

&lt;p&gt;这里就和(1)很像了，我们把$log(X_i)$纳入到$w_i$的偏置中，为了对称性，也加到$\hat{w_k}$中，得到如下公式&lt;/p&gt;

&lt;p&gt;​					$\color{blue}{log(X_{ik})=w_i^T\hat{w_k}+b_i+\hat{b_k}—–(11)}$&lt;/p&gt;

&lt;h2 id=&quot;参考链接&quot;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.fanyeong.com/2018/02/19/glove-in-detail/&quot;&gt;1、GloVe详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/mr_tyting/article/details/80180780&quot;&gt;2、论文分享–&amp;gt;GloVe: Global Vectors for Word Representation&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 16 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/Glove/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/Glove/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>A Neural Probabilistic Language Model</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;在摘要中作者提到，通过学习一个分布式的词表示来克服维数的诅咒，它允许每个训练句子向模型告知一个指数数量的语义相邻句子。该模型同时学习 (1) 每个单词的分布式表示，以及 (2) 用这些表示 表示的单词序列的概率函数。&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;在介绍中，作者举例到，如果相对10000个词建立连续10个词得联合分布，那么可能需要得参数是$10000^{10}-1$，当对连续变量进行建模时，我们比较容易获得泛化（光滑得函数类，神经网路模型，高斯模型等），对于离散空间泛化结构就不明显了，每个离散变量得取值很大时，大多观测对象在汉明距离上几乎是最大得。&lt;/p&gt;

&lt;p&gt;本文提出在高维情况下，重要的是将概率质量均匀分布在每个训练点周围的各个方向上。这里提出得泛化方式也与之前最先进得统计方法模型不同。&lt;/p&gt;

&lt;p&gt;语言得统计模型公式如下$P({w_1}^T)=\prod_{t=1}^T(P(w_t|w_1^{t-1}))$我们可以看出，每次计算t得概率时，都需要从1~t-1的概率，这无形中就增加了很大的计算量，本文利用统计上词序更加依赖暂时距离较近的词这一事实，因此提出了n-gram模型结构，计算公式如下$P(w_t|w_1^{t-1})=P(w_t|w_{t-n+1}^{t-1})$。&lt;/p&gt;

&lt;p&gt;在文本中总会出现上下文连续的词但预料中没有出现的情况，一种简单的解决办法就是将n回退到三元模型。从本质上说一个新的单词序列是通过“粘合”非常短的长度为1,2…或者在训练数据中经常出现的单词上。获取下一个片段概率的规则隐含在后退或插值n-gram算法的细节中。研究中一般采用3，但显然，单词前面的序列中又更多的信息需要预测，并不仅仅时单词前的两个单词。因此该方法至少需要两点需要改进&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;它不考虑超过1到2个单词的上下文&lt;/li&gt;
  &lt;li&gt;不考虑单词之间的“相似性”&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fighting-the-curse-of-dimensionality-with-distributed-representations&quot;&gt;Fighting the Curse of Dimensionality with Distributed Representations&lt;/h2&gt;

&lt;p&gt;论文提出方法思想总结如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;将vocabulary的每个单词关联到一个分布式单词特征上（$R^m$）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将词序列中的词的特征向量表示为词序列的联合概率函数&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;同时学习单词特征向量和联合概率函数的参数&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;特征向量的不同维度表示单词的不同方位，每个单词的都与向量空间中的一个点相关联，特征的维度m远小于词汇表大小，概率函数表述为给定前一个词的情况下下个词的条件概率的乘积&lt;/p&gt;

&lt;h2 id=&quot;a-neural-model&quot;&gt;A Neural Model&lt;/h2&gt;

&lt;p&gt;论文提出的模型结构如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-MiUNPZwHQJTWEwsPhe9%2Fuploads%2FhxjpdeHfXxC46KhUdDNy%2Fimage.png?alt=media&amp;amp;token=140d8d29-01a9-435b-bfe0-5ca432a8a846&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Neural architecture&lt;/p&gt;

&lt;p&gt;上图输入是$W_{t-n+1}、…、w_{t-2}、w_t$，本质上就是通过前面n-1个词预测第t个词，这也就是n-gram模型&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$C(w_{t-n+1})$：t-n+1单词对应的词向量，也是模型最终的收获；&lt;/li&gt;
  &lt;li&gt;C：矩阵，行是vocab，列是word对应的词向量$|V*m|$；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;模型的输入即把单词concat通过隐藏层，最后通过tanh激活，模型最后输出层公是如下：$y=b+Wx+Utanh(d+Hx)$，最后加的这一项是中间隐藏层的表示，其中x的表示如下：$x=(C(w_{t-1}),C(w_{t-2}),…,C(w_{t-n+1}))$。&lt;/p&gt;

&lt;p&gt;【论文第三部分提出两种训练加速方式，一种是内存共享，另一种是参数共享】&lt;/p&gt;

</description>
        <pubDate>Sun, 14 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/A-Neural-Probabilistic-Language-Model/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/A-Neural-Probabilistic-Language-Model/</guid>
        
        <category>论文</category>
        
        
      </item>
    
      <item>
        <title>文本分类</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;三步骤&quot;&gt;三步骤&lt;/h2&gt;

    &lt;h3 id=&quot;特征工程&quot;&gt;特征工程&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;常用的特征主要是词袋特征，复杂的特征词性标签、名词短语和树核&lt;/li&gt;
    &lt;/ol&gt;

    &lt;h3 id=&quot;特征选择&quot;&gt;特征选择&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;特征选择旨在删除噪音特征，常用的就是移除法&lt;/li&gt;
      &lt;li&gt;信息增益、L1正则&lt;/li&gt;
    &lt;/ol&gt;

    &lt;h3 id=&quot;分类器&quot;&gt;分类器&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;机器学习方面主要有逻辑回归、朴素贝叶斯、支持向量机 —– 数据稀疏性问题&lt;/li&gt;
      &lt;li&gt;深度学习和表征学习为解决数据稀疏问题&lt;/li&gt;
    &lt;/ol&gt;

    &lt;h2 id=&quot;rcnn&quot;&gt;RCNN&lt;/h2&gt;

    &lt;ol&gt;
      &lt;li&gt;在我们的模型中，当学习单词表示时，我们应用递归结构来尽可能多地捕获上下文信息，与传统的基于窗口的神经网络相比，这可以引入相当少的噪声。&lt;/li&gt;
      &lt;li&gt;还使用了一个最大池层，它自动判断哪些单词在文本分类中起关键作用，以捕获文本中的关键成分。&lt;/li&gt;
      &lt;li&gt;双向递归神经网络来捕捉上下文。&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/v2-198c6d37d18d1708f22fedd3043a7340_r.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$c_l(w_i)$用公式一计算，其中$c_l(w_{i-1})$是当前 word 的左侧的 context，$e(w_{i-1})$是单词的嵌入，稠密向量&lt;/li&gt;
    &lt;/ul&gt;

    &lt;ol&gt;
      &lt;li&gt;TextCNN比较类似，都是把文本表示为一个嵌入矩阵，再进行卷积操作。不同的是TextCNN中的文本嵌入矩阵每一行只是文本中一个词的向量表示，而在RCNN中，文本嵌入矩阵的每一行是当前词的词向量以及上下文嵌入表示的拼接&lt;/li&gt;
    &lt;/ol&gt;

    &lt;h2 id=&quot;han&quot;&gt;HAN&lt;/h2&gt;

    &lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/54165155&quot;&gt;论文解读&lt;/a&gt;&lt;/p&gt;

    &lt;h2 id=&quot;gcn&quot;&gt;GCN&lt;/h2&gt;

    &lt;p&gt;&lt;a href=&quot;https://lsvih.com/2019/06/27/Graph Convolutional Networks for Text Classification/&quot;&gt;论文解读&lt;/a&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;提出了一种新的文本分类图形神经网络方法。据我们所知，这是第一次将整个语料库建模为异构图，并利用图神经网络联合学习单词和文档嵌入的研究。&lt;/li&gt;
      &lt;li&gt;在几个基准数据集上的结果表明，我们的方法优于现有的文本分类方法，不使用预先训练的单词嵌入或外部知识。我们的方法还自动学习预测单词和文档嵌入。&lt;/li&gt;
    &lt;/ol&gt;

    &lt;div class=&quot;language-tex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;输入特征矩阵X=I：one-hot
图边计算方式
    word-document：tf-idf
    word-word：PMI-为了利用全局词共现信息，我们对语料库中的所有文档使用固定大小的滑动窗口来收集共现统计。
A&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;ij计算方式如下：
    PMI(i, j)； i, j are words, PMI(i, j) &amp;gt; 0
    TF-IDF&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;ij；  i is document, j is word
    1； i = j
    0； otherwise
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;不直接用$D^{-1}A$而选用$D^{-1/2}AD^{-1/2}$，是因为$D^{-1}A$的结果不是对称矩阵，这个大家动手算一下就知道了。虽然两者结果不相同，但是$D^{-1/2}AD^{-1/2}$已经做到了近似的归一化，而且保持了矩阵的对称性，我想这就是选用对称归一化的拉普拉斯矩阵的原因&lt;a href=&quot;https://blog.csdn.net/qq_35516657/article/details/108225441&quot;&gt;链接&lt;/a&gt;，属于拉普拉斯对称归一化&lt;a href=&quot;https://zhuanlan.zhihu.com/p/362416124&quot;&gt;链接&lt;/a&gt;。&lt;/p&gt;

    &lt;h2 id=&quot;损失函数&quot;&gt;损失函数&lt;/h2&gt;

    &lt;h3 id=&quot;交叉熵损失&quot;&gt;交叉熵损失&lt;/h3&gt;

    &lt;p&gt;KL距离常用来度量两个分布之间的距离，其具有如下形式其中p是真实分布，q是拟合分布，H(p)是p的熵，为常数。因此 度量了p和q之间的距离，叫做交叉熵损失。&lt;/p&gt;

    &lt;h2 id=&quot;优化方法&quot;&gt;优化方法&lt;/h2&gt;

    &lt;h3 id=&quot;数据优化&quot;&gt;数据优化&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;训练集合和测试集合部分特征抽取方式不一致&lt;/li&gt;
      &lt;li&gt;最后的结果过于依赖某一特征&lt;/li&gt;
    &lt;/ol&gt;

    &lt;ul&gt;
      &lt;li&gt;优化方法
        &lt;ul&gt;
          &lt;li&gt;在全连接层 增加dropout层，设置神经元随机失活的比例为0.3，即keep_rate= 0.7&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;在数据预处理的时候，随机去掉10%的A特征&lt;/li&gt;
    &lt;/ul&gt;

    &lt;ol&gt;
      &lt;li&gt;泛化能力较差&lt;/li&gt;
    &lt;/ol&gt;

    &lt;ul&gt;
      &lt;li&gt;优化方法
        &lt;ul&gt;
          &lt;li&gt;增加槽位抽取：针对部分query, 增加槽位抽取的处理，比如将英文统一用&lt;ENG&gt;表示，模型见到的都是一样的，不存在缺乏泛化能力的问题. 瓶颈在于槽位抽取的准确率。&lt;/ENG&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;ol&gt;
      &lt;li&gt;缺乏先验知识&lt;/li&gt;
    &lt;/ol&gt;

    &lt;h3 id=&quot;模型优化&quot;&gt;模型优化&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;embedding_dim长度&lt;/li&gt;
      &lt;li&gt;在全连接层增加dropout层&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 14 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>信息抽取-seq2seq_DGCNN</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;数据类型&quot;&gt;数据类型&lt;/h2&gt;

&lt;h3 id=&quot;原始数据类型&quot;&gt;原始数据类型&lt;/h3&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;九玄珠是在纵横中文网连载的一部小说，作者是龙马&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;spo_list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;九玄珠&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;连载网站&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;纵横中文网&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;九玄珠&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;作者&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;龙马&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;笔者实际任务&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2、负责oa系统建设、运营及维护&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;spo_list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;oa系统&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;建设&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;oa系统&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;运营&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;oa系统&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;维护&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2、策划并运营公司品牌及产品在线上的推广与管理&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;spo_list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;策划&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;公司品牌&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;运营&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;公司品牌&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;产品&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;线上&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;线上&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;推广&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;线上&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;链接&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;管理&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;数据输出就是三元组（s，p，o）的形式，s就是subject，即主实体，o是object，即客实体，p是predicate，实体关系，最终从主实体链接到客实体，客实体若还是另一三元组的主实体，则继续链接到客实体作为主实体的客实体，即s-&amp;gt;o(s)-&amp;gt;o。&lt;/p&gt;

&lt;h3 id=&quot;样本特点&quot;&gt;样本特点&lt;/h3&gt;

&lt;p&gt;观察数据发现“一对多”的抽取+分类任务，数据有如下特点&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;s和o的实体并不一定都能被切词工具切出来，因此存在切错边界的问题，所以我们采用字来做；&lt;/li&gt;
  &lt;li&gt;样本存在多种情况，有一个s对应多个o的情况，eg：线上推广与管理-&amp;gt;线上推广、线上管理，也存在多个s对应一个o的情况，eg：软件开发以及调试-&amp;gt;软件开发、软件调试；&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;模型设计&quot;&gt;模型设计&lt;/h2&gt;

&lt;p&gt;seq2seq解码器建模如下$P(y_1,y_2,…,y_n∣x)=P(y_1∣x)P(y_2∣x,y_1)…P(y_n∣x,y_1,y_2,…,y_{n−1})$&lt;/p&gt;

&lt;p&gt;实际预测的时候，先通过x来预测第一个单词，然后假设第一个词已知预测第二个单词，以此类推，知道结果标记出现。在三元组中参考此思路。&lt;/p&gt;

&lt;p&gt;$P(s,p,o)=P(s)P(o∣s)P(p∣o,s)P(s,p,o)$&lt;/p&gt;

&lt;p&gt;通过先预测s，然后传入s来预测s对应的o，在传入s，o来预测关系p，实际应用中，我们还可以把o，p的预测合并为一步。&lt;/p&gt;

&lt;p&gt;理论上，上述模型只能抽取单一一个三元组，为了处理多个情况，我们全部使用“半指针-半标注”结构，即将softmax换成sigmoid，在DGCNN中介绍。&lt;/p&gt;

&lt;div class=&quot;language-tex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# copy
注1：为什么不先预测o然后再预测s及对应的p？

那是因为引入在第二步预测的时候要采样传入第一步的结果（而且只采样一个），而前面已经分析了，
多数样本的o的数目比s的数目要多，所以我们先预测s，然后传入s再预测o、p的时候，对s的采样就
很容易充分了（因为s少），反过来如果要对o进行采样就不那么容易充分（因为o可能很多）。

带着这个问题继续读下去，读者会更清楚地认识到这一点。

注2：刷到最近的arxiv论文，发现在思想上，本文的这种抽取设计与文章《Entity-Relation 
Extraction as Multi-Turn Question Answering》类似。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;模型结构&quot;&gt;模型结构&lt;/h2&gt;

&lt;p&gt;模型结构示意图如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/信息抽取1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;模型处理流程如下:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;输入字id序列，然后通过字词混合Embedding得到对应的子向量序列，然后加上Position Embedding；&lt;/li&gt;
  &lt;li&gt;将得到的“字-词-位置Embedding”输入到12层DGCNN进行编码，得到编码后的序列（记为$H$）；&lt;/li&gt;
  &lt;li&gt;将$H$传入一层Self Attention后，将输出结果与先验特征进行拼接（先验可加可不加）；&lt;/li&gt;
  &lt;li&gt;将拼接后的结果传入CNN、Dense，用“半指针-半标注”结构预测s的首、尾位置；&lt;/li&gt;
  &lt;li&gt;训练时随机采样一个标注的s（预测时逐一遍历所有的s），然后将HHH对应此s的子序列传入到一个双向LSTM中，得到s的编码向量，然后加上相对位置的Position Embedding，得到一个与输入序列等长的向量序列；&lt;/li&gt;
  &lt;li&gt;将HHH传入另一层Self Attention后，将输出结果与第5步输出的向量序列、先验特征进行拼接（先验特征可加可不加，构建方式后面再详细介绍）；&lt;/li&gt;
  &lt;li&gt;将拼接后的结果传入CNN、Dense，对于每一种p，都构建一个“半指针-半标注”结构来预测对应的o的首、尾位置，这样就同时把o、p都预测出来了。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;问题一：为啥第5步只采样一个s？&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1、采样一个就够了，采样多个相当于等效增大batch size；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;2、采样一个比较好操作。&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;模型细节&quot;&gt;模型细节&lt;/h2&gt;

&lt;h3 id=&quot;字词混合embedding&quot;&gt;字词混合Embedding&lt;/h3&gt;

&lt;p&gt;单纯的字向量难以表达语义信息，而词向量有语音信息，但随之有很多切词问题需要解决，比如如何切准确；文章提出一种自行设计的字词混合方式，在多个任务中均取得了有效的提升，具体做法如下：&lt;/p&gt;

&lt;p&gt;首先以字为单位的文本序列，经过Embedding后得到字向量序列；然后将文本分词，通过预训练好的模型来提取对应的词向量，为了得到字向量对齐到词向量序列，我们可将每个词重复“词的字数”那么多次；得到对齐的词向量序列后，我们将词向量序列经过一个矩阵变换到跟字向量一样的维度，并将两者相加。流程图如下：&lt;/p&gt;

&lt;p&gt;字词混合Embedding方式图示&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/字词编码.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;实现上，文章使用pyhanlp作为分词工具，训练了一个Word2Vec模型（Skip Gram + 负采样），而字向量则使用随机初始化的字Embedding层，在模型训练过程中，固定Word2Vec词向量不变，只优化变换矩阵和字向量，从另一个角度看也可以认为是我们是&lt;strong&gt;通过字向量和变换矩阵对Word2Vec的词向量进行微调&lt;/strong&gt;。这样一来，我们既融合了预训练词向量模型所带来的&lt;strong&gt;先验语义信息，又保留了字向量的灵活性&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;position-embedding&quot;&gt;Position Embedding&lt;/h3&gt;

&lt;p&gt;模型主要使用CNN+Attention进行编码，所以编码出的向量位置信息并不敏感，信息抽取中，往往s在于开头，o在s附近，加入一个有效的位置编码信息是有用的；&lt;/p&gt;

&lt;p&gt;具体做法是设定一个最大长度为512，然后全零初始化一个新的Embedding层（维度和字向量一样），传入位置ID后输出对应的Position Embedding，并把这个Position Embedding加到前面的字词混合Embedding中，作为完整的Embedding结果，传入到下述DGCNN中。&lt;/p&gt;

&lt;p&gt;模型另一处用到了Position Embedding是在编码s的时候，采样得到的s经过BiLSTM进行编码后，得到一个固定大小的向量，然后我们将它复制拼接到原来的编码序列中，作为预测o、p的条件之一。不过考虑到o更可能是s附近的词，所以笔者并非直接统一复制，而是复制同时还加上了当前位置相对于s所谓位置的“相对位置向量”（如果对此描述还感觉模糊，请直接阅读源码），它跟开头的输入共用同一个Embedding层。&lt;/p&gt;

&lt;h3 id=&quot;dgcnn&quot;&gt;DGCNN&lt;/h3&gt;

&lt;p&gt;这部分接受另起一篇介绍&lt;/p&gt;

&lt;h2 id=&quot;知识蒸馏&quot;&gt;知识蒸馏&lt;/h2&gt;

&lt;p&gt;该部分内容是作者在实验阶段的处理手法，基于任务不同数据不同，笔者只单纯对知识蒸馏做一总结，在实际任务中，我们训练集往往有一定的缺陷以及不规范，因此可以使用类似知识蒸馏的方式来重新整理训练集，改善训练集质量。&lt;/p&gt;

&lt;p&gt;具体的，我们通过对原始训练集以交叉验证的方式，得到k个模型，然后利用k个模型对训练集进行预测，得到关于训练集的k份预测结果，如果某个样本同时在k份预测结果中但没有在训练集中标注，那么可以对此进行标注，同样的，某个样本在k份预测中都没有出现，但标注了，那么可以对此标注删除，这样以增一减之后训练集就会完善很多，这种方式也在我们实际各个任务中可以使用。&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;

&lt;p&gt;https://kexue.fm/archives/6671&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96-seq2seq_DGCNN)/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96-seq2seq_DGCNN)/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>seq2seq 综述</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;seq2seq是一种循环神经网络的变种，包括编码器和解码器两部分，可用于机器翻译、对话系统、自动摘要&lt;/p&gt;

&lt;h1 id=&quot;常见seq2seq结构&quot;&gt;常见seq2seq结构&lt;/h1&gt;

&lt;p&gt;seq2seq是一种重要的RNN模型，也称为Encoder-Decoder模型，可以理解为 N&lt;em&gt;M&lt;/em&gt;N&lt;em&gt;∗&lt;/em&gt;M* 的模型，模型包含两部分，Encoder部分编码序列的信息，将任意长度的信息编码到一个向量 c&lt;em&gt;c&lt;/em&gt; 里，而Decoder部分通过向量 c&lt;em&gt;c&lt;/em&gt; 将信息解码，并输出序列，常见的结构如下几种：&lt;/p&gt;

&lt;p&gt;第一种 seq2seq结构&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/seq2seq1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第二种 seq2seq结构&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/seq2seq2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第三种 seq2seq结构&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/seq2seq3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;encoder&quot;&gt;Encoder&lt;/h2&gt;

&lt;p&gt;· 这三种模型结构的主要区别在于Decoder上，Encoder是一样的，Encoder部分的RNN通过接受输入 $X$ ，通过RNN编码所有信息到$c$（所有神经元最后的输出，不要中间神经元的输出）。&lt;/p&gt;

&lt;h2 id=&quot;decoder&quot;&gt;Decoder&lt;/h2&gt;

&lt;p&gt;· 第一种Decoder，结构简单，将Encoder编码信息$c$作为Decoder解码隐层的初始输入，后续神经元接受上一神经元的隐层输出状态$h`$&lt;/p&gt;

&lt;p&gt;· 第二种Decoder，将Encoder编码信息 $c$作为Decoder解码隐层每个神经元的输入，而不在作为隐层的初始输入；&lt;/p&gt;

&lt;p&gt;·  第三种Decoder，再第二种的基础上，每个神经元的输入有了两部分，一部分是Encoder编码信息 $c$，另一部分是Decoder上个神经元的输出，两部分通过不同的权重矩阵来作为当前神经元的输入。&lt;/p&gt;

&lt;h1 id=&quot;使用trick&quot;&gt;使用trick&lt;/h1&gt;

&lt;h2 id=&quot;attention&quot;&gt;Attention&lt;/h2&gt;

&lt;p&gt;seq2seq模型中，Encoder总是将源句子的信息编码到一个固定长度的上下文 $c$ 中，然后Decoder的过程中 $c$总是不变的，这样就存在几个问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;当源句子较长时，很难用一个定长的向量 c&lt;em&gt;c&lt;/em&gt; 表示完所有的信息；&lt;/li&gt;
  &lt;li&gt;RNN存在梯度小时的问题，只使用最后一个神经元得到的向量 c&lt;em&gt;c&lt;/em&gt; 效果不理想；&lt;/li&gt;
  &lt;li&gt;与人类阅读时的注意力方式不同，人类总是把注意力放在当前句子上&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Attention（注意力机制），是一种将模型注意力放在当前翻译单词上的一种机制。例如翻译“I have a cat”，翻译我时注意力在”I“上，翻译猫时注意力在”cat“上。&lt;/p&gt;

&lt;p&gt;使用Attention后，Decoder的输入就不是固定的上下文向量 $c$了，而是根据当前翻译的信息，计算当前  $c$。&lt;/p&gt;

&lt;p&gt;Attention&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/attention.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Attention需要保留Encoder每一个神经元的隐藏层向量 $h$ ，然后Decoder的第$t$个神经元要根据上一个神经元的隐藏层向量 $h`_{t-1}$计算当前状态与Encoder每一个神经元的相关性 $e_t$。 $e_t$是一个N维的向量（Encoder神经元个数维N），若$e_t$第$i$维越大，则说明当前节点与Encoder第$i$个神经元的相关性越大。$e_t$的计算方式有很多，即相关性系数的计算函数$a$有很多种：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/attention_scal.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面得到相关性向量 $e_t$后，需要进行归一化，使用 softmax 归一化。然后用归一化后的系数融合 Encoder 的多个隐藏层向量得到 Decoder 当前神经元的上下文向量 $c_t$：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/attention_scal_softmax.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Attention 计算上下文$c$&lt;/p&gt;

&lt;p&gt;总体流程图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/attention_all.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;teacher-forcing&quot;&gt;Teacher Forcing&lt;/h2&gt;

&lt;p&gt;Teacher Forcing用于训练阶段，主要针对上面第三种Decoder模型来说，第三种Decoder模型输入包含了上一时刻的输出神经元 $y’$，这样就会有个问题，如果上一时刻$y’$是错误的，则后面的神经元也会很容易错误，导致错误传递下去。&lt;/p&gt;

&lt;p&gt;Teacher Forcing在一定程度上可以解决这个问题，在训练阶段，Decoder模型每个神经元接受的输入是正确的label，而并未上一时刻神经元的输出，对比如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/teach_1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;不适用 Teacher Forcing&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/自然语言处理/teach_2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;使用Teacher Forcing&lt;/p&gt;

&lt;h2 id=&quot;beam-search&quot;&gt;Beam Search&lt;/h2&gt;

&lt;p&gt;Beam Search 方法不用于训练的过程，而是用在测试的。在每一个神经元中，我们都选取当前输出概率值最大的 &lt;strong&gt;top k&lt;/strong&gt; 个输出传递到下一个神经元。下一个神经元分别用这 k 个输出，计算出 L 个单词的概率 (L 为词汇表大小)，然后在 kL 个结果中得到 &lt;strong&gt;top k&lt;/strong&gt; 个最大的输出，重复这一步骤。&lt;/p&gt;

&lt;h1 id=&quot;参考&quot;&gt;参考&lt;/h1&gt;

&lt;p&gt;https://www.jianshu.com/p/80436483b13b&lt;/p&gt;

&lt;p&gt;https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/seq2seq/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/seq2seq/</guid>
        
        <category>自然语言处理</category>
        
        
      </item>
    
      <item>
        <title>百问机器学习</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;一特征工程&quot;&gt;一、特征工程&lt;/h2&gt;

&lt;p&gt;巧妇难为无米之炊&lt;/p&gt;

&lt;h3 id=&quot;1为什么归一化&quot;&gt;1、为什么归一化&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;归一化能够更快的找到迭代优化的方向，因此在实际中通过梯度下降求解的模型通常是需要归一化，eg：线性回归、逻辑回归、支持向量机、神经网络等，决策模型则不适用，eg：C4.5，决策树节点分裂主要依靠数据集D关于特征x的信息增益比，信息增益比与特征是否经过归一化是无关的。&lt;/li&gt;
  &lt;li&gt;常用的归一化方法有线性函数归一化（最大最小值归一化），零均值归一化$z=\frac{x-\mu}{标准差}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2类别特征数据预处理&quot;&gt;2、类别特征数据预处理&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;类别特征主要指性别（男女）、血腥（A、B、AB、O）等&lt;/li&gt;
  &lt;li&gt;主要方法：序号编码、独热编码、二进制编码&lt;/li&gt;
  &lt;li&gt;序号编码：分桶，成绩高中低&lt;/li&gt;
  &lt;li&gt;独热编码：ont-hot编码
    &lt;ul&gt;
      &lt;li&gt;稀疏，可以用稀疏向量来节省空间&lt;/li&gt;
      &lt;li&gt;配合特征选择来降低维度。高维度特征会带来几方面的问题。一是在K 近邻算法中，高维空间下两点之间的距离很难得到有效的衡量；二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;二进制编码：类别特征的个数最少用几位的二级制可以表示，转换成对应的二级制编码&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3组合特征如何处理高维组合特征&quot;&gt;3、组合特征？如何处理高维组合特征？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;组合特征就是对当前的特征两两（nn）组合成为一个新的特征&lt;/li&gt;
  &lt;li&gt;高维组合特征不一定有意义，并且有可能使得参数变多造成过拟合，因此可以通过矩阵分解的方式进行降维&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;4怎么有效的找到组合特征&quot;&gt;4、怎么有效的找到组合特征&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;利用决策树，决策树从根节点到叶子节点，路径上的组合特征即可以看成一种路径结合方式&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;5有那些文本表示模型他们各有什么优缺点&quot;&gt;5、有那些文本表示模型？他们各有什么优缺点？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;词袋模型和N-gram模型&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;词袋模型就是将一段文本看成一个袋子，忽略了每个词出现的顺序，一般采用TF-IDF计算权重 TF−IDF(t,d)=TF(t,d)∗IDF(t)TF-IDF(t,d) = TF(t,d)*IDF(t)TF−IDF(t,d)=TF(t,d)∗IDF(t)&lt;/p&gt;

        &lt;p&gt;其中TF(t,d)为单词t再文档d中出现的频率，IDF(t)是逆文档频率，衡量单词t对表达语义所起的重要性，表示如下 IDF(t)=log文章总数包含单词t的文章总数+1IDF(t) = log^\frac{文章总数}{包含单词t的文章总数+1}IDF(t)=log包含单词t的文章总数+1文章总数&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;缺点：natural language processing（自然语言处理），分词后三个词表达的含义与之大相径庭。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;主题模型&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;词嵌入和深度学习模型&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;词嵌入是一种词向量化的统称，核心思想就是将每个映射到低维空间K上的一个稠密向量，K上每一维可以看作是一个隐含的主题，只不过不像主题模型那么直观。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;6word2vec是如何工作的它和lda有什么区别和联系&quot;&gt;6、Word2Vec是如何工作的？它和LDA有什么区别和联系？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Word2Vec 见&lt;a href=&quot;https://www.cnblogs.com/monkeyT/p/12217490.html&quot;&gt;鄙人博客&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;不同
    &lt;ul&gt;
      &lt;li&gt;LDA是利用文档中单词的共现关系来对单词按主题聚类，可以理解为对“文档－单词”进行矩阵分解，得到“文档－主题”和“主题－单词”两个概率分布；而Word2Vec 是利用“文档-单词”矩阵学习上下文，因此Word2Vec融入了很多的上下文信息，因此两个词对应的词向量就比较相似了。&lt;/li&gt;
      &lt;li&gt;最大的不同在于两者之间的模型本身，主题模型是一种基于概率图模型的生成式模型，而Word2Vec则利用神经网络最后映射到稠密向量。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;7图像任务中训练数据不足会带来什么问题如何缓解数据量不足带来的问题&quot;&gt;7、图像任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;模型所能提供的信息一般来于两个方面，一个是训练数据中蕴含的信息；二十模型在形成过程中（包括构造、学习、推理等）人们提供的先验信息。&lt;/li&gt;
  &lt;li&gt;训练数据不足容易导致过拟合，从两方面进行改进，
    &lt;ul&gt;
      &lt;li&gt;一方面简化模型，比如从非线性模型改为线性模型，或者添加约束条件（L1、L2）dropout等，&lt;/li&gt;
      &lt;li&gt;另一方面就是增加数据，图像数据增加数据技术相对成熟，旋转、偏移、高斯噪声、颜色对比亮度等变换；自然语言处理中可以通过随机删除、替换以及利用深度模型等进行数据扩充；迁移学习等&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;二模型评估&quot;&gt;二、模型评估&lt;/h2&gt;

&lt;p&gt;没有测量就没有科学&lt;/p&gt;

&lt;h3 id=&quot;8准确率的局限性&quot;&gt;8、准确率的局限性&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;首先从数据出发，数据是否存在样本严重不平衡，在此数据下模型的准确率就不能衡量模型整体的指标，可以考虑采用平均准确率&lt;/li&gt;
  &lt;li&gt;其次考虑模型是否存在过拟合或者欠拟合，这时候模型的准确率就不能代替实际效果&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;9精确率和召回率的权衡&quot;&gt;9、精确率和召回率的权衡&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;精确度是识别的个数中正确的占比，召回率是识别正确的占应该识别正确的总数的占比，如果只看精确度不看召回，存在线上搜索结果搜索显示不全，同样的只看召回存在召回准确性的问题&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;通常用 f1-score 来对两者进行权衡，即两者的调和平均值，计算如下$f1 = \frac{2&lt;em&gt;precision&lt;/em&gt;recall}{precision+recall}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;10平方根误差的意外&quot;&gt;10、平方根误差的“意外”&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;平方根误差一般用来衡量回归问题，计算公式如下$RMSE=\sqrt{\frac{\sum_{i=1}^n(y_i-\hat y_i)^2}{n}}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从公式可以看出，RMSE可以衡量y的真实值和预测值之间的差距，但如果样本中存在离群点时，即使实际准确性较高，但是RMSE可能会存在较大的情况，这时候平方根误差就出现了意外，在实际中考虑两方面，一方面这些离群点是否是噪声，那么在数据处理阶段就应该踢除，另一方面如果不是噪声，那么模型应该在拟合的时候就考虑这些离群点。第一种情况时可以考虑换其他误差函数衡量，例如MPAE（平均绝对百分比误差），计算公式如下 $MPAE = \sum_{i=1}^n|\frac{y_i-\hat y_i}{y_i}|*\frac{100}{n}$&lt;/p&gt;

&lt;p&gt;相比RMSE，MPAE相当于把每个点的误差都进行了归一化，这样就降低了离群点带来的绝对误差的影响；第二种情况就是一个很大的场景了，不展开叙述&lt;/p&gt;

&lt;h3 id=&quot;11什么是roc曲线&quot;&gt;11、什么是ROC曲线&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ROC曲线全称受试者工作曲线，横坐标为假阳性率（FPR），纵坐标为真阳性率（TPR），计算公式如下 $FRP=\frac{FP}{N},TPR=\frac{TP}{P}$&lt;/p&gt;

    &lt;p&gt;简单点说FPR就是所有负类中识别为正类的占比，TPR就是所有正类中识别为正类的占比&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;12如何绘制roc曲线&quot;&gt;12、如何绘制ROC曲线&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;样本序号&lt;/th&gt;
      &lt;th&gt;真实标签&lt;/th&gt;
      &lt;th&gt;模型输出概率&lt;/th&gt;
      &lt;th&gt;TPR&lt;/th&gt;
      &lt;th&gt;FPR&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;1/10&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;2/10&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;2/10&lt;/td&gt;
      &lt;td&gt;1/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.6&lt;/td&gt;
      &lt;td&gt;3/10&lt;/td&gt;
      &lt;td&gt;1/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.55&lt;/td&gt;
      &lt;td&gt;4/10&lt;/td&gt;
      &lt;td&gt;1/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.54&lt;/td&gt;
      &lt;td&gt;5/10&lt;/td&gt;
      &lt;td&gt;1/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;0.53&lt;/td&gt;
      &lt;td&gt;5/10&lt;/td&gt;
      &lt;td&gt;2/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;0.52&lt;/td&gt;
      &lt;td&gt;5/10&lt;/td&gt;
      &lt;td&gt;3/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.51&lt;/td&gt;
      &lt;td&gt;6/10&lt;/td&gt;
      &lt;td&gt;3/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;0.505&lt;/td&gt;
      &lt;td&gt;6/10&lt;/td&gt;
      &lt;td&gt;4/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
      &lt;td&gt;7/10&lt;/td&gt;
      &lt;td&gt;4/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;0.39&lt;/td&gt;
      &lt;td&gt;7/10&lt;/td&gt;
      &lt;td&gt;5/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.38&lt;/td&gt;
      &lt;td&gt;8/10&lt;/td&gt;
      &lt;td&gt;5/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;0.37&lt;/td&gt;
      &lt;td&gt;8/10&lt;/td&gt;
      &lt;td&gt;6/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;0.36&lt;/td&gt;
      &lt;td&gt;8/10&lt;/td&gt;
      &lt;td&gt;7/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;0.35&lt;/td&gt;
      &lt;td&gt;8/10&lt;/td&gt;
      &lt;td&gt;8/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.34&lt;/td&gt;
      &lt;td&gt;9/10&lt;/td&gt;
      &lt;td&gt;8/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;0.33&lt;/td&gt;
      &lt;td&gt;9/10&lt;/td&gt;
      &lt;td&gt;9/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt;0.30&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;9/10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;0.1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;13如何计算auc&quot;&gt;13、如何计算AUC&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;AUC是ROC曲线下方面积的大小，一般对横轴做积分即可，AUC的取值范围一般在[0.5-1]之间，原因是ROC曲线总在y=x之上，如果在下方，则把p换成1-p即可以得到一个更好的分类器&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;14roc曲线相比p-r曲线有什么特点&quot;&gt;14、ROC曲线相比P-R曲线有什么特点&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ROC曲线有一个特点是当正负样本的分布发生变化（正负样本比例1/1000–&amp;gt; 1/10000）时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈的变化，这就是的ROC曲线能尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;15结合你的学习和研究经历探讨为什么在一些场景中要使用余弦相似度而不-是欧氏距离&quot;&gt;15、结合你的学习和研究经历，探讨为什么在一些场景中要使用余弦相似度而不 是欧氏距离？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;总体来说，欧氏距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。
    &lt;ul&gt;
      &lt;li&gt;1）例如，统计两部剧的用户观看行为，用户A的观看向量为(0,1)，用户B为(1,0)；此时二者的余弦距很大，而欧氏距离很小；我们分析两个用户对于不同视频的偏好，更关注相对差异，显然应当使用余弦距离。&lt;/li&gt;
      &lt;li&gt;2）而当我们分析用户活跃度，以登陆次数(单位：次)和平均观看时长(单：分钟)作为特征时，余弦距离会认为(1,10)、(10,100)两个用户距离很近；但显然这两个用户活跃度是有着极大差异的，此时我们更关注数值绝对差异，应当使用欧氏距离。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;余弦相似度的取值范围是[−1,1]， 相同的两个向量之间的相似度为1。如果希望得到类似于距离的表示，将1减去余 弦相似度即为余弦距离。因此，余弦距离的取值范围为[0,2]，相同的两个向量余 弦距离为0，空间余弦角度在0-90°，余弦相似度在1-0，余弦距离在0-1，因此我们在使用词向量计算余弦距离时也在0-1，越小越相似&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;16在对模型进行过充分的离线评估之后为什么还要进行在线ab测试&quot;&gt;16、在对模型进行过充分的离线评估之后，为什么还要进行在线A/B测试？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;离线评估无法完全消除模型过拟合的影响，所以离线评估无法代替线上评估&lt;/li&gt;
  &lt;li&gt;离线评估无法完全还原线上工程环境&lt;/li&gt;
  &lt;li&gt;线上系统的某些商业指标在离线评估中无法计算，eg：点击率等&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;17如何进行线上-ab-测试&quot;&gt;17、如何进行线上 A/B 测试？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A/B测试主要手段是通过对用户分桶，即将用户分成实验组和对照组，对实验组用新模型，对照组用旧模型，在分桶中注意样本独立性和采样方式的无偏性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;18如何划分实验组和对照组&quot;&gt;18、如何划分实验组和对照组&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;假设我们系统中对A国的用户替换了推荐模型，再上线时希望对比实验组和对照组的结果，那么如何划分实验组和对照组；由于我们是对A国的用户推荐替换模型，因此我们实验组和对照组都应该是A国用户，并且分的时候要注意独立和无偏，而不能混进其他类型的用户进行对照，一方面会稀释我们的结果，另一方面也有可能会导致我们对比不准确&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;19在模型评估过程中有哪些主要的验证方法它们的优缺点是什么&quot;&gt;19、在模型评估过程中，有哪些主要的验证方法，它们的优缺点是什么?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;holdout方法：模型数据按照70%-30%分为训练集和测试集，70%的数据用来训练，30%的数据用来测试，这就存在一个问题，当数据切分不均匀时，会导致测试集评估结果和训练集存在和大的gap，因此提出下面的方法&lt;/li&gt;
  &lt;li&gt;k-fold交叉验证：数据集分为k份，每次留一份进行测试，其他用来训练，最后k次测试的平均结果作为模型结果，缺点也很明显，时间增加很多，当数据很大时会很耗费时间，当数据较少时有不适用，因此提出下面的方法&lt;/li&gt;
  &lt;li&gt;自助法：当数据较少时我们对数据（n）进行又放回的抽样(n)次，然后用没抽样到的样本进行测试&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;20在自助法的采样过程中对n个样本进行n次自助抽样当n趋于无穷大时-最终有多少数据从未被选择过&quot;&gt;20、在自助法的采样过程中，对n个样本进行n次自助抽样，当n趋于无穷大时， 最终有多少数据从未被选择过？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;1/e，因此样本很大时，大约有36.8%的样本从未被选择过，可作为验证集&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;21超参数有那些调优方法&quot;&gt;21、超参数有那些调优方法？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;网格搜索
    &lt;ul&gt;
      &lt;li&gt;通过搜索查找范围所有点来确定最优点，这里所有点其实是以较小的步长进行搜索，问题是非常消耗计算资源和时间，因此再使用的时候开始会以较大的步长进行搜索，找到最优点再以小步幅进行搜索，但一般目标函数是非凸优化问题，所以可能会错过全局最优值&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;随机搜索
    &lt;ul&gt;
      &lt;li&gt;在搜索范围内随机搜索点，理论上只要样本点足够多，那么通过随机采样就能找到最优点，随机搜索一般比网格搜索快一些，但是也没法保证得到最优点&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;贝叶斯优化算法
    &lt;ul&gt;
      &lt;li&gt;贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全 不同的方法。网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息； 而贝叶斯优化算法则充分利用了之前的信息。贝叶斯优化算法通过对目标函数形 状进行学习，找到使目标函数向全局最优值提升的参数。具体来说，它学习目标 函数形状的方法是，首先根据先验分布，假设一个搜集函数；然后，每一次使用 新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布；最 后，算法测试由后验分布给出的全局最值最可能出现的位置的点。对于贝叶斯优 化算法，有一个需要注意的地方，一旦找到了一个局部最优值，它会在该区域不 断采样，所以很容易陷入局部最优值。为了弥补这个缺陷，贝叶斯优化算法会在 探索和利用之间找到一个平衡点，“探索”就是在还未取样的区域获取采样点； 而“利用”则是根据后验分布在最可能出现全局最值的区域进行采样。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;22在模型评估过程中过拟合和欠拟合具体是指什么现象&quot;&gt;22、在模型评估过程中，过拟合和欠拟合具体是指什么现象？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;过拟合
    &lt;ul&gt;
      &lt;li&gt;模型在训练集上表现良好，测试集上表现很差&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;欠拟合
    &lt;ul&gt;
      &lt;li&gt;模型在训练集和测试集上表现都不好&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;23能否说出几种降低过拟合和欠拟合风险的方法&quot;&gt;23、能否说出几种降低过拟合和欠拟合风险的方法？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;过拟合直观解释，再对训练数据上进行拟合时，需要照顾每个点，从而导致拟合函数波动大，即方差大。&lt;/li&gt;
  &lt;li&gt;误差图判断过拟合还是欠拟合：&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;模型再训练集与测试集上误差均很大，则说明模型bias很大，欠拟合&lt;/p&gt;

  &lt;p&gt;训练集和测试集误差之间有很大的Gap，则说明Variance很大，过拟合&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;欠拟合解决方法&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;增加新的特征，考虑加入组合特征、高次特征，来增大假设空间&lt;/li&gt;
    &lt;li&gt;尝试非线性模型，比如SVM、决策树、DNN等模型&lt;/li&gt;
    &lt;li&gt;有正则项则尝试降低正则项参数\lambda&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;过拟合解决方法&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;交叉验证，通过交叉验证得到最优的模型&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;特征选择，减少特征数或者使用较少的特征组合，对于区间化离散特征，增大划分的空间&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;正则化，常用的有L_1,L_2正则，而且L_1正则还可以自动进行特征选择&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;如果有正则项考虑增大正则项参数\lambda&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;增加训练数据可以有效的避免过拟合&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Bagging，将多个弱学习器Bagging一下效果会好很多，比如随机森林&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;DNN中常用的方法&lt;/p&gt;

      &lt;ul&gt;
        &lt;li&gt;
          &lt;p&gt;早停。本质还是交叉验证策略，选择合适的训练次数，避免训练的网络过度拟合训练数据&lt;/p&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;集成学习策略。利用Bagging思路来正则化，首先对原始的m个训练样本进行又放回的随机采样，构建N组m个样本的数据集，然后分别用N组数据训练DNN，即得到N个DNN模型的ｗ，ｂ参数组合，最后对N个DNN模型的输出用加权平均法或者投票法决定最终输出。缺点就是参数增加了N倍，导致训练花费更多的时间和空间，因此N一般不能太多，例如5-10个。&lt;/p&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;Dropout策略。&lt;/p&gt;

          &lt;blockquote&gt;
            &lt;p&gt;训练的时候一一定的概率p让神经元失活，类似于Bagging策略，测试时所有神经元都参与，但是在其输出上乘以1-p.&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;simpler model structure（简化模型）&lt;/p&gt;

          &lt;blockquote&gt;
            &lt;p&gt;简单模型拟合复杂数据时，导致模型很难拟合数据的真实分布，这边是模型欠拟合了，同样的，复杂模型拟合简单数据，会导致数据的噪声也被拟合了，导致模型再训练集上效果非常好，但泛化性能很差。&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;regularization（正则化）&lt;/p&gt;

          &lt;ul&gt;
            &lt;li&gt;
              &lt;p&gt;$L_1$会产生稀疏解，&lt;/p&gt;
            &lt;/li&gt;
            &lt;li&gt;$L_2$正则会产生较小的解，&lt;/li&gt;
            &lt;li&gt;假设均误差损失函数为$J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}||a^L-y||_2^2$;&lt;/li&gt;
            &lt;li&gt;$L_2$通常只针对稀疏矩阵w，而不针对偏置系数b，则损失函数为$J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}||a^L-y||&lt;em&gt;2^2 + \frac{\lambda}{2m}\sum&lt;/em&gt;{l=2}^L|w|_2^2$&lt;/li&gt;
            &lt;li&gt;反向传播从:$w^l =w^l −\alpha\sum_{i=1}^m\sigma^{i,l}(\alpha^{i,l-1})^T$变为$w^l =w^l −\alpha\sum_{i=1}^m\sigma^{i,l}(\alpha^{i,l-1})^T-\alpha\lambda w^l$&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;data augmentation(数据集扩增)&lt;/p&gt;

      &lt;blockquote&gt;
        &lt;p&gt;有效前提是训练集与将来的数据是&lt;strong&gt;独立同分布&lt;/strong&gt;的,或者近似独立同分布&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;从数据源头采集更多的数据&lt;/li&gt;
          &lt;li&gt;复制原有数据并加上随机噪声&lt;/li&gt;
          &lt;li&gt;重采样&lt;/li&gt;
          &lt;li&gt;根据当前数据估计数据分布参数,使用该分布产生更多数据&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/blockquote&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Bootstrap/Bagging（封装）&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;ensemble（集成）&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;early stopping（提前终止训练）&lt;/p&gt;

      &lt;blockquote&gt;
        &lt;p&gt;迭代次数截断的方法来防止过拟合,再模型对训练数据集迭代收敛之前停止迭代防止过拟合.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;utilize invariance（利用不变性）&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Bayesian（贝叶斯方法）&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;三经典算法&quot;&gt;三、经典算法&lt;/h2&gt;

&lt;h3 id=&quot;24在空间上线性可分的两类点分别向svm分类的超平面上做投影这些点在-超平面上的投影仍然是线性可分的吗&quot;&gt;24、在空间上线性可分的两类点，分别向SVM分类的超平面上做投影，这些点在 超平面上的投影仍然是线性可分的吗？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;对于任意线性可分的两组点，它们在SVM分类的超平面上的投影 都是线性不可分的&lt;/li&gt;
  &lt;li&gt;该问题也可以通过凸优化理论中的超平面分离定理（Separating Hyperplane Theorem，SHT）更加轻巧地解决。该定理描述的是，对于不相交的两 个凸集，存在一个超平面，将两个凸集分离。对于二维的情况，两个凸集间距离 最短两点连线的中垂线就是一个将它们分离的超平面。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;25是否存在一组参数使svm训练误差为0&quot;&gt;25、是否存在一组参数使SVM训练误差为0？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;当所有样本都预测正确，训练误差即为0&lt;/li&gt;
  &lt;li&gt;补充：
    &lt;ul&gt;
      &lt;li&gt;svm有三种，线性可分SVM（硬间隔），线性SVM（软间隔），非线性SVM（核技巧）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 14 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/%E7%99%BE%E9%97%AE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/%E7%99%BE%E9%97%AE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>机器学习过拟合欠拟合</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;1、当遇到过拟合问题，当优化开发集和训练集的差异时，面临整体误差上升，有什么好的解决办法&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;过拟合，数据简单，模型复杂，开发训练分布不一致，优化数据，整体误差上升，欠拟合，模型不能拟合数据，第一：模型不适用这个数据，第二：数据量不够或者数据特征不够&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2、拟牛顿法比牛顿法的优点是什么&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;牛顿法是迭代算法，每一步都需求解目标函数的海塞矩阵（Hessian Matrix），计算比较复杂。拟牛顿法通过正定矩阵近似海塞矩阵的逆矩阵或海塞矩阵，简化了这一计算过程。&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://www.zhihu.com/equation?tex=x_{k%2B1}%3Dx_{k}-\lambda_k+H^{-1}_{k}g_k\\&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;img src=&quot;/images/posts/机器学习/v2-dac11d82ecb2566f54ce8b518d51293c_b.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;牛顿法虽然收敛速度快，但是需要计算海塞矩阵的逆矩阵 &lt;img src=&quot;https://www.zhihu.com/equation?tex=H%5E%7B-1%7D&quot; alt=&quot;img&quot; /&gt; ，而且有时目标函数的海塞矩阵无法保持正定，从而使得牛顿法失效。为了克服这两个问题，人们提出了拟牛顿法。这个方法的基本思想是：不用二阶偏导数而构造出可以近似海塞矩阵（或海塞矩阵的逆）的正定对称阵。不同的构造方法就产生了不同的拟牛顿法（&lt;strong&gt;DFP&lt;/strong&gt;/&lt;strong&gt;BFGS算法&lt;/strong&gt;）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;牛顿法是二阶收敛，梯度下降法是一阶收敛，所以牛顿法更快，下图形象化地显示了这一点：&lt;/p&gt;

    &lt;p&gt;其中，红色路径代表牛顿法，绿色路径代表梯度下降法。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;牛顿法和深度学习&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;深度学习中，往往采用梯度下降法作为优化算子，而很少采用牛顿法，主要原因有以下几点：&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;1.神经网络通常是非凸的，这种情况下，牛顿法的收敛性难以保证；&lt;/li&gt;
      &lt;li&gt;2.即使是凸优化，只有在迭代点离全局最优很近时，牛顿法才会体现出收敛快的优势；&lt;/li&gt;
      &lt;li&gt;3.可能被鞍点吸引。(鞍点：不是局部极值点的驻点)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3、推荐中如何模拟人的期望与偏好，是基于学术理论或研究建立一个用户偏好模型吗？标签怎么弄？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;是的&lt;/li&gt;
  &lt;li&gt;假设对一个交友活动网站来说，对于一个用户，有基础特征，行为特征等，其基础特征会包含性别、年龄、身高、体重等基础信息，除此之外我们假设一个用户老是搜索电影，那么当前用户的偏好就可以打上爱好：电影，而另一个用户喜欢搜索文学，那么可以打上爱好：读书，或者利用一些组合特征打上其他标签，比如性格（读书，女）安静，（篮球，男）活泼，那么我们再推荐一些线下读书交流会给性格安静的，而推荐一些团体运动会给活泼的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4、某项任务上机器已经超越人类，那么当前任务上人类存在的价值是？ 5、短文本分类的方法？（材料无关）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;短文本分类首要问题需要标签，
    &lt;ul&gt;
      &lt;li&gt;可以先通过聚类（LDA），相同簇的分类比较接近，标注起来比较方便；其次对应较少的类，也可以抽取到给人工标注&lt;/li&gt;
      &lt;li&gt;基于关键词标注，假设要给新闻标题分类，那么对于“体育”类，“足球”、“篮球”、“NBA”等就可以作为这个类的关键词&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;朴素贝叶斯、逻辑回归、支持向量机、GBDT、随机森林&lt;/li&gt;
  &lt;li&gt;深度学习（FastText，text-cnn，bert，图模型）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;6、近似搜索算法如何避免陷入局部最优？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;beam search&lt;/li&gt;
  &lt;li&gt;遗传算法
    &lt;ul&gt;
      &lt;li&gt;模拟物竞天择的生物进化过程，通过维护一个潜在解的群体执行了多方向的搜索，并支持这些方向上的信息构成和交换。是以面为单位的搜索，比以点为单位的搜索，更能发现全局最优解。（在遗传算法中，有很多袋鼠，它们降落到喜玛拉雅山脉的任意地方。这些袋鼠并不知道它们的任务是寻找珠穆朗玛峰。但每过几年，就在一些海拔高度较低的地方射杀一些袋鼠，并希望存活下来的袋鼠是多产的，在它们所处的地方生儿育女。）（或者换个说法。从前，有一大群袋鼠，它们被莫名其妙的零散地遗弃于喜马拉雅山脉。于是只好在那里艰苦的生活。海拔低的地方弥漫着一种无色无味的毒气，海拔越高毒气越稀薄。可是可怜的袋鼠们对此全然不觉，还是习惯于活蹦乱跳。于是，不断有袋鼠死于海拔较低的地方，而越是在海拔高的袋鼠越是能活得更久，也越有机会生儿育女。就这样经过许多年，这些袋鼠们竟然都不自觉地聚拢到了一个个的山峰上，可是在所有的袋鼠中，只有聚拢到珠穆朗玛峰的袋鼠被带回了美丽的澳洲。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;退火算法
    &lt;ul&gt;
      &lt;li&gt;这个方法来自金属热加工过程的启发。在金属热加工过程中，当金属的温度超过它的熔点（Melting Point）时，原子就会激烈地随机运动。与所有的其它的物理系统相类似，原子的这种运动趋向于寻找其能量的极小状态。在这个能量的变迁过程中，开始时，温度非常高， 使得原子具有很高的能量。随着温度不断降低，金属逐渐冷却，金属中的原子的能量就越来越小，最后达到所有可能的最低点。利用模拟退火的时候，让算法从较大的跳跃开始，使到它有足够的“能量”逃离可能“路过”的局部最优解而不至于限制在其中，当它停在全局最优解附近的时候，逐渐的减小跳跃量，以便使其“落脚 ”到全局最优解上。（在模拟退火中，袋鼠喝醉了，而且随机地大跳跃了很长时间。运气好的话，它从一个山峰跳过山谷，到了另外一个更高的山峰上。但最后，它渐渐清醒了并朝着它所在的峰顶跳去。）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;进化策略&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;7、降低模型过拟合和欠拟台风险的方法?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;过拟合直观解释，再对训练数据上进行拟合时，需要照顾每个点，从而导致拟合函数波动大，即方差大。&lt;/li&gt;
  &lt;li&gt;误差图判断过拟合还是欠拟合：&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;模型再训练集与测试集上误差均很大，则说明模型bias很大，欠拟合&lt;/p&gt;

  &lt;p&gt;训练集和测试集误差之间有很大的Gap，则说明Variance很大，过拟合&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;欠拟合解决方法&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;增加新的特征，考虑加入组合特征、高次特征，来增大假设空间&lt;/li&gt;
    &lt;li&gt;尝试非线性模型，比如SVM、决策树、DNN等模型&lt;/li&gt;
    &lt;li&gt;有正则项则尝试降低正则项参数\lambda&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;过拟合解决方法&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;交叉验证，通过交叉验证得到最优的模型&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;特征选择，减少特征数或者使用较少的特征组合，对于区间化离散特征，增大划分的空间&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;正则化，常用的有L_1,L_2正则，而且L_1正则还可以自动进行特征选择&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;如果有正则项考虑增大正则项参数\lambda&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;增加训练数据可以有效的避免过拟合&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Bagging，将多个弱学习器Bagging一下效果会好很多，比如随机森林&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;DNN中常用的方法&lt;/p&gt;

      &lt;ul&gt;
        &lt;li&gt;
          &lt;p&gt;早停。本质还是交叉验证策略，选择合适的训练次数，避免训练的网络过度拟合训练数据&lt;/p&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;集成学习策略。利用Bagging思路来正则化，首先对原始的m个训练样本进行又放回的随机采样，构建N组m个样本的数据集，然后分别用N组数据训练DNN，即得到N个DNN模型的ｗ，ｂ参数组合，最后对N个DNN模型的输出用加权平均法或者投票法决定最终输出。缺点就是参数增加了N倍，导致训练花费更多的时间和空间，因此N一般不能太多，例如5-10个。&lt;/p&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;Dropout策略。&lt;/p&gt;

          &lt;blockquote&gt;
            &lt;p&gt;训练的时候一一定的概率p让神经元失活，类似于Bagging策略，测试时所有神经元都参与，但是在其输出上乘以1-p.&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;simpler model structure（简化模型）&lt;/p&gt;

          &lt;blockquote&gt;
            &lt;p&gt;简单模型拟合复杂数据时，导致模型很难拟合数据的真实分布，这边是模型欠拟合了，同样的，复杂模型拟合简单数据，会导致数据的噪声也被拟合了，导致模型再训练集上效果非常好，但泛化性能很差。&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/li&gt;
        &lt;li&gt;
          &lt;p&gt;regularization（正则化）&lt;/p&gt;

          &lt;blockquote&gt;
            &lt;p&gt;L_1 正则项会产生稀疏解&lt;/p&gt;

            &lt;p&gt;L_2正则项会产生比较小的解&lt;/p&gt;

            &lt;p&gt;假设均方差误差损失函数&lt;/p&gt;

            &lt;table&gt;
              &lt;tbody&gt;
                &lt;tr&gt;
                  &lt;td&gt;J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}&lt;/td&gt;
                  &lt;td&gt; &lt;/td&gt;
                  &lt;td&gt;a^L-y&lt;/td&gt;
                  &lt;td&gt; &lt;/td&gt;
                  &lt;td&gt;_2^2&lt;/td&gt;
                &lt;/tr&gt;
              &lt;/tbody&gt;
            &lt;/table&gt;

            &lt;p&gt;L_2通常只针对系数矩阵w,而不针对偏置系数b,则损失函数为&lt;/p&gt;

            &lt;table&gt;
              &lt;tbody&gt;
                &lt;tr&gt;
                  &lt;td&gt;J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}&lt;/td&gt;
                  &lt;td&gt; &lt;/td&gt;
                  &lt;td&gt;a^L-y&lt;/td&gt;
                  &lt;td&gt; &lt;/td&gt;
                  &lt;td&gt;&lt;em&gt;2^2+\frac{\lambda}{2m}\sum&lt;/em&gt;{L=2}^L&lt;/td&gt;
                  &lt;td&gt; &lt;/td&gt;
                  &lt;td&gt;w&lt;/td&gt;
                  &lt;td&gt; &lt;/td&gt;
                  &lt;td&gt;_2^2&lt;/td&gt;
                &lt;/tr&gt;
              &lt;/tbody&gt;
            &lt;/table&gt;

            &lt;p&gt;反向传播从:&lt;/p&gt;

            &lt;p&gt;w^l=w^l-\alpha\sum_{i=1}^{m}\sigma^{i,l}(a^{x,l-1})^T&lt;/p&gt;

            &lt;p&gt;变为:&lt;/p&gt;

            &lt;p&gt;w^l=w^l-\alpha\sum_{i=1}^{m}\sigma^{i,l}(a^{x,l-1})^T-\alpha\lambda w^l&lt;/p&gt;
          &lt;/blockquote&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;data augmentation(数据集扩增)&lt;/p&gt;

      &lt;blockquote&gt;
        &lt;p&gt;有效前提是训练集与将来的数据是&lt;strong&gt;独立同分布&lt;/strong&gt;的,或者近似独立同分布&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;从数据源头采集更多的数据&lt;/li&gt;
          &lt;li&gt;复制原有数据并加上随机噪声&lt;/li&gt;
          &lt;li&gt;重采样&lt;/li&gt;
          &lt;li&gt;根据当前数据估计数据分布参数,使用该分布产生更多数据&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/blockquote&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Bootstrap/Bagging（封装）&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;ensemble（集成）&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;early stopping（提前终止训练）&lt;/p&gt;

      &lt;blockquote&gt;
        &lt;p&gt;迭代次数截断的方法来防止过拟合,再模型对训练数据集迭代收敛之前停止迭代防止过拟合.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;utilize invariance（利用不变性）&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Bayesian（贝叶斯方法）&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Sun, 14 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>关于 GBDT 特征编码</title>
        <description>&lt;h2 id=&quot;问题起因&quot;&gt;问题起因&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;在用户风险预测模型（安全审核）中，提出了特征应该用类别特征还是 one-hot 特征，针对此问题进行调研&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;理论分析&quot;&gt;理论分析&lt;/h2&gt;

&lt;h3 id=&quot;1gbdt做为树模型对高维稀疏特征处理效果差究其原因到底是因为什么呢&quot;&gt;1、GBDT做为树模型对高维稀疏特征处理效果差，究其原因到底是因为什么呢？？&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;假设在分类场景下，某一维特征有 1000 个类别，那么采用 one-hot 编码，每一维特征就是 0 or 1，那么 GBDT 可能会错误的学习到只要某个特征为 0 或者 1 就是某个类别，这种情况下就会产生过拟合，在训练数据不充足的情况下，这种问题更严重；&lt;/li&gt;
  &lt;li&gt;同样的场景下，不采用 one-hot 而采用 label encoding ，那么会存在什么问题，树模型在分裂的过程中，特征会变成分段划取，比如x&amp;lt;90 and x&amp;gt;50，这就会导致 50-90 之间的类别特征都有了关联来进行处理的，因此引入了额外的噪音（特征隐含信息）；&lt;/li&gt;
  &lt;li&gt;这也就解释了为啥在薪资预测模型中，GBDT 的效果差于线性模型，原因也在于特征通过 one-hot 编码过于稀疏，而线性模型可以很好的利用正则化来解决数据系数问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2那-gbdt应该用什么编码方式&quot;&gt;2、那 GBDT应该用什么编码方式&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;不适用 one-hot 编码原因
    &lt;ul&gt;
      &lt;li&gt;微软在 light GBM 文档里面说明了，category 特征可以直接输入，不需要 one-hot 编码，速度快八倍，准确度差不多&lt;/li&gt;
      &lt;li&gt;one-hot 编码容易导致特征维度过大且稀疏，导致GBDT模型学校效果差很多&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;不适用 label-encoding 编码原因
    &lt;ul&gt;
      &lt;li&gt;label-encoding 编码后 GBDT 会使得特征编码之间具有了关联性，容易引入噪音，尤其对一些训练集中未出现的类别，再实际测试集中有的类别特征容易出现问题，例如88这个类别特征没有出现过，但在测试时却有，模型很容易把他和同区间的类别特征统一处理，从而导致线上不可控问题，one-hot 可以很好的解决这种问题&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;实践出真知&quot;&gt;实践出真知&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;公说公有理，婆说婆有理，不管谁有理，实践出真知&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1label-encoding&quot;&gt;1.label-encoding&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;模型&lt;/th&gt;
      &lt;th&gt;LR&lt;/th&gt;
      &lt;th&gt;Random Forest&lt;/th&gt;
      &lt;th&gt;GBDT&lt;/th&gt;
      &lt;th&gt;XGboost&lt;/th&gt;
      &lt;th&gt;GradientBoosting + LogisticRegression&lt;/th&gt;
      &lt;th&gt;lightgbm&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;acc&lt;/td&gt;
      &lt;td&gt;0.7533&lt;/td&gt;
      &lt;td&gt;0.8657&lt;/td&gt;
      &lt;td&gt;0.8697&lt;/td&gt;
      &lt;td&gt;0.8511&lt;/td&gt;
      &lt;td&gt;0.8703&lt;/td&gt;
      &lt;td&gt;0.8710&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;confusion_matrix&lt;/td&gt;
      &lt;td&gt;[[542 163] [208 591]]&lt;/td&gt;
      &lt;td&gt;[[671 122] [ 79 632]]&lt;/td&gt;
      &lt;td&gt;[[668 114] [ 82 640]]&lt;/td&gt;
      &lt;td&gt;[[669 146] [ 81 608]]&lt;/td&gt;
      &lt;td&gt;[[660 105] [ 90 649]]&lt;/td&gt;
      &lt;td&gt;[[662 106] [ 88 648]]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;其他指标&lt;/td&gt;
      &lt;td&gt;1-precision: 0.7838 1-recall: 0.7397 1-f1: 0.7611 0-precision: 0.7227 0-recall: 0.7688 0-f1: 0.745&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8382 1-recall: 0.8889 1-f1: 0.8628 0-precision: 0.8947 0-recall: 0.8462 0-f1: 0.8697&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8488 1-recall: 0.8864 1-f1: 0.8672 0-precision: 0.8907 0-recall: 0.8542 0-f1: 0.8721&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8064 1-recall: 0.8824 1-f1: 0.8427 0-precision: 0.892 0-recall: 0.8209 0-f1: 0.855&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8607 1-recall: 0.8782 1-f1: 0.8694 0-precision: 0.88 0-recall: 0.8627 0-f1: 0.8713&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8594 1-recall: 0.8804 1-f1: 0.8698 0-precision: 0.8827 0-recall: 0.862 0-f1: 0.8722&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;2.one-hot&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;模型&lt;/th&gt;
      &lt;th&gt;LR&lt;/th&gt;
      &lt;th&gt;Random Forest&lt;/th&gt;
      &lt;th&gt;GBDT&lt;/th&gt;
      &lt;th&gt;XGboost&lt;/th&gt;
      &lt;th&gt;GradientBoosting + LogisticRegression&lt;/th&gt;
      &lt;th&gt;lightgbm&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;acc&lt;/td&gt;
      &lt;td&gt;0.8364&lt;/td&gt;
      &lt;td&gt;0.8684&lt;/td&gt;
      &lt;td&gt;0.8677&lt;/td&gt;
      &lt;td&gt;0.8664&lt;/td&gt;
      &lt;td&gt;0.8703&lt;/td&gt;
      &lt;td&gt;0.8690&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;confusion_matrix&lt;/td&gt;
      &lt;td&gt;[[[646 142] [104 612]]&lt;/td&gt;
      &lt;td&gt;[[670 118] [ 80 636]]&lt;/td&gt;
      &lt;td&gt;[[666 115] [ 84 639]]&lt;/td&gt;
      &lt;td&gt;[[668 119] [ 82 635]]&lt;/td&gt;
      &lt;td&gt;[[659 104] [ 91 650]]&lt;/td&gt;
      &lt;td&gt;[[655 102] [ 95 652]]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;其他指标&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8117 1-recall: 0.8547 1-f1: 0.8327 0-precision: 0.8613 0-recall: 0.8198 0-f1: 0.8401&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8435 1-recall: 0.8883 1-f1: 0.8653 0-precision: 0.8933 0-recall: 0.8503 0-f1: 0.8713&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8475 1-recall: 0.8838 1-f1: 0.8653 0-precision: 0.888 0-recall: 0.8528 0-f1: 0.87&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8422 1-recall: 0.8856 1-f1: 0.8634 0-precision: 0.8907 0-recall: 0.8488 0-f1: 0.8692&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8621 1-recall: 0.8772 1-f1: 0.8696 0-precision: 0.8787 0-recall: 0.8637 0-f1: 0.8711&lt;/td&gt;
      &lt;td&gt;1-precision: 0.8647 1-recall: 0.8728 1-f1: 0.8688 0-precision: 0.8733 0-recall: 0.8653 0-f1: 0.8693&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;结论：两者对比发现，one-hot编码再线性模型中提升较大，但对于树模型来说基本没有起到作用，或者说两种编码方式均可，可能的原因我们数据类别特征较少，均为两个或者三个类别，用one-hot和label-encoding没有很大的区别，最后值得指出的是再实际使用场景中，应该根据实际的数据特征来进行分析，不确定的情况下两者均进行尝试，对比效果再对其进行原因分析。&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/GBDT%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/GBDT%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
  </channel>
</rss>
