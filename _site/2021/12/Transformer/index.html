<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>Transformer探索</title>
  <meta name="description" content="        ">
  <meta name="author" content="MonkeyTB">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Transformer探索">
  <meta name="twitter:description" content="        ">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="Transformer探索">
  <meta property="og:description" content="        ">
  
  <link rel="icon" type="image/png" href="/images/favicon.png" />
  <link href="/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

<!-- 站点统计 -->
  <script 
  async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
  </script>  

<!-- 百度统计 -->
  
  <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "//hm.baidu.com/hm.js?7193fc529267517a0eea8035d7fede15";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
      })();
  </script>
  

<!-- google 统计 -->
  

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9005224472374751",
    enable_page_level_ads: true
  });
</script>

</head>


  <body>

    <span class="mobile btn-mobile-menu">        
      <div class="nav_container">
         <nav class="nav-menu-item" style = "float:right">
            <i class="nav-menu-item">
              <a href="/#blog" title="" class="blog-button">  博客主页
              </a>
            </i>
            
                <i class="nav-menu-item">

                  <a href="/archive" title="archive" class="btn-mobile-menu__icon">
                      所有文章
                  </a>
                </i>
            
                <i class="nav-menu-item">

                  <a href="/tags" title="tags" class="btn-mobile-menu__icon">
                      标签
                  </a>
                </i>
            
                <i class="nav-menu-item">

                  <a href="/about" title="about" class="btn-mobile-menu__icon">
                      关于我
                  </a>
                </i>
            
          </nav>
      </div>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">
        <!-- 头像效果-start -->
        <div class="ih-item circle effect right_to_left">            
            <a href="/#blog" title="前往 YiYao 的主页" class="blog-button">
                <div class="img"><img src="/images/avatar.jpg" alt="img"></div>
                <div class="info">
                    <div class="info-back">
                        <h2> 
                            
                        </h2>
                        <p>
                           
                                自然语言处理 / 机器学习
                            
                        </p>
                    </div>
                </div>
            </a>
        </div>
        <!-- 头像效果-end -->
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for YiYao" class="blog-button">YiYao</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">个人站</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">欢迎来到我的个人站~</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        

        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">博客主页</a></li>
                
                  <li class="navigation__item"><a href="/archive" title="archive">所有文章</a></li>
                
                  <li class="navigation__item"><a href="/tags" title="tags">标签</a></li>
                
                  <li class="navigation__item"><a href="/about" title="about">关于我</a></li>
                
              </ul>
            </nav>
          </div>          
        </div>


        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-clear"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <head>
  <link rel="stylesheet" href="/css/post.css">
</head>

<article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title">Transformer探索</h1>
    <div class="post-meta">
      <img src="/images/calendar.png" width="20px"/> 
      <time datetime="2021-12-04 00:00:00 +0800" itemprop="datePublished" class="post-meta__date date">2021-12-04</time>  

      <span id="busuanzi_container_page_pv"> | 阅读：<span id="busuanzi_value_page_pv"></span>次</span>
    </p>
    </div>
  </header>

  
    <h2 class="post-title">目录</h2>
    <ul>
  <li><a href="#1什么是transformer">1、什么是Transformer</a></li>
  <li><a href="#2transformer结构">2、Transformer结构</a>
    <ul>
      <li><a href="#21-总体结构">2.1 总体结构</a></li>
      <li><a href="#22-encoder">2.2 Encoder</a></li>
      <li><a href="#23-decoder">2.3 Decoder</a></li>
      <li><a href="#24-attention">2.4 Attention</a></li>
      <li><a href="#25-multi-head-attention">2.5 Multi-head Attention</a></li>
    </ul>
  </li>
  <li><a href="#3transformer相比于rnnlstm有什么优势为什么">3、Transformer相比于RNN/LSTM，有什么优势？为什么？</a></li>
  <li><a href="#参考文献">参考文献</a></li>
</ul>

  

  <section class="post">
    <head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

<h2 id="1什么是transformer">1、什么是Transformer</h2>

<blockquote>
  <p>《Attention is All You Need》是一篇谷歌提出的将attention思想发挥到极致的论文，这篇论文提出一个全新的模型，叫Transformer，抛弃了以往深度学习中的CNN和RNN。包括后来的Bert、GPT等都是基于此构建的，广泛的应用于NLP领域。</p>
</blockquote>

<h2 id="2transformer结构">2、Transformer结构</h2>

<h3 id="21-总体结构">2.1 总体结构</h3>

<p>​	Transformer的结构和Attention模型一样，Transformer模型中也采用了encoder-decoder结构。论文中encoder层由6个encoder堆叠在一起，decoder层也一样。</p>

<ul>
  <li>
    <p>encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点仅仅关注当前的词，从而能获得上下文语意</p>
  </li>
  <li>
    <p>decoder，包含encoder提到的两层，但这两层（encoder-decoder）中间还有一层attention层，帮助当前节点获得需要关注的重点内容。</p>
  </li>
</ul>

<p><img src="/images/posts/自然语言处理/transformer.jpg" alt="" /></p>

<h3 id="22-encoder">2.2 Encoder</h3>

<p>​	如图所示，encoder由6个相同的transformer block构成，针对一个block来讲，我们可以看出，首先对其输入进行embedding编码输入，这一步呢类似于word2vec编码，然后通过self-attention送到前馈神经网络，然后到下一个block。</p>

<p><img src="/images/posts/自然语言处理/transformer-encoder.png" alt="" /></p>

<p>下面对encoder每个模块进行说明：</p>

<ul>
  <li>
    <p>输入embedding</p>

    <blockquote>
      <p>对输入文本进行向量编码，值得注意的是，编码维度为$\color{blue}{d_{model}}$，encoder的输入和decoder的输入我们进行了权值共享，并且在embedding层每个权重都乘了一个$\color{blue}{\sqrt{d_{model}}}$。</p>
    </blockquote>
  </li>
  <li>
    <p>位置embedding</p>

    <blockquote>
      <p>Transformer结构由于天然无法感知空间，因此对于文本位置信息并不明确，而对于我们的翻译类任务，顺序信息尤为重要，因此引入位置信息的表达也就迫在眉睫，论文中引入了绝对位置编码，公式如下：</p>

      <p>$\color{blue}{PE_{(pos,2i)}=sin(pos/10000^{\frac{2i}{d_{model}}})}$</p>

      <p>$\color{blue}{PE_{(pos,2i)}=cos(pos/10000^{\frac{2i}{d_{model}}})}$</p>

      <p>其中$\color{blue}{pos}$代表位置，$\color{blue}i$代表向量维度中的index，这样对每个维度都有一个位置编码，偶数位置用正弦编码，奇数位置用余弦编码，最后将位置embedding和输入embedding相加送入block模块中。</p>
    </blockquote>
  </li>
  <li>
    <p>layer normalization</p>

    <blockquote>
      <p>这里主要说明一下Layer normalization 和Batch normalization的区别，他们都是数据归一化的一种方式，本质都是为了解决梯度消失或者梯度爆炸，主要是将模型中每层偏离的数据拉回归一化数据，这样有利于模型的学习（梯度更新）。区别如图所示：</p>

      <p><img src="/images/posts/自然语言处理/Layer Normalization.png" alt="" /></p>

      <p>图中可以看出，假设输入为三个字符，每个字符的embedding为6维，那么Batch Normalization是在特征维度进行归一化，而Layer Normalization不同的是在seq也就是图中的batch方向归一化，既对每个字符进行归一化。</p>

      <p>（小声bb：图中的batch当作sequence理解，纵向当作每个word的embedding理解，理解二维后再提升到三维理解（带上batch））</p>
    </blockquote>
  </li>
</ul>

<h3 id="23-decoder">2.3 Decoder</h3>

<p>​	decoder和encoder结构大同小异，基本一致，也是对输入做了embedding和位置编码，可以参照encoder，不同之处在于decoder中用到了mask attention，这是较为关键的一点，下面我们对论文提到的三种attention进行说明。</p>

<h3 id="24-attention">2.4 Attention</h3>

<ul>
  <li>self attention</li>
</ul>

<blockquote>
  <p>$\color{yellow}{self-attention}$再论文中用于encoder阶段，既每个block模块中对输入进行了$\color{yellow}{self-attention}$，也叫自注意力机制，自注意力机制输入的$\color{blue}{query、key、value}$是相同的，简单来讲，论文中的输入为（None，512），copy为三份，对每一份都用不同的权重矩阵（$\color{blue}{W^q、W^k、W^v\in R^{64\times 512}}$)，如图所示：</p>

  <p><img src="/images/posts/自然语言处理/self attention 1.png" alt="" /></p>

  <p>对于$\color{yellow}{self-attention}$来讲，就是要对当前query的词计算对每个词的注意力权重，这个计算由query和key计算得来，计算方式如下，对$\color{blue}{q_1}$来讲，分别计算每个k的Score，论文中提到，对于$\color{blue}{q}$和$\color{blue}{k}$的点乘结果除以一个8（$\color{blue}{\sqrt{64}}$），最后做一个softmax，就得到当前q对每个q的注意力分值，对应分值再乘以value向量，然后相加，便得到当前节点的self attention的值。</p>

  <p><img src="/images/posts/自然语言处理/self attention 2.png" alt="" /></p>

  <p>实际计算中，为了提升效率，q、k、v都为矩阵，计算公式如下：$\color{blue}{Attention(K,Q,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V}$。论文中也把这种方式叫做$\color{yellow}{Scaled Dot-Product Attention}$。</p>
</blockquote>

<ul>
  <li>attention</li>
</ul>

<blockquote>
  <p>解码器和编码器之间也用到attention，我们暂叫做普通attention，就是相当于把$\color{yellow}{self-attention}$的query向量变为了decoder中的输入编码，encoder的输出向量作为key、value向量。计算方式同上。</p>
</blockquote>

<ul>
  <li>mask attention</li>
</ul>

<blockquote>
  <p>$\color{yellow}{mask-attention}$用于解码器中，由于再解码的过程中，实际上我们并不知道下文会出现什么，而天然的attention会自动学习到后面的信息，这对于实际预测来讲并不公平，因此采用了$\color{yellow}{mask-attention}$，故名思意，就是对当前节点我们通过mask的方式不让attention看到后面的信息，transformer中具体用到了两种mask，一种是$\color{yellow}{padding-mask}$，这部分再$\color{yellow}{self-attention}$中用到，一个是$\color{yellow}{sequence-mask}$，这部分再解码器中用到，具体做法如下：</p>

  <ul>
    <li>padding mask</li>
  </ul>

  <blockquote>
    <p>$\color{yellow}{padding-mask}$是因为模型接受到的输入长度不同，为了便于计算我们通常要padding到相同的长度，这就会有一些短的会补0，而这些是没有意义的，因此再计算attention的时候我们需要把这部分无用的剔除。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！而我们的$\color{yellow}{padding-mask}$实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。</p>
  </blockquote>

  <ul>
    <li>sequence mask</li>
  </ul>

  <blockquote>
    <p>$\color{yellow}{sequence-mask}$是为了解码的时候不看到未来的信息，具体做法就是我们构造一个上三角全为0的的矩阵作用到序列上，这样我们再解码计算的时候就不会看到但前节点以后的信息了。</p>
  </blockquote>
</blockquote>

<h3 id="25-multi-head-attention">2.5 Multi-head Attention</h3>

<p>​	原论文中说到进行Multi-head Attention的原因是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，最后再将各个方面的信息综合起来。其实直观上也可以想到，如果自己设计这样的一个模型，必然也不会只做一次attention，多次attention综合的结果至少能够起到增强模型的作用，也可以类比CNN中同时使用<strong>多个卷积核</strong>的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征/信息</strong>。</p>

<h2 id="3transformer相比于rnnlstm有什么优势为什么">3、Transformer相比于RNN/LSTM，有什么优势？为什么？</h2>

<p>​	1、Transformer 相比RNN/LSTM并行计算的能力更好，序列信息由于要等信息的传递，所以并行度不高。</p>

<p>​	2、Transformer 相比RNN/LSTM特征提取能力更强，但不代表可以完全取代RNN/LSTM，任何模型都有其特定的使用场景。</p>

<h2 id="参考文献">参考文献</h2>

<p><a href="https://www.cnblogs.com/mantch/p/11591937.html">1、Transformer各层网络结构详解！面试必备！(附代码实现)</a></p>

<p><a href="https://space.bilibili.com/1567748478/video">2、视频</a></p>

<p><a href="https://readpaper.com/pdf-annotate/note?noteId=627057414680178688&amp;pdfId=4557871218548547585">3、论文</a></p>


  </section>

</article>

<section>

            <div class="content-play">
              <p><a href="javascript:void(0)" onclick="dashangToggle()" class="dashang" title="打赏，支持一下">打赏一个呗</a></p>
              <div class="hide_box-play"></div>
              <div class="shang_box-play">
                <a class="shang_close-play" href="javascript:void(0)" onclick="dashangToggle()" title="关闭"><img src="/images/payimg/close.jpg" alt="取消" /></a>
                <div class="shang_tit-play">
                  <p>感谢您的支持，我会继续努力的!</p>
                </div>
                <div class="shang_payimg">
                    <img src="/images/payimg/alipayimg.jpg" alt="扫码支持" title="扫一扫" />
                </div>
              <div class="shang_payimg">    
                    <img src="/images/payimg/weipayimg.jpg" alt="扫码支持" title="扫一扫" />
                </div>
                <div class="pay_explain">扫码打赏，你说多少就多少</div>
                <div class="shang_payselect">
                  <div class="pay_item checked" data-id="alipay">
                    <span class="pay_logo"><img src="/images/payimg/alipay.jpg" alt="支付宝" /></span>
                  </div>
                  <div class="pay_item" data-id="weipay">
                    <span class="pay_logo"><img src="/images/payimg/wechat.jpg" alt="微信" /></span>
                  </div>
                </div>
                <div class="shang_info-play">
                  <p>打开<span id="shang_pay_txt">微信</span>扫一扫，即可进行扫码打赏哦</p>
                </div>
              </div>
            </div>
            <script type="text/javascript">
            function dashangToggle(){
              $(".hide_box-play").fadeToggle();
              $(".shang_box-play").fadeToggle();
            }
            </script>

            <div style="text-align:center;margin:50px 0; font:normal 14px/24px 'MicroSoft YaHei';"></div>

            <style type="text/css">
              .content-play{width:80%;margin-top: 20px;margin-bottom: 10px;height:40px;}
              .hide_box-play{z-index:999;filter:alpha(opacity=50);background:#666;opacity: 0.5;-moz-opacity: 0.5;left:0;top:0;height:99%;width:100%;position:fixed;display:none;}
              .shang_box-play{width:540px;height:540px;padding:10px;background-color:#fff;border-radius:10px;position:fixed;z-index:1000;left:50%;top:50%;margin-left:-280px;margin-top:-280px;border:1px dotted #dedede;display:none;}
              .shang_box-play img{border:none;border-width:0;}
              .dashang{display:block;width:100px;margin:5px auto;height:25px;line-height:25px;padding:10px;background-color:#E74851;color:#fff;text-align:center;text-decoration:none;border-radius:10px;font-weight:bold;font-size:16px;transition: all 0.3s;}
              .dashang:hover{opacity:0.8;padding:15px;font-size:18px;}
              .shang_close-play{float:right;display:inline-block;
                margin-right: 10px;margin-top: 20px;
              }
              .shang_logo{display:block;text-align:center;margin:20px auto;}
              .shang_tit-play{width: 100%;height: 75px;text-align: center;line-height: 66px;color: #a3a3a3;font-size: 16px;background: url('/images/payimg/cy-reward-title-bg.jpg');font-family: 'Microsoft YaHei';margin-top: 7px;margin-right:2px;}
              .shang_tit-play p{color:#a3a3a3;text-align:center;font-size:16px;}
              .shang_payimg{width:140px;padding:10px;padding-left: 80px; /*border:6px solid #EA5F00;**/margin:0 auto;border-radius:3px;height:140px;display:inline-block;}
              .shang_payimg img{display:inline-block;margin-right:10px;float:left;text-align:center;width:140px;height:140px; }
              .pay_explain{text-align:center;margin:10px auto;font-size:12px;color:#545454;}
              .shang_payselect{text-align:center;margin:0 auto;margin-top:40px;cursor:pointer;height:60px;width:500px;margin-left:110px;}
              .shang_payselect .pay_item{display:inline-block;margin-right:140px;float:left;}
              .shang_info-play{clear:both;}
              .shang_info-play p,.shang_info-play a{color:#C3C3C3;text-align:center;font-size:12px;text-decoration:none;line-height:2em;}
            </style>

       <ul class="pager">
        
        <li class="previous">
            <a href="/2021/11/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E6%8E%A2%E7%B4%A2/" data-toggle="tooltip" data-placement="top" title="生成式模型探索">上一篇：  <span>生成式模型探索</span>
            </a>
        </li>
        
        
    </ul>
</section>

<section class="post-comments">
<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="Mi8xLzM=">
<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
</script>
<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
</section>


            <section class="footer">
    <footer>
        <div class = "footer_div">  
        <nav class="cover-navigation navigation--social">
          <ul class="navigation">

          
          <!-- Github -->
          <li class="navigation__item_social">
            <a href="https://github.com/monkeytb" title="@monkeytb 的 Github" target="_blank">
              <div class="footer-social-icon" style="background:url(/images/github.png);"></div>
            </a>
          </li>
          

          

          

          

          

          
          


          
          <!-- Email -->
          <li class="navigation__item_social">
            <a href="mailto:13689446615@163.com" title="Contact me">
              <div class="footer-social-icon" style="background:url(/images/email.png);"></div>
            </a>
          </li>
          

          </ul>
        </nav>

        </div>

        <div class = "footer_div">  
           <p class="copyright text-muted">
            Copyright &copy; YiYao 2021 Theme by <a href="https://monkeytb.github.io">MonkeyTB</a> |
            <iframe
                style="margin-left: 2px; margin-bottom:-5px;"
                frameborder="0" scrolling="0" width="91px" height="20px"
                src="https://ghbtns.com/github-btn.html?user=monkeytb&repo=monkeytb.github.io&type=star&count=true" >
            </iframe>
            </p>
        	<div align="right">
    			<link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

          <!-- 访问统计 -->
          <span id="busuanzi_container_site_pv">
            本站总访问量
            <span id="busuanzi_value_site_pv"></span>次
          </span>

        </div>
        <div>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>
<script type="text/javascript" src="/js/main.js"></script>

<script type="text/javascript" src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>

</html>
